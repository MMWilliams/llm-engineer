{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow DAG for Movie Data Processing\n",
    "\n",
    "This is an Apache Airflow DAG (Directed Acyclic Graph) that processes movie data through several steps to create searchable vector embeddings. Here are the main components and functionality:\n",
    "\n",
    "## 1. Data Pipeline Overview\n",
    "- Loads movie data from both CSV and JSON files stored in Google Cloud Storage (GCS).\n",
    "- Processes movie titles and overviews.\n",
    "- Generates embeddings using OpenAI's `text-embedding-ada-002` model.\n",
    "- Stores the embeddings in Pinecone (a vector database).\n",
    "- Saves results and metadata to BigQuery tables.\n",
    "\n",
    "## 2. Key Features\n",
    "- Handles large datasets through chunked processing.\n",
    "- Implements parallel processing using both `ThreadPoolExecutor` and `ProcessPoolExecutor`.\n",
    "- Includes robust error handling and retry mechanisms.\n",
    "- Splits long texts to handle OpenAI's token limits.\n",
    "- Processes batch operations to optimize API calls.\n",
    "- Uses Dask for distributed computing capabilities.\n",
    "\n",
    "## 3. Main Processing Steps\n",
    "- **Text Preprocessing** (in parallel)\n",
    "- **Embedding Generation** with OpenAI\n",
    "- **Vector Storage** in Pinecone\n",
    "- **Data Storage** in BigQuery (partitioned and clustered tables)\n",
    "\n",
    "## 4. Infrastructure\n",
    "- Runs on **Apache Airflow**.\n",
    "- Uses **Google Cloud Platform** (GCS and BigQuery).\n",
    "- Integrates with **OpenAI** for embeddings.\n",
    "- Uses **Pinecone** for vector storage.\n",
    "- Implements configuration management through **Airflow Variables**.\n",
    "\n",
    "## 5. Performance Optimizations\n",
    "- **Dynamic Worker Allocation** based on CPU cores.\n",
    "- **Batch Processing** with configurable chunk sizes.\n",
    "- **Parallel Uploads** to both Pinecone and BigQuery.\n",
    "- **Text Splitting** to handle token limits efficiently.\n",
    "- **Memory-Efficient Processing** of large datasets.\n",
    "\n",
    "## DAG Characteristics\n",
    "The DAG runs daily and includes error handling, retries, and monitoring capabilities. It's designed to be scalable and efficient when processing large volumes of movie data while maintaining fault tolerance.\n",
    "\n",
    "--- \n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.google.cloud.operators.bigquery import BigQueryOperator\n",
    "from airflow.models import Variable\n",
    "from airflow.utils.dates import days_ago\n",
    "\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Generator\n",
    "from google.cloud import storage, bigquery\n",
    "from openai import OpenAI\n",
    "import pinecone\n",
    "import logging\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import multiprocessing\n",
    "from itertools import islice\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Constants for batch processing\n",
    "CHUNK_SIZE = 1000\n",
    "EMBEDDING_BATCH_SIZE = 100\n",
    "MAX_RETRIES = 3\n",
    "NUM_CORES = multiprocessing.cpu_count()\n",
    "MAX_WORKERS = min(32, NUM_CORES * 2)  # Limit max workers to avoid overwhelming resources\n",
    "\n",
    "class PipelineConfig:\n",
    "    def __init__(self):\n",
    "        self.gcs_bucket = Variable.get('GCS_BUCKET')\n",
    "        self.input_path = Variable.get('INPUT_PATH')\n",
    "        self.project_id = Variable.get('GCP_PROJECT_ID')\n",
    "        self.bq_dataset = Variable.get('BQ_DATASET')\n",
    "        self.full_text_table = f\"{self.project_id}.{self.bq_dataset}.full_text\"\n",
    "        self.metadata_table = f\"{self.project_id}.{self.bq_dataset}.metadata\"\n",
    "        self.dropped_table = f\"{self.project_id}.{self.bq_dataset}.dropped\"\n",
    "        self.pinecone_api_key = Variable.get('PINECONE_API_KEY')\n",
    "        self.pinecone_env = Variable.get('PINECONE_ENV')\n",
    "        self.index_name = Variable.get('PINECONE_INDEX_NAME')\n",
    "        self.openai_api_key = Variable.get('OPENAI_API_KEY')\n",
    "        self.num_processes = NUM_CORES\n",
    "def split_text_by_tokens(text: str, encoder, max_tokens: int = 4096, overlap: int = 100) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into chunks that don't exceed the token limit, with optional overlap\n",
    "    \n",
    "    Args:\n",
    "        text: Text to split\n",
    "        encoder: Tokenizer encoder\n",
    "        max_tokens: Maximum tokens per chunk\n",
    "        overlap: Number of tokens to overlap between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks that each fit within token limit\n",
    "    \"\"\"\n",
    "    tokens = encoder.encode(text)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        # Find the end position for this chunk\n",
    "        end = start + max_tokens\n",
    "        if end >= len(tokens):\n",
    "            chunk_tokens = tokens[start:]\n",
    "        else:\n",
    "            # Look for a good splitting point (space or punctuation)\n",
    "            # Go backwards from max_tokens to find a good split point\n",
    "            split_point = end\n",
    "            while split_point > start + max_tokens - 100 and split_point < len(tokens):\n",
    "                # Check if this token represents a good splitting point\n",
    "                if tokens[split_point] in [encode_token for encode_token in encoder.encode(\". \")] or \\\n",
    "                   tokens[split_point] in [encode_token for encode_token in encoder.encode(\"? \")] or \\\n",
    "                   tokens[split_point] in [encode_token for encode_token in encoder.encode(\"! \")] or \\\n",
    "                   tokens[split_point] in [encode_token for encode_token in encoder.encode(\"\\n\")]:\n",
    "                    break\n",
    "                split_point -= 1\n",
    "            \n",
    "            if split_point == start + max_tokens - 100:\n",
    "                # If no good splitting point found, just split at max_tokens\n",
    "                split_point = end\n",
    "            \n",
    "            chunk_tokens = tokens[start:split_point]\n",
    "        \n",
    "        # Decode chunk back to text\n",
    "        chunk_text = encoder.decode(chunk_tokens)\n",
    "        chunks.append(chunk_text.strip())\n",
    "        \n",
    "        # Move start position for next chunk, accounting for overlap\n",
    "        start = start + len(chunk_tokens) - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def parallel_text_preprocessing(texts: List[str]) -> List[str]:\n",
    "    \"\"\"Parallel text preprocessing using ProcessPoolExecutor\"\"\"\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        processed_texts = list(executor.map(preprocess_text, texts))\n",
    "    return processed_texts\n",
    "\n",
    "def batch_generator(iterable, batch_size):\n",
    "    \"\"\"Generate batches from an iterable\"\"\"\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, batch_size)):\n",
    "        yield batch\n",
    "\n",
    "def parallel_generate_embeddings(texts: List[str], openai_client: OpenAI) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings in parallel using ThreadPoolExecutor with text splitting\"\"\"\n",
    "    encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def process_batch(batch_texts):\n",
    "        retry_count = 0\n",
    "        current_batch_size = len(batch_texts)\n",
    "        \n",
    "        while retry_count < MAX_RETRIES:\n",
    "            try:\n",
    "                response = openai_client.embeddings.create(\n",
    "                    input=batch_texts,\n",
    "                    model=\"text-embedding-ada-002\"\n",
    "                )\n",
    "                return [item.embedding for item in response.data]\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                logger.error(f\"Error in batch embedding: {e}. Retry {retry_count}/{MAX_RETRIES}\")\n",
    "                if retry_count == MAX_RETRIES:\n",
    "                    return [None] * current_batch_size\n",
    "                time.sleep(2 ** retry_count)\n",
    "    \n",
    "    # Split long texts and track their original indices\n",
    "    processed_texts = []\n",
    "    text_map = {}  # Maps new indices to original indices\n",
    "    current_idx = 0\n",
    "    \n",
    "    for idx, text in enumerate(texts):\n",
    "        chunks = split_text_by_tokens(text, encoder)\n",
    "        for chunk in chunks:\n",
    "            processed_texts.append(chunk)\n",
    "            text_map[current_idx] = {'original_idx': idx, 'total_chunks': len(chunks)}\n",
    "            current_idx += 1\n",
    "    \n",
    "    # Process all chunks in parallel batches\n",
    "    batches = list(batch_generator(processed_texts, EMBEDDING_BATCH_SIZE))\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=min(8, len(batches))) as executor:\n",
    "        batch_results = list(executor.map(process_batch, batches))\n",
    "    \n",
    "    # Flatten batch results\n",
    "    chunk_embeddings = []\n",
    "    for batch in batch_results:\n",
    "        if batch:\n",
    "            chunk_embeddings.extend(batch)\n",
    "    \n",
    "    # Combine embeddings for chunks from the same original text\n",
    "    final_embeddings = [None] * len(texts)\n",
    "    current_original_idx = -1\n",
    "    current_chunks = []\n",
    "    \n",
    "    for i, embedding in enumerate(chunk_embeddings):\n",
    "        if embedding is None:\n",
    "            continue\n",
    "            \n",
    "        original_idx = text_map[i]['original_idx']\n",
    "        total_chunks = text_map[i]['total_chunks']\n",
    "        \n",
    "        if original_idx != current_original_idx:\n",
    "            # Process previous chunks if any\n",
    "            if current_chunks:\n",
    "                final_embeddings[current_original_idx] = np.mean(current_chunks, axis=0).tolist()\n",
    "            # Start new chunk collection\n",
    "            current_original_idx = original_idx\n",
    "            current_chunks = [embedding]\n",
    "        else:\n",
    "            current_chunks.append(embedding)\n",
    "        \n",
    "        # Process last chunk if it's all chunks for this text\n",
    "        if len(current_chunks) == total_chunks:\n",
    "            final_embeddings[current_original_idx] = np.mean(current_chunks, axis=0).tolist()\n",
    "            current_chunks = []\n",
    "    \n",
    "    # Process any remaining chunks\n",
    "    if current_chunks:\n",
    "        final_embeddings[current_original_idx] = np.mean(current_chunks, axis=0).tolist()\n",
    "    \n",
    "    return final_embeddings\n",
    "\n",
    "def process_chunk(chunk: pd.DataFrame, config: PipelineConfig, openai_client: OpenAI, pinecone_index) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single chunk of data with parallel processing and text splitting\"\"\"\n",
    "    logger.info(f\"Processing chunk with {len(chunk)} records.\")\n",
    "    \n",
    "    # Parallel text preprocessing\n",
    "    texts = chunk['title'].fillna('') + \". \" + chunk['overview'].fillna('')\n",
    "    chunk['combined_text'] = parallel_text_preprocessing(texts.tolist())\n",
    "    \n",
    "    # Parallel embedding generation with text splitting\n",
    "    embeddings = parallel_generate_embeddings(chunk['combined_text'].tolist(), openai_client)\n",
    "    chunk['embedding'] = embeddings\n",
    "    \n",
    "    # Prepare vectors for Pinecone\n",
    "    vectors = [\n",
    "        (str(row.get('id', idx)), row['embedding'], {})\n",
    "        for idx, row in chunk.iterrows()\n",
    "        if row['embedding'] is not None\n",
    "    ]\n",
    "    \n",
    "    # Parallel upload to Pinecone\n",
    "    upload_success = parallel_upload_to_pinecone(vectors, pinecone_index)\n",
    "    \n",
    "    # Prepare results - now we shouldn't have any dropped records due to length\n",
    "    processed_records = chunk[chunk['embedding'].notna()]\n",
    "    \n",
    "    return {\n",
    "        'full_text': processed_records[['id', 'combined_text']],\n",
    "        'metadata': processed_records[['id']].assign(\n",
    "            process_date=datetime.utcnow(),\n",
    "            filename=f\"batch_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}\",\n",
    "            status='processed',\n",
    "            created_at=datetime.utcnow()\n",
    "        ),\n",
    "        'dropped': chunk[chunk['embedding'].isna()]  # Only includes records that failed for other reasons\n",
    "    }\n",
    "\n",
    "def parallel_upload_to_pinecone(vectors: List[tuple], pinecone_index, batch_size: int = 100):\n",
    "    \"\"\"Upload vectors to Pinecone in parallel batches\"\"\"\n",
    "    def upload_batch(batch):\n",
    "        retry_count = 0\n",
    "        while retry_count < MAX_RETRIES:\n",
    "            try:\n",
    "                pinecone_index.upsert(vectors=batch)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                logger.error(f\"Error uploading to Pinecone: {e}. Retry {retry_count}/{MAX_RETRIES}\")\n",
    "                if retry_count == MAX_RETRIES:\n",
    "                    return False\n",
    "                time.sleep(2 ** retry_count)\n",
    "    \n",
    "    batches = list(batch_generator(vectors, batch_size))\n",
    "    with ThreadPoolExecutor(max_workers=min(8, len(batches))) as executor:\n",
    "        results = list(executor.map(upload_batch, batches))\n",
    "    \n",
    "    return all(results)\n",
    "\n",
    "def process_chunk(chunk: pd.DataFrame, config: PipelineConfig, openai_client: OpenAI, pinecone_index) -> Dict[str, Any]:\n",
    "    \"\"\"Process a single chunk of data with parallel processing\"\"\"\n",
    "    logger.info(f\"Processing chunk with {len(chunk)} records.\")\n",
    "    \n",
    "    # Parallel text preprocessing\n",
    "    texts = chunk['title'].fillna('') + \". \" + chunk['overview'].fillna('')\n",
    "    chunk['combined_text'] = parallel_text_preprocessing(texts.tolist())\n",
    "    \n",
    "    # Parallel embedding generation\n",
    "    embeddings = parallel_generate_embeddings(chunk['combined_text'].tolist(), openai_client)\n",
    "    chunk['embedding'] = embeddings\n",
    "    \n",
    "    # Prepare vectors for Pinecone\n",
    "    vectors = [\n",
    "        (str(row.get('id', idx)), row['embedding'], {})\n",
    "        for idx, row in chunk.iterrows()\n",
    "        if row['embedding'] is not None\n",
    "    ]\n",
    "    \n",
    "    # Parallel upload to Pinecone\n",
    "    upload_success = parallel_upload_to_pinecone(vectors, pinecone_index)\n",
    "    \n",
    "    # Prepare results\n",
    "    dropped_records = chunk[chunk['embedding'].isna()]\n",
    "    processed_records = chunk[chunk['embedding'].notna()]\n",
    "    \n",
    "    return {\n",
    "        'full_text': processed_records[['id', 'combined_text']],\n",
    "        'metadata': processed_records[['id']].assign(\n",
    "            process_date=datetime.utcnow(),\n",
    "            filename=f\"batch_{datetime.utcnow().strftime('%Y%m%d%H%M%S')}\",\n",
    "            status='processed',\n",
    "            created_at=datetime.utcnow()\n",
    "        ),\n",
    "        'dropped': dropped_records\n",
    "    }\n",
    "\n",
    "def parallel_upload_to_bigquery(dfs: List[pd.DataFrame], table_id: str, config: PipelineConfig):\n",
    "    \"\"\"Upload multiple dataframes to BigQuery in parallel\"\"\"\n",
    "    def upload_single_df(df, partition_id):\n",
    "        client = bigquery.Client()\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            schema=[\n",
    "                bigquery.SchemaField(\"id\", \"STRING\"),\n",
    "                bigquery.SchemaField(\"combined_text\", \"STRING\"),\n",
    "                bigquery.SchemaField(\"created_at\", \"TIMESTAMP\")\n",
    "            ],\n",
    "            write_disposition=\"WRITE_APPEND\",\n",
    "            time_partitioning=bigquery.TimePartitioning(\n",
    "                type_=bigquery.TimePartitioningType.DAY,\n",
    "                field=\"created_at\"\n",
    "            ),\n",
    "            clustering_fields=[\"id\"]\n",
    "        )\n",
    "        \n",
    "        uri = f\"gs://{config.gcs_bucket}/tmp/{table_id}_{partition_id}.csv\"\n",
    "        df.to_csv(f\"/tmp/{table_id}_{partition_id}.csv\", index=False)\n",
    "        \n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(config.gcs_bucket)\n",
    "        blob = bucket.blob(f\"tmp/{table_id}_{partition_id}.csv\")\n",
    "        blob.upload_from_filename(f\"/tmp/{table_id}_{partition_id}.csv\")\n",
    "        \n",
    "        load_job = client.load_table_from_uri(uri, table_id, job_config=job_config)\n",
    "        return load_job.result()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=min(8, len(dfs))) as executor:\n",
    "        futures = [executor.submit(upload_single_df, df, i) for i, df in enumerate(dfs)]\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "def process_movie_data(**context):\n",
    "    \"\"\"Main processing function with enhanced parallelization\"\"\"\n",
    "    config = PipelineConfig()\n",
    "    \n",
    "    # Initialize clients\n",
    "    openai_client = OpenAI(api_key=config.openai_api_key)\n",
    "    pinecone.init(api_key=config.pinecone_api_key, environment=config.pinecone_env)\n",
    "    pinecone_index = pinecone.Index(config.index_name)\n",
    "    \n",
    "    # Load and combine data\n",
    "    logger.info(\"Loading data from GCS...\")\n",
    "    df_csv = load_data_from_gcs(config.gcs_bucket, f\"{config.input_path}/output.csv\")\n",
    "    df_json = load_data_from_gcs(config.gcs_bucket, f\"{config.input_path}/output.json\")\n",
    "    df = ensure_matching_schema(df_csv, df_json)\n",
    "    \n",
    "    # Process in parallel using Dask\n",
    "    chunks = df.repartition(npartitions=config.num_processes)\n",
    "    \n",
    "    # Process chunks with enhanced parallelization\n",
    "    process_chunk_partial = partial(process_chunk, config=config, \n",
    "                                  openai_client=openai_client, \n",
    "                                  pinecone_index=pinecone_index)\n",
    "    \n",
    "    results = chunks.map_partitions(process_chunk_partial).compute()\n",
    "    \n",
    "    # Combine results\n",
    "    full_text_dfs = [result['full_text'] for result in results]\n",
    "    metadata_dfs = [result['metadata'] for result in results]\n",
    "    dropped_dfs = [result['dropped'] for result in results if not result['dropped'].empty]\n",
    "    \n",
    "    # Parallel upload to BigQuery\n",
    "    logger.info(\"Uploading processed data to BigQuery...\")\n",
    "    parallel_upload_to_bigquery(full_text_dfs, config.full_text_table, config)\n",
    "    parallel_upload_to_bigquery(metadata_dfs, config.metadata_table, config)\n",
    "    if dropped_dfs:\n",
    "        parallel_upload_to_bigquery(dropped_dfs, config.dropped_table, config)\n",
    "    \n",
    "    logger.info(f\"Processing complete. Total records processed: {len(df)}\")\n",
    "\n",
    "# DAG definition remains the same\n",
    "# Create DAG\n",
    "with DAG(\n",
    "    'movie_vector_processing',\n",
    "    default_args={\n",
    "        'owner': 'airflow',\n",
    "        'depends_on_past': False,\n",
    "        'email_on_failure': True,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 2,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "        'start_date': datetime(2024, 10, 27),\n",
    "        'pool': 'movie_processing_pool',\n",
    "    },\n",
    "    description='Process movie data and generate vector embeddings',\n",
    "    schedule_interval='0 0 * * *',\n",
    "    catchup=False,\n",
    "    tags=['movies', 'vectors', 'embeddings'],\n",
    "    concurrency=4,\n",
    ") as dag:\n",
    "    \n",
    "    create_full_text_table = BigQueryOperator(\n",
    "        task_id='create_full_text_table',\n",
    "        sql=CREATE_FULL_TEXT_TABLE_QUERY,\n",
    "        use_legacy_sql=False,\n",
    "        params={\n",
    "            'project_id': '{{ var.value.GCP_PROJECT_ID }}',\n",
    "            'dataset': '{{ var.value.BQ_DATASET }}'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    create_metadata_table = BigQueryOperator(\n",
    "        task_id='create_metadata_table',\n",
    "        sql=CREATE_METADATA_TABLE_QUERY,\n",
    "        use_legacy_sql=False,\n",
    "        params={\n",
    "            'project_id': '{{ var.value.GCP_PROJECT_ID }}',\n",
    "            'dataset': '{{ var.value.BQ_DATASET }}'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    process_data = PythonOperator(\n",
    "        task_id='process_movie_data',\n",
    "        python_callable=process_movie_data,\n",
    "        provide_context=True,\n",
    "    )\n",
    "    \n",
    "    [create_full_text_table, create_metadata_table] >> process_data\n",
    "\n",
    "    ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Composer Deployment Script Summary\n",
    "\n",
    "This script automates the deployment of an Apache Airflow DAG in Google Cloud Composer for movie data processing. It performs the following steps:\n",
    "\n",
    "1. **Set Environment Variables**: Configures project, region, Composer environment, and file paths.\n",
    "\n",
    "2. **Create Temporary Directory**: Manages temporary files needed for deployment.\n",
    "\n",
    "3. **Generate `requirements.txt`**: Lists Python dependencies required for the DAG.\n",
    "\n",
    "4. **Configure Google Cloud Project**: Sets the correct Google Cloud project context.\n",
    "\n",
    "5. **Get DAG Folder Location**: Retrieves and processes the Composer DAG bucket path.\n",
    "\n",
    "6. **Upload DAG and Requirements**: Uploads the DAG and dependencies to the Composer bucket.\n",
    "\n",
    "7. **Install Dependencies**: Updates Composer to install Python packages from `requirements.txt`.\n",
    "\n",
    "8. **Set Airflow Variables**: Configures key variables for input paths, API keys, and project settings.\n",
    "\n",
    "9. **Create BigQuery Dataset**: Ensures the BigQuery dataset is available.\n",
    "\n",
    "10. **Clean Up**: Deletes the temporary deployment directory.\n",
    "\n",
    "11. **Deployment Completion**: Notifies that the deployment is complete and prompts verification in the Cloud Composer UI.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```bash \n",
    "#!/bin/bash\n",
    "\n",
    "# Set your Google Cloud project and Composer environment variables\n",
    "PROJECT_ID=\"your-project-id\"\n",
    "REGION=\"your-region\"  # e.g., us-central1\n",
    "ENVIRONMENT_NAME=\"your-composer-environment\"\n",
    "DAG_NAME=\"movie_vector_processing.py\"\n",
    "REQUIREMENTS_FILE=\"requirements.txt\"\n",
    "\n",
    "# Create a temporary directory for deployment files\n",
    "TEMP_DIR=$(mktemp -d)\n",
    "echo \"Created temporary directory: $TEMP_DIR\"\n",
    "\n",
    "# Create requirements.txt for additional dependencies\n",
    "cat > \"$TEMP_DIR/$REQUIREMENTS_FILE\" << EOL\n",
    "dask[complete]>=2024.1.0\n",
    "openai>=1.3.0\n",
    "pinecone-client>=2.2.4\n",
    "tiktoken>=0.5.0\n",
    "nltk>=3.8.1\n",
    "google-cloud-storage>=2.13.0\n",
    "google-cloud-bigquery>=3.13.0\n",
    "EOL\n",
    "\n",
    "# Set the Google Cloud project\n",
    "gcloud config set project $PROJECT_ID\n",
    "\n",
    "# Get the Composer environment's DAG folder location\n",
    "BUCKET_NAME=$(gcloud composer environments describe $ENVIRONMENT_NAME \\\n",
    "    --location $REGION \\\n",
    "    --format=\"get(config.dagGcsPrefix)\")\n",
    "\n",
    "# Remove the 'gs://' prefix and '/dags' suffix from the bucket path\n",
    "BUCKET_NAME=$(echo $BUCKET_NAME | sed 's|gs://||' | sed 's|/dags||')\n",
    "\n",
    "# Upload the DAG file to the Composer environment\n",
    "echo \"Uploading DAG file to Cloud Composer...\"\n",
    "gsutil cp $DAG_NAME gs://$BUCKET_NAME/dags/\n",
    "\n",
    "# Create a plugins directory if it doesn't exist\n",
    "gsutil ls gs://$BUCKET_NAME/plugins || gsutil mb gs://$BUCKET_NAME/plugins\n",
    "\n",
    "# Upload requirements file to the plugins directory\n",
    "echo \"Uploading requirements file...\"\n",
    "gsutil cp $TEMP_DIR/$REQUIREMENTS_FILE gs://$BUCKET_NAME/plugins/requirements.txt\n",
    "\n",
    "# Install additional dependencies in the Composer environment\n",
    "gcloud composer environments update $ENVIRONMENT_NAME \\\n",
    "    --location $REGION \\\n",
    "    --update-pypi-packages-from-file gs://$BUCKET_NAME/plugins/requirements.txt\n",
    "\n",
    "# Set Airflow variables\n",
    "echo \"Setting Airflow variables...\"\n",
    "gcloud composer environments run $ENVIRONMENT_NAME \\\n",
    "    --location $REGION variables -- \\\n",
    "    set GCS_BUCKET \"your-gcs-bucket\"\n",
    "\n",
    "gcloud composer environments run $ENVIRONMENT_NAME \\\n",
    "    --location $REGION variables -- \\\n",
    "    set INPUT_PATH \"your/input/path\"\n",
    "\n",
    "gcloud composer environments run $ENVIRONMENT_NAME \\\n",
    "    --location $REGION variables -- \\\n",
    "    set GCP_PROJECT_ID \"$PROJECT_ID\"\n",
    "\n",
    "gcloud composer environments run $ENVIRONMENT_NAME \\\n",
    "    --location $REGION variables -- \\\n",
    "    set BQ_DATASET \"your_dataset\"\n",
    "\n",
    "gcloud composer environments run $ENVIRONMENT_NAME \\\n",
    "    --location $REGION variables -- \\\n",
    "    set PINECONE_API_KEY \"your-pinecone-api-key\"\n",
    "\n",
    "gcloud composer environments run $ENVIRONMENT_NAME \\\n",
    "    --location $REGION variables -- \\\n",
    "    set PINECONE_ENV \"your-pinecone-environment\"\n",
    "\n",
    "gcloud composer environments run $ENVIRONMENT_NAME \\\n",
    "    --location $REGION variables -- \\\n",
    "    set PINECONE_INDEX_NAME \"your-index-name\"\n",
    "\n",
    "gcloud composer environments run $ENVIRONMENT_NAME \\\n",
    "    --location $REGION variables -- \\\n",
    "    set OPENAI_API_KEY \"your-openai-api-key\"\n",
    "\n",
    "# Create BigQuery dataset if it doesn't exist\n",
    "echo \"Creating BigQuery dataset if it doesn't exist...\"\n",
    "bq mk --dataset \\\n",
    "    --description \"Dataset for movie vector processing\" \\\n",
    "    ${PROJECT_ID}:your_dataset\n",
    "\n",
    "# Clean up temporary directory\n",
    "rm -rf $TEMP_DIR\n",
    "\n",
    "echo \"Deployment complete! Please check the Cloud Composer UI to verify the DAG is running correctly.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
