{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Search and Summary API with FastAPI\n",
    "\n",
    "This code implements a semantic search and summarization API for movie data using **FastAPI**, leveraging vector embeddings, Pinecone, and BigQuery. Here's an overview of the main components and how they work:\n",
    "\n",
    "## Overview\n",
    "\n",
    "### 1. Dependencies and Initialization\n",
    "- **FastAPI**: The API framework used to manage requests.\n",
    "- **Pinecone**: Used as a vector database for semantic search.\n",
    "- **OpenAI**: Provides text embeddings to represent movie data semantically.\n",
    "- **BigQuery**: Stores movie metadata and descriptions.\n",
    "- **Prometheus**: Used for metrics collection and monitoring, providing insights into API performance.\n",
    "\n",
    "### 2. API Features\n",
    "\n",
    "#### 2.1 Search and Summarize Endpoint (`/search`)\n",
    "- **Input**: Takes a search query (`query`), number of results (`top_k`), and a minimum similarity score (`min_score`).\n",
    "- **Workflow**:\n",
    "  - Generates an embedding of the search query using OpenAI.\n",
    "  - Searches the **Pinecone** index for similar vectors.\n",
    "  - Retrieves movie descriptions from **BigQuery** based on the search results.\n",
    "  - Generates a summary of relevant movies using OpenAI, based on the movie descriptions.\n",
    "\n",
    "#### 2.2 Health Check Endpoint (`/health`)\n",
    "- A health endpoint that checks connectivity to **Pinecone**, **BigQuery**, and **OpenAI** to determine if the API services are running properly.\n",
    "\n",
    "### 3. Key Components\n",
    "\n",
    "#### 3.1 Lifespan Management\n",
    "- The `@asynccontextmanager` (`lifespan`) handles the startup and shutdown activities:\n",
    "  - Initializes Pinecone, OpenAI, and BigQuery clients at startup.\n",
    "  - Performs cleanup during shutdown.\n",
    "\n",
    "#### 3.2 Middleware\n",
    "- **Logging and Metrics Middleware**:\n",
    "  - Logs request details and response times.\n",
    "  - Collects metrics for request count, latency, and endpoint-specific latencies for Prometheus monitoring.\n",
    "\n",
    "#### 3.3 Core Functions\n",
    "- **`get_embedding()`**: Generates an embedding of the given text using OpenAI's API. The resulting vector is used to find similar items.\n",
    "- **`search_pinecone()`**: Queries the Pinecone index using the embedding, retrieving relevant matches.\n",
    "- **`get_full_text_from_bigquery()`**: Uses BigQuery to retrieve movie descriptions by their IDs.\n",
    "- **`generate_summary()`**: Uses OpenAI's chat model to generate a summary based on the movie descriptions retrieved.\n",
    "\n",
    "### 4. Prometheus Metrics\n",
    "- **Total HTTP Requests** (`http_requests_total`): Counts total requests by method, endpoint, and status.\n",
    "- **HTTP Request Latency** (`http_request_duration_seconds`): Measures latency of each request.\n",
    "- **Embedding, Search, and Summary Latency Metrics**: Measures specific latencies for key API operations (`embedding_generation_seconds`, `vector_search_seconds`, `summary_generation_seconds`).\n",
    "\n",
    "### 5. API Workflow\n",
    "\n",
    "- **Input Handling**: Takes a search query to look for related movies.\n",
    "- **Query Processing**:\n",
    "  - Generates an embedding for the query.\n",
    "  - Searches for similar movie vectors using **Pinecone**.\n",
    "  - Fetches detailed movie descriptions from **BigQuery**.\n",
    "  - Generates a summary using OpenAI.\n",
    "- **Output**: Returns a summary along with related movie data.\n",
    "\n",
    "### 6. Deployment\n",
    "- **CORS Middleware**: Configured to allow requests from all origins (`allow_origins=[\"*\"]`), which should be modified for security in a production environment.\n",
    "- **Metrics Endpoint (`/metrics`)**: Provides Prometheus metrics for monitoring the API.\n",
    "\n",
    "### Example Usage\n",
    "1. **Search Request (`POST /search`)**:\n",
    "   - Accepts a `query` string to find semantically similar movie data.\n",
    "   - Returns a summary with a list of matched movies.\n",
    "  \n",
    "2. **Health Check (`GET /health`)**:\n",
    "   - Ensures all dependent services are up and running.\n",
    "\n",
    "### Running the Application\n",
    "To run this API:\n",
    "```bash\n",
    "uvicorn main:app --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "### Notes\n",
    "- **Logging**: Structured logging using `structlog` for better traceability.\n",
    "- **Error Handling**: Each function has error handling that logs the issue and raises an appropriate HTTP exception.\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import pinecone\n",
    "from openai import OpenAI\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from contextlib import asynccontextmanager\n",
    "import time\n",
    "import logging\n",
    "from prometheus_client import Counter, Histogram, make_asgi_app\n",
    "import structlog\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure structured logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "# Prometheus metrics\n",
    "REQUESTS = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])\n",
    "LATENCY = Histogram('http_request_duration_seconds', 'HTTP request latency', ['endpoint'])\n",
    "EMBEDDING_LATENCY = Histogram('embedding_generation_seconds', 'Embedding generation latency')\n",
    "SEARCH_LATENCY = Histogram('vector_search_seconds', 'Vector search latency')\n",
    "SUMMARY_LATENCY = Histogram('summary_generation_seconds', 'Summary generation latency')\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str\n",
    "    top_k: int = 5\n",
    "    min_score: float = 0.7\n",
    "\n",
    "class MovieSummary(BaseModel):\n",
    "    query: str\n",
    "    movies: List[dict]\n",
    "    summary: str\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Startup and shutdown events handler\"\"\"\n",
    "    logger.info(\"application_startup\", message=\"Initializing services\")\n",
    "    try:\n",
    "        # Initialize Pinecone\n",
    "        pinecone.init(\n",
    "            api_key=os.getenv('PINECONE_API_KEY'),\n",
    "            environment=os.getenv('PINECONE_ENV')\n",
    "        )\n",
    "        app.state.pinecone_index = pinecone.Index(os.getenv('PINECONE_INDEX_NAME'))\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        app.state.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        \n",
    "        # Initialize BigQuery client\n",
    "        app.state.bq_client = bigquery.Client(project=os.getenv('GCP_PROJECT_ID'))\n",
    "        \n",
    "        logger.info(\"application_startup_success\", message=\"Services initialized successfully\")\n",
    "        yield\n",
    "    except Exception as e:\n",
    "        logger.error(\"application_startup_failed\", error=str(e))\n",
    "        raise\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        logger.info(\"application_shutdown\", message=\"Shutting down services\")\n",
    "        if hasattr(app.state, 'bq_client'):\n",
    "            app.state.bq_client.close()\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Movie Search and Summary API\",\n",
    "    description=\"API for semantic search and summarization of movie data\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Configure appropriately for production\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Add prometheus metrics endpoint\n",
    "metrics_app = make_asgi_app()\n",
    "app.mount(\"/metrics\", metrics_app)\n",
    "\n",
    "@app.middleware(\"http\")\n",
    "async def add_logging_and_metrics(request: Request, call_next):\n",
    "    \"\"\"Middleware for logging and metrics collection\"\"\"\n",
    "    start_time = time.time()\n",
    "    request_id = str(time.time())\n",
    "    \n",
    "    logger.info(\n",
    "        \"request_started\",\n",
    "        request_id=request_id,\n",
    "        method=request.method,\n",
    "        url=str(request.url),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = await call_next(request)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        REQUESTS.labels(\n",
    "            method=request.method,\n",
    "            endpoint=request.url.path,\n",
    "            status=response.status_code\n",
    "        ).inc()\n",
    "        LATENCY.labels(endpoint=request.url.path).observe(duration)\n",
    "        \n",
    "        logger.info(\n",
    "            \"request_completed\",\n",
    "            request_id=request_id,\n",
    "            duration=duration,\n",
    "            status_code=response.status_code\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"request_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__\n",
    "        )\n",
    "        raise\n",
    "\n",
    "def get_embedding(text: str, request_id: str = None) -> List[float]:\n",
    "    \"\"\"Generate embedding for the input text\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\"generating_embedding\", request_id=request_id, text_length=len(text))\n",
    "    \n",
    "    try:\n",
    "        response = app.state.openai_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        EMBEDDING_LATENCY.observe(duration)\n",
    "        \n",
    "        logger.info(\n",
    "            \"embedding_generated\",\n",
    "            request_id=request_id,\n",
    "            duration=duration\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"embedding_generation_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__\n",
    "        )\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error generating embedding: {str(e)}\"\n",
    "        )\n",
    "\n",
    "def search_pinecone(vector: List[float], top_k: int, min_score: float, request_id: str = None) -> List[tuple]:\n",
    "    \"\"\"Search Pinecone index for similar vectors\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\n",
    "        \"searching_pinecone\",\n",
    "        request_id=request_id,\n",
    "        top_k=top_k,\n",
    "        min_score=min_score\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = app.state.pinecone_index.query(\n",
    "            vector=vector,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        SEARCH_LATENCY.observe(duration)\n",
    "        \n",
    "        filtered_results = [\n",
    "            (match.id, match.score)\n",
    "            for match in results.matches\n",
    "            if match.score >= min_score\n",
    "        ]\n",
    "        \n",
    "        logger.info(\n",
    "            \"pinecone_search_completed\",\n",
    "            request_id=request_id,\n",
    "            duration=duration,\n",
    "            results_count=len(filtered_results)\n",
    "        )\n",
    "        return filtered_results\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"pinecone_search_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__\n",
    "        )\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error searching Pinecone: {str(e)}\"\n",
    "        )\n",
    "\n",
    "def get_full_text_from_bigquery(ids: List[str], request_id: str = None) -> List[dict]:\n",
    "    \"\"\"Retrieve full text from BigQuery for given IDs\"\"\"\n",
    "    logger.info(\n",
    "        \"querying_bigquery\",\n",
    "        request_id=request_id,\n",
    "        ids_count=len(ids)\n",
    "    )\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT id, full_text\n",
    "    FROM `{os.getenv('GCP_PROJECT_ID')}.{os.getenv('BQ_DATASET')}.full_text`\n",
    "    WHERE id IN UNNEST(@ids)\n",
    "    \"\"\"\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "        query_parameters=[\n",
    "            bigquery.ArrayParameter(\"ids\", \"STRING\", ids)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        results = app.state.bq_client.query(query, job_config=job_config).result()\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        results_list = [dict(row) for row in results]\n",
    "        \n",
    "        logger.info(\n",
    "            \"bigquery_query_completed\",\n",
    "            request_id=request_id,\n",
    "            duration=duration,\n",
    "            results_count=len(results_list)\n",
    "        )\n",
    "        return results_list\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"bigquery_query_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__\n",
    "        )\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error querying BigQuery: {str(e)}\"\n",
    "        )\n",
    "\n",
    "def generate_summary(query: str, texts: List[dict], request_id: str = None) -> str:\n",
    "    \"\"\"Generate summary using OpenAI\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\n",
    "        \"generating_summary\",\n",
    "        request_id=request_id,\n",
    "        query=query,\n",
    "        texts_count=len(texts)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following movie descriptions, provide a brief summary that addresses this search query: \"{query}\"\n",
    "    \n",
    "Movie descriptions:\n",
    "{chr(10).join([f'- {text[\"full_text\"]}' for text in texts])}\n",
    "\n",
    "Please provide a concise summary that highlights the most relevant aspects related to the search query.\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = app.state.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo-preview\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides concise and relevant summaries of movie information.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        SUMMARY_LATENCY.observe(duration)\n",
    "        \n",
    "        summary = response.choices[0].message.content\n",
    "        \n",
    "        logger.info(\n",
    "            \"summary_generated\",\n",
    "            request_id=request_id,\n",
    "            duration=duration,\n",
    "            summary_length=len(summary)\n",
    "        )\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"summary_generation_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__\n",
    "        )\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error generating summary: {str(e)}\"\n",
    "        )\n",
    "\n",
    "@app.post(\"/search\", response_model=MovieSummary)\n",
    "async def search_and_summarize(request: SearchRequest, req: Request):\n",
    "    \"\"\"Endpoint to search for similar movies and generate a summary\"\"\"\n",
    "    request_id = str(time.time())\n",
    "    logger.info(\n",
    "        \"search_request_received\",\n",
    "        request_id=request_id,\n",
    "        query=request.query,\n",
    "        top_k=request.top_k\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Generate embedding for query\n",
    "        query_embedding = get_embedding(request.query, request_id)\n",
    "        \n",
    "        # Search Pinecone\n",
    "        similar_vectors = search_pinecone(\n",
    "            vector=query_embedding,\n",
    "            top_k=request.top_k,\n",
    "            min_score=request.min_score,\n",
    "            request_id=request_id\n",
    "        )\n",
    "        \n",
    "        if not similar_vectors:\n",
    "            logger.info(\n",
    "                \"no_results_found\",\n",
    "                request_id=request_id,\n",
    "                query=request.query\n",
    "            )\n",
    "            return MovieSummary(\n",
    "                query=request.query,\n",
    "                movies=[],\n",
    "                summary=\"No relevant movies found for your query.\"\n",
    "            )\n",
    "        \n",
    "        # Get IDs and scores\n",
    "        ids = [id for id, _ in similar_vectors]\n",
    "        scores = {id: score for id, score in similar_vectors}\n",
    "        \n",
    "        # Get full text from BigQuery\n",
    "        movie_texts = get_full_text_from_bigquery(ids, request_id)\n",
    "        \n",
    "        # Add similarity scores to movie data\n",
    "        for movie in movie_texts:\n",
    "            movie['similarity_score'] = scores.get(movie['id'], 0)\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        movie_texts.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = generate_summary(request.query, movie_texts, request_id)\n",
    "        \n",
    "        logger.info(\n",
    "            \"search_request_completed\",\n",
    "            request_id=request_id,\n",
    "            movies_count=len(movie_texts)\n",
    "        )\n",
    "        \n",
    "        return MovieSummary(\n",
    "            query=request.query,\n",
    "            movies=movie_texts,\n",
    "            summary=summary\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"search_request_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__,\n",
    "            query=request.query\n",
    "        )\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Verify all services are connected\n",
    "        _ = app.state.pinecone_index.describe_index_stats()\n",
    "        _ = app.state.bq_client.list_datasets()\n",
    "        _ = app.state.openai_client.models.list()\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"services\": {\n",
    "                \"pinecone\": \"connected\",\n",
    "                \"bigquery\": \"connected\",\n",
    "                \"openai\": \"connected\"\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(\"health_check_failed\", error=str(e))\n",
    "        return {\n",
    "            \"status\": \"unhealthy\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dockerfile for FastAPI Application\n",
    "\n",
    "This Dockerfile creates a container to run a **FastAPI** application using **Python 3.9 slim**.\n",
    "\n",
    "1. **Base Image**: Uses `python:3.9-slim` for a lightweight Python environment.\n",
    "2. **Set Working Directory**: Sets `/app` as the working directory.\n",
    "3. **Install Dependencies**:\n",
    "   - Uses `apt-get` to install system-level tools (`build-essential`) needed for Python packages that require compilation.\n",
    "4. **Copy and Install Python Requirements**:\n",
    "   - Copies `requirements.txt` and installs dependencies using `pip`.\n",
    "5. **Copy Application Code**: Copies all app files to the container.\n",
    "6. **Expose Port**: Opens port `8000` for the app.\n",
    "7. **Run the App**: Uses `uvicorn` to start the FastAPI app on host `0.0.0.0`, port `8000`.\n",
    "\n",
    "This Dockerfile is lightweight, efficient, and well-suited for deploying FastAPI apps in a containerized environment.\n",
    "\n",
    "--- \n",
    "\n",
    "```bash\n",
    "# Use Python 3.9 slim image\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements first to leverage Docker cache\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Command to run the application\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastAPI Movie Search API Request Summary\n",
    "\n",
    "To search for movies using the **Movie Search and Summary API**, use the Python `requests` library to send a **POST** request to the `/search` endpoint.\n",
    "\n",
    "### Example Request\n",
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://api_url:8000/search\",\n",
    "    json={\n",
    "        \"query\": \"adventure movies with magical elements\",\n",
    "        \"top_k\": 5,\n",
    "        \"min_score\": 0.7\n",
    "    }\n",
    ")\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "### Key Parameters\n",
    "- **`query`**: The search phrase (e.g., `\"adventure movies with magical elements\"`).\n",
    "- **`top_k`**: Number of top results to return.\n",
    "- **`min_score`**: Minimum similarity score (between `0` and `1`).\n",
    "\n",
    "### Expected Output\n",
    "- **Movies** matching the query, with metadata and similarity scores.\n",
    "- A **summary** of relevant movies.\n",
    "\n",
    "This example provides a straightforward way to send a search request and get results including a summary of related movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying FastAPI Application to Kubernetes\n",
    "\n",
    "This configuration deploys the **Movie Search and Summary API** to a Kubernetes cluster using various Kubernetes resources:\n",
    "\n",
    "### 1. **ConfigMap (`configmap.yaml`)**:\n",
    "- Stores environment configuration, such as `GCP_PROJECT_ID`, `BQ_DATASET`, and structured logging settings.\n",
    "  \n",
    "### 2. **Secret (`secret.yaml`)**:\n",
    "- Contains sensitive data like `OPENAI_API_KEY` and `PINECONE_API_KEY` encoded in base64.\n",
    "\n",
    "### 3. **Deployment (`deployment.yaml`)**:\n",
    "- **Deployment of API**:\n",
    "  - Runs `movie-search-api` with 3 replicas.\n",
    "  - Uses environment variables from the ConfigMap and Secret.\n",
    "  - Includes health and readiness probes for monitoring the health of the service.\n",
    "  - **Logging**:\n",
    "    - Uses **Fluent Bit** as a sidecar for log processing and sending logs to **Google Stackdriver**.\n",
    "  - **Resource Management**: Requests and limits for CPU and memory are defined.\n",
    "\n",
    "### 4. **Fluent Bit Config (`fluent-bit-config.yaml`)**:\n",
    "- Configures Fluent Bit to collect container logs, enrich them with Kubernetes metadata, and send them to **Google Stackdriver**.\n",
    "\n",
    "### 5. **Service (`service.yaml`)**:\n",
    "- Defines a **LoadBalancer** service named `movie-search-service` to expose the application externally on port `80` (mapped to port `8000` in the container).\n",
    "\n",
    "### 6. **Horizontal Pod Autoscaler (`hpa.yaml`)**:\n",
    "- Scales the number of pods between **3** and **10** based on **CPU utilization** (targeting 70%).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Deploying to kubernetes\n",
    "```yaml\n",
    "# config/configmap.yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: movie-search-config\n",
    "data:\n",
    "  GCP_PROJECT_ID: \"your-project-id\"\n",
    "  BQ_DATASET: \"your-dataset\"\n",
    "  PINECONE_ENV: \"your-pinecone-env\"\n",
    "  PINECONE_INDEX_NAME: \"your-index-name\"\n",
    "  LOG_LEVEL: \"INFO\"\n",
    "  # Structured logging configuration\n",
    "  LOGGING_CONFIG: |\n",
    "    {\n",
    "      \"version\": 1,\n",
    "      \"disable_existing_loggers\": false,\n",
    "      \"formatters\": {\n",
    "        \"json\": {\n",
    "          \"format\": \"%(levelname)s %(asctime)s %(name)s %(message)s\",\n",
    "          \"datefmt\": \"%Y-%m-%d %H:%M:%S\",\n",
    "          \"class\": \"pythonjsonlogger.jsonlogger.JsonFormatter\"\n",
    "        }\n",
    "      },\n",
    "      \"handlers\": {\n",
    "        \"console\": {\n",
    "          \"class\": \"logging.StreamHandler\",\n",
    "          \"formatter\": \"json\",\n",
    "          \"stream\": \"ext://sys.stdout\"\n",
    "        }\n",
    "      },\n",
    "      \"root\": {\n",
    "        \"level\": \"INFO\",\n",
    "        \"handlers\": [\"console\"]\n",
    "      }\n",
    "    }\n",
    "---\n",
    "# config/secret.yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: movie-search-secrets\n",
    "type: Opaque\n",
    "data:\n",
    "  OPENAI_API_KEY: \"base64-encoded-key\"\n",
    "  PINECONE_API_KEY: \"base64-encoded-key\"\n",
    "---\n",
    "# config/deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: movie-search-api\n",
    "  labels:\n",
    "    app: movie-search-api\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: movie-search-api\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: movie-search-api\n",
    "      annotations:\n",
    "        # Enable GCP Cloud Logging\n",
    "        logging.cloud.google.com/agent: '{\"plugins\":[\"opentelemetry\",\"prometheus\",\"application\"]}'\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: movie-search-api\n",
    "        image: gcr.io/your-project-id/movie-search-api:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"250m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "        env:\n",
    "        # Add trace context to logs\n",
    "        - name: OTEL_SERVICE_NAME\n",
    "          value: \"movie-search-api\"\n",
    "        - name: OTEL_PROPAGATORS\n",
    "          value: \"tracecontext,baggage\"\n",
    "        # Add pod metadata to logs\n",
    "        - name: POD_NAME\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              fieldPath: metadata.name\n",
    "        - name: NAMESPACE\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              fieldPath: metadata.namespace\n",
    "        envFrom:\n",
    "        - configMapRef:\n",
    "            name: movie-search-config\n",
    "        - secretRef:\n",
    "            name: movie-search-secrets\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 10\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 15\n",
    "          periodSeconds: 20\n",
    "        # Mount fluentbit config for log processing\n",
    "        volumeMounts:\n",
    "        - name: varlog\n",
    "          mountPath: /var/log\n",
    "        - name: varlibdockercontainers\n",
    "          mountPath: /var/lib/docker/containers\n",
    "          readOnly: true\n",
    "      # Sidecar container for log collection\n",
    "      - name: fluent-bit\n",
    "        image: fluent/fluent-bit:latest\n",
    "        volumeMounts:\n",
    "        - name: varlog\n",
    "          mountPath: /var/log\n",
    "        - name: varlibdockercontainers\n",
    "          mountPath: /var/lib/docker/containers\n",
    "          readOnly: true\n",
    "        - name: fluent-bit-config\n",
    "          mountPath: /fluent-bit/etc/\n",
    "      volumes:\n",
    "      - name: varlog\n",
    "        emptyDir: {}\n",
    "      - name: varlibdockercontainers\n",
    "        hostPath:\n",
    "          path: /var/lib/docker/containers\n",
    "      - name: fluent-bit-config\n",
    "        configMap:\n",
    "          name: fluent-bit-config\n",
    "---\n",
    "# config/fluent-bit-config.yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: fluent-bit-config\n",
    "data:\n",
    "  fluent-bit.conf: |\n",
    "    [SERVICE]\n",
    "        Flush         1\n",
    "        Log_Level     info\n",
    "        Daemon        off\n",
    "        Parsers_File  parsers.conf\n",
    "\n",
    "    [INPUT]\n",
    "        Name             tail\n",
    "        Path             /var/log/containers/*.log\n",
    "        Parser           docker\n",
    "        Tag              kube.*\n",
    "        Mem_Buf_Limit    5MB\n",
    "        Skip_Long_Lines  On\n",
    "\n",
    "    [FILTER]\n",
    "        Name                kubernetes\n",
    "        Match               kube.*\n",
    "        Kube_URL           https://kubernetes.default.svc:443\n",
    "        Merge_Log          On\n",
    "        K8S-Logging.Parser On\n",
    "\n",
    "    [OUTPUT]\n",
    "        Name            stackdriver\n",
    "        Match           *\n",
    "        resource        k8s_container\n",
    "---\n",
    "# config/service.yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: movie-search-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: movie-search-api\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "---\n",
    "# config/hpa.yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: movie-search-api-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: movie-search-api\n",
    "  minReplicas: 3\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GKE Cluster Setup Script \n",
    "\n",
    "1. **Set Environment**: Define project ID, cluster name, and region.\n",
    "2. **Create GKE Cluster**: Create a cluster named `movie-search` with autoscaling (3-10 nodes), multi-zone setup, logging, and monitoring.\n",
    "3. **Get Credentials**: Allow `kubectl` to manage the cluster.\n",
    "4. **Namespace and Service Account**: Create namespace and IAM service account (`movie-search-sa`) for workloads.\n",
    "5. **Grant Permissions**: Allow the service account to access BigQuery (`dataViewer`) and log services (`logWriter`).\n",
    "6. **Verify**: Ensure nodes are ready with `kubectl get nodes`.\n",
    "\n",
    "This script sets up a scalable, monitored GKE cluster with proper permissions for deploying the Movie Search application.\n",
    "\n",
    "--- \n",
    "\n",
    "```bash\n",
    "# Set environment variables\n",
    "export PROJECT_ID=your-project-id\n",
    "export CLUSTER_NAME=movie-search\n",
    "export REGION=us-central1\n",
    "\n",
    "# Set project\n",
    "gcloud config set project $PROJECT_ID\n",
    "\n",
    "# Create cluster with essential features\n",
    "gcloud container clusters create $CLUSTER_NAME \\\n",
    "    --region $REGION \\\n",
    "    --num-nodes 3 \\\n",
    "    --machine-type e2-standard-2 \\\n",
    "    --enable-autoscaling \\\n",
    "    --min-nodes 3 \\\n",
    "    --max-nodes 10 \\\n",
    "    --node-locations $REGION-a,$REGION-b,$REGION-c \\\n",
    "    --logging=SYSTEM,WORKLOAD \\\n",
    "    --monitoring=SYSTEM \\\n",
    "    --enable-ip-alias \\\n",
    "    --workload-pool=${PROJECT_ID}.svc.id.goog \\\n",
    "    --labels=app=movie-search\n",
    "\n",
    "# Get credentials\n",
    "gcloud container clusters get-credentials $CLUSTER_NAME --region $REGION\n",
    "\n",
    "# Create namespace and service account\n",
    "kubectl create namespace movie-search\n",
    "\n",
    "# Create service account for workload identity\n",
    "gcloud iam service-accounts create movie-search-sa \\\n",
    "    --display-name=\"Movie Search Service Account\"\n",
    "\n",
    "# Grant permissions\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=\"serviceAccount:movie-search-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n",
    "    --role=\"roles/bigquery.dataViewer\"\n",
    "\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=\"serviceAccount:movie-search-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n",
    "    --role=\"roles/logging.logWriter\"\n",
    "\n",
    "# Verify setup\n",
    "kubectl get nodes\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "# Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set environment variables\n",
    "ENV PYTHONUNBUFFERED=1 \\\n",
    "    PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PIP_NO_CACHE_DIR=1 \\\n",
    "    PIP_DISABLE_PIP_VERSION_CHECK=1\n",
    "\n",
    "# Create non-root user\n",
    "RUN useradd -m -s /bin/bash app\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install dependencies first\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Set ownership and permissions\n",
    "RUN chown -R app:app /app\n",
    "\n",
    "# Switch to non-root user\n",
    "USER app\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=3s \\\n",
    "  CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"4\"]\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Set environment variables\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export IMAGE_NAME=\"movie-search-api\"\n",
    "export IMAGE_TAG=$(git rev-parse --short HEAD 2>/dev/null || echo \"latest\")\n",
    "\n",
    "echo \"Building and pushing image to GCR...\"\n",
    "\n",
    "# Build the image\n",
    "docker build -t gcr.io/$PROJECT_ID/$IMAGE_NAME:$IMAGE_TAG \\\n",
    "            -t gcr.io/$PROJECT_ID/$IMAGE_NAME:latest .\n",
    "\n",
    "# Push the images\n",
    "docker push gcr.io/$PROJECT_ID/$IMAGE_NAME:$IMAGE_TAG\n",
    "docker push gcr.io/$PROJECT_ID/$IMAGE_NAME:latest\n",
    "\n",
    "echo \"Successfully built and pushed: gcr.io/$PROJECT_ID/$IMAGE_NAME:$IMAGE_TAG\"\n",
    "\n",
    "# Update Kubernetes deployment if needed\n",
    "read -p \"Do you want to update the Kubernetes deployment? (y/n) \" -n 1 -r\n",
    "echo\n",
    "if [[ $REPLY =~ ^[Yy]$ ]]\n",
    "then\n",
    "    kubectl set image deployment/movie-search-api \\\n",
    "            movie-search-api=gcr.io/$PROJECT_ID/$IMAGE_NAME:$IMAGE_TAG \\\n",
    "            -n movie-search\n",
    "    echo \"Deployment updated successfully!\"\n",
    "fi\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubernetes Monitoring Configuration\n",
    "\n",
    "This configuration deploys **Prometheus** and **Grafana** for monitoring the **Movie Search API** in a Kubernetes environment.\n",
    "\n",
    "### 1. **Prometheus and Grafana Values (`prometheus-values.yaml`)**:\n",
    "\n",
    "- **Grafana**:\n",
    "  - **Admin Password**: Sets a secure admin password.\n",
    "  - **Persistence**: Enables persistent storage of **10Gi** for dashboards and data.\n",
    "  - **Dashboard Configuration**: Preloads an \"API Monitoring\" dashboard, visualizing metrics such as the **Request Rate** (`rate(search_requests_total[5m])`).\n",
    "\n",
    "- **Prometheus**:\n",
    "  - **Retention**: Data retention of **15 days**.\n",
    "  - **Resources**: Defines resource requests (`512Mi` memory, `500m` CPU) and limits (`2Gi` memory, `1000m` CPU).\n",
    "  - **Storage**: Configures **50Gi** persistent storage.\n",
    "\n",
    "- **Alertmanager**:\n",
    "  - **Enabled**: Configured with a **Slack** integration for alert notifications.\n",
    "  - **Alert Settings**: Alerts are grouped by `job` with a repeat interval of **12 hours** for critical alerts.\n",
    "\n",
    "### 2. **ServiceMonitor (`service-monitor.yaml`)**:\n",
    "\n",
    "- **ServiceMonitor**:\n",
    "  - Monitors the **Movie Search API** for metrics.\n",
    "  - Collects metrics every **15 seconds** from the `/metrics` endpoint.\n",
    "\n",
    "### 3. **Prometheus Alerts (`prometheus-rules.yaml`)**:\n",
    "\n",
    "- **Alerts Configuration**:\n",
    "  - **HighErrorRate**: Triggers if the HTTP error rate (`5xx`) exceeds **5%** for **5 minutes**.\n",
    "  - **HighLatency**: Alerts if **95th percentile latency** is above **2 seconds** for **5 minutes**.\n",
    "  - **HighCPUUsage**: Warns when **CPU usage** exceeds **80%** of the container's CPU quota for **15 minutes**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "```bash\n",
    "# monitoring/prometheus-values.yaml\n",
    "grafana:\n",
    "  adminPassword: \"your-secure-password\"\n",
    "  persistence:\n",
    "    enabled: true\n",
    "    size: 10Gi\n",
    "  dashboardProviders:\n",
    "    dashboardproviders.yaml:\n",
    "      apiVersion: 1\n",
    "      providers:\n",
    "      - name: 'default'\n",
    "        orgId: 1\n",
    "        folder: ''\n",
    "        type: file\n",
    "        disableDeletion: false\n",
    "        editable: true\n",
    "        options:\n",
    "          path: /var/lib/grafana/dashboards\n",
    "  dashboards:\n",
    "    default:\n",
    "      api-monitoring:\n",
    "        json: |\n",
    "          {\n",
    "            \"annotations\": {\n",
    "              \"list\": []\n",
    "            },\n",
    "            \"editable\": true,\n",
    "            \"fiscalYearStartMonth\": 0,\n",
    "            \"graphTooltip\": 0,\n",
    "            \"links\": [],\n",
    "            \"liveNow\": false,\n",
    "            \"panels\": [\n",
    "              {\n",
    "                \"datasource\": {\n",
    "                  \"type\": \"prometheus\",\n",
    "                  \"uid\": \"prometheus\"\n",
    "                },\n",
    "                \"fieldConfig\": {\n",
    "                  \"defaults\": {\n",
    "                    \"color\": {\n",
    "                      \"mode\": \"palette-classic\"\n",
    "                    },\n",
    "                    \"custom\": {\n",
    "                      \"axisCenteredZero\": false,\n",
    "                      \"axisColorMode\": \"text\",\n",
    "                      \"axisLabel\": \"\",\n",
    "                      \"axisPlacement\": \"auto\",\n",
    "                      \"barAlignment\": 0,\n",
    "                      \"drawStyle\": \"line\",\n",
    "                      \"fillOpacity\": 10,\n",
    "                      \"gradientMode\": \"none\",\n",
    "                      \"hideFrom\": {\n",
    "                        \"legend\": false,\n",
    "                        \"tooltip\": false,\n",
    "                        \"viz\": false\n",
    "                      },\n",
    "                      \"lineInterpolation\": \"linear\",\n",
    "                      \"lineWidth\": 1,\n",
    "                      \"pointSize\": 5,\n",
    "                      \"scaleDistribution\": {\n",
    "                        \"type\": \"linear\"\n",
    "                      },\n",
    "                      \"showPoints\": \"never\",\n",
    "                      \"spanNulls\": false,\n",
    "                      \"stacking\": {\n",
    "                        \"group\": \"A\",\n",
    "                        \"mode\": \"none\"\n",
    "                      },\n",
    "                      \"thresholdsStyle\": {\n",
    "                        \"mode\": \"off\"\n",
    "                      }\n",
    "                    },\n",
    "                    \"mappings\": [],\n",
    "                    \"thresholds\": {\n",
    "                      \"mode\": \"absolute\",\n",
    "                      \"steps\": [\n",
    "                        {\n",
    "                          \"color\": \"green\",\n",
    "                          \"value\": null\n",
    "                        }\n",
    "                      ]\n",
    "                    },\n",
    "                    \"unit\": \"short\"\n",
    "                  },\n",
    "                  \"overrides\": []\n",
    "                },\n",
    "                \"gridPos\": {\n",
    "                  \"h\": 8,\n",
    "                  \"w\": 12,\n",
    "                  \"x\": 0,\n",
    "                  \"y\": 0\n",
    "                },\n",
    "                \"id\": 1,\n",
    "                \"options\": {\n",
    "                  \"legend\": {\n",
    "                    \"calcs\": [],\n",
    "                    \"displayMode\": \"list\",\n",
    "                    \"placement\": \"bottom\",\n",
    "                    \"showLegend\": true\n",
    "                  },\n",
    "                  \"tooltip\": {\n",
    "                    \"mode\": \"single\",\n",
    "                    \"sort\": \"none\"\n",
    "                  }\n",
    "                },\n",
    "                \"targets\": [\n",
    "                  {\n",
    "                    \"datasource\": {\n",
    "                      \"type\": \"prometheus\",\n",
    "                      \"uid\": \"prometheus\"\n",
    "                    },\n",
    "                    \"expr\": \"rate(search_requests_total[5m])\",\n",
    "                    \"refId\": \"A\"\n",
    "                  }\n",
    "                ],\n",
    "                \"title\": \"Request Rate\",\n",
    "                \"type\": \"timeseries\"\n",
    "              }\n",
    "            ],\n",
    "            \"refresh\": \"5s\",\n",
    "            \"schemaVersion\": 38,\n",
    "            \"style\": \"dark\",\n",
    "            \"tags\": [],\n",
    "            \"templating\": {\n",
    "              \"list\": []\n",
    "            },\n",
    "            \"time\": {\n",
    "              \"from\": \"now-1h\",\n",
    "              \"to\": \"now\"\n",
    "            },\n",
    "            \"timepicker\": {},\n",
    "            \"timezone\": \"\",\n",
    "            \"title\": \"API Monitoring\",\n",
    "            \"version\": 0,\n",
    "            \"weekStart\": \"\"\n",
    "          }\n",
    "\n",
    "prometheusOperator:\n",
    "  enabled: true\n",
    "  serviceMonitor:\n",
    "    enabled: true\n",
    "\n",
    "prometheus:\n",
    "  prometheusSpec:\n",
    "    retention: 15d\n",
    "    resources:\n",
    "      requests:\n",
    "        memory: 512Mi\n",
    "        cpu: 500m\n",
    "      limits:\n",
    "        memory: 2Gi\n",
    "        cpu: 1000m\n",
    "    storageSpec:\n",
    "      volumeClaimTemplate:\n",
    "        spec:\n",
    "          accessModes: [\"ReadWriteOnce\"]\n",
    "          resources:\n",
    "            requests:\n",
    "              storage: 50Gi\n",
    "\n",
    "alertmanager:\n",
    "  enabled: true\n",
    "  config:\n",
    "    global:\n",
    "      resolve_timeout: 5m\n",
    "    route:\n",
    "      group_by: ['job']\n",
    "      group_wait: 30s\n",
    "      group_interval: 5m\n",
    "      repeat_interval: 12h\n",
    "      receiver: 'slack'\n",
    "      routes:\n",
    "      - match:\n",
    "          severity: critical\n",
    "        receiver: 'slack'\n",
    "    receivers:\n",
    "    - name: 'slack'\n",
    "      slack_configs:\n",
    "      - api_url: 'https://hooks.slack.com/services/your-webhook-url'\n",
    "        channel: '#alerts'\n",
    "        send_resolved: true\n",
    "---\n",
    "# monitoring/service-monitor.yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: movie-search-monitor\n",
    "  labels:\n",
    "    release: prometheus\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: movie-search-api\n",
    "  endpoints:\n",
    "  - port: http\n",
    "    path: /metrics\n",
    "    interval: 15s\n",
    "---\n",
    "# monitoring/prometheus-rules.yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PrometheusRule\n",
    "metadata:\n",
    "  name: movie-search-alerts\n",
    "  labels:\n",
    "    release: prometheus\n",
    "spec:\n",
    "  groups:\n",
    "  - name: movie-search\n",
    "    rules:\n",
    "    - alert: HighErrorRate\n",
    "      expr: |\n",
    "        sum(rate(http_requests_total{status=~\"5..\"}[5m])) \n",
    "        / \n",
    "        sum(rate(http_requests_total[5m])) \n",
    "        > 0.05\n",
    "      for: 5m\n",
    "      labels:\n",
    "        severity: critical\n",
    "      annotations:\n",
    "        summary: High error rate detected\n",
    "        description: Error rate is above 5% for more than 5 minutes\n",
    "    \n",
    "    - alert: HighLatency\n",
    "      expr: |\n",
    "        histogram_quantile(0.95, sum(rate(search_latency_seconds_bucket[5m])) by (le)) \n",
    "        > 2\n",
    "      for: 5m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: High latency detected\n",
    "        description: 95th percentile latency is above 2 seconds\n",
    "    \n",
    "    - alert: HighCPUUsage\n",
    "      expr: |\n",
    "        container_cpu_usage_seconds_total{container=\"movie-search-api\"} \n",
    "        > \n",
    "        container_spec_cpu_quota{container=\"movie-search-api\"} * 0.8\n",
    "      for: 15m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: High CPU usage detected\n",
    "        description: Container is using more than 80% of its CPU quota\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
