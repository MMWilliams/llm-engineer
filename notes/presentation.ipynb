{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Presentation Title:**\n",
    "### **Optimizing Movie Data Processing for LLM Fine-Tuning with Apache Airflow**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction**\n",
    "\n",
    "- **Welcome and Agenda**\n",
    "  - Introduction to the project and its objectives.\n",
    "  - Overview of the data pipeline.\n",
    "  - Detailed walkthrough of the DAG and its components.\n",
    "  - Discussion on design choices, optimizations, and trade-offs.\n",
    "  - Deployment strategy using Google Cloud Composer.\n",
    "  - Q&A session.\n",
    "\n",
    "- **Project Objective**\n",
    "  - To preprocess large volumes of movie data for fine-tuning and training Large Language Models (LLMs).\n",
    "  - Transform raw data into searchable vector embeddings to enhance model performance and retrieval capabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Data Pipeline Overview**\n",
    "\n",
    "### **Pipeline Steps:**\n",
    "\n",
    "1. **Data Ingestion:**\n",
    "   - **Sources:** CSV and JSON files stored in Google Cloud Storage (GCS).\n",
    "   - **Purpose:** Consolidate diverse data formats into a unified dataset for processing.\n",
    "\n",
    "2. **Data Processing:**\n",
    "   - **Tasks:**\n",
    "     - **Text Processing:** Clean and preprocess movie titles and overviews.\n",
    "     - **Embedding Generation:** Utilize OpenAI's `text-embedding-ada-002` model to create vector representations.\n",
    "\n",
    "3. **Vector Storage:**\n",
    "   - **Service:** Pinecone (a vector database).\n",
    "   - **Function:** Store and index embeddings for efficient similarity searches and retrieval.\n",
    "\n",
    "4. **Data Storage:**\n",
    "   - **Destination:** BigQuery tables.\n",
    "   - **Types of Tables:**\n",
    "     - **Full Text Table:** Stores the processed text data.\n",
    "     - **Metadata Table:** Contains metadata related to the processing.\n",
    "     - **Dropped Table:** Logs records that failed processing.\n",
    "\n",
    "### **Visual Diagram:**\n",
    "*(Include a flowchart illustrating the data flow from GCS to BigQuery via preprocessing and embedding generation.)*\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Key Features**\n",
    "\n",
    "### **A. Handling Large Datasets:**\n",
    "- **Chunked Processing:**\n",
    "  - **Reason:** Manage memory efficiently and enable parallelism.\n",
    "  - **Implementation:** Split data into manageable chunks (e.g., 1000 records per chunk).\n",
    "\n",
    "### **B. Parallel Processing:**\n",
    "- **Executors Used:**\n",
    "  - **ThreadPoolExecutor:** For I/O-bound tasks such as API calls to OpenAI and Pinecone.\n",
    "  - **ProcessPoolExecutor:** For CPU-bound tasks like text preprocessing using NLTK.\n",
    "\n",
    "- **Benefits:**\n",
    "  - **Speed:** Reduces overall processing time by leveraging multi-core CPUs.\n",
    "  - **Efficiency:** Maximizes resource utilization without overwhelming the system.\n",
    "\n",
    "### **C. Robust Error Handling and Retries:**\n",
    "- **Mechanism:**\n",
    "  - **Retries:** Configured up to 3 attempts for failed operations.\n",
    "  - **Logging:** Detailed error messages for troubleshooting.\n",
    "\n",
    "- **Purpose:**\n",
    "  - **Reliability:** Ensures transient errors do not halt the entire pipeline.\n",
    "  - **Fault Tolerance:** Maintains pipeline continuity despite intermittent failures.\n",
    "\n",
    "### **D. Text Splitting for Token Limits:**\n",
    "- **Challenge:** OpenAI models have a maximum token limit (e.g., 4096 tokens).\n",
    "- **Solution:** Implement `split_text_by_tokens` to divide long texts into overlapping chunks.\n",
    "\n",
    "- **Benefit:**\n",
    "  - **Completeness:** Maintains context across splits to preserve semantic meaning.\n",
    "  - **Compliance:** Adheres to API constraints, preventing errors due to oversized inputs.\n",
    "\n",
    "### **E. Batch Operations:**\n",
    "- **Embedding Generation:**\n",
    "  - **Batch Size:** Configurable (e.g., 100 texts per batch).\n",
    "  - **Advantage:** Optimizes API calls, reducing overhead and cost.\n",
    "\n",
    "- **Data Uploads:**\n",
    "  - **Parallel Uploads:** Concurrently upload vectors to Pinecone and data to BigQuery.\n",
    "\n",
    "### **F. Distributed Computing with Dask:**\n",
    "- **Usage:**\n",
    "  - **Repartitioning:** Divide the dataset into partitions based on CPU cores.\n",
    "  - **Parallel Execution:** Distribute processing tasks across multiple workers.\n",
    "\n",
    "- **Benefits:**\n",
    "  - **Scalability:** Efficiently handles large-scale data processing.\n",
    "  - **Performance:** Accelerates computations by leveraging distributed resources.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Main Processing Steps**\n",
    "\n",
    "### **A. Text Preprocessing (In Parallel)**\n",
    "- **Tasks:**\n",
    "  - **Tokenization:** Break down text into tokens using NLTK.\n",
    "  - **Stopword Removal:** Eliminate common, non-informative words.\n",
    "  - **Lemmatization:** Reduce words to their base forms.\n",
    "\n",
    "- **Implementation:**\n",
    "  - **ProcessPoolExecutor:** Utilized to parallelize CPU-intensive preprocessing tasks.\n",
    "\n",
    "### **B. Embedding Generation with OpenAI**\n",
    "- **Model Used:** `text-embedding-ada-002`.\n",
    "- **Process:**\n",
    "  - **Batch Processing:** Send batches of preprocessed texts to OpenAI API.\n",
    "  - **Error Handling:** Implement retries with exponential backoff for robustness.\n",
    "\n",
    "### **C. Vector Storage in Pinecone**\n",
    "- **Function:** Store the generated embeddings as vectors.\n",
    "- **Advantages:**\n",
    "  - **Similarity Search:** Enables efficient retrieval based on vector similarity.\n",
    "  - **Scalability:** Designed to handle large-scale vector data.\n",
    "\n",
    "### **D. Data Storage in BigQuery**\n",
    "- **Tables:**\n",
    "  - **Full Text Table:** Stores `id` and `combined_text`.\n",
    "  - **Metadata Table:** Stores processing metadata (`id`, `process_date`, `filename`, `status`, `created_at`).\n",
    "  - **Dropped Table:** Captures records that failed embedding generation.\n",
    "\n",
    "- **Features:**\n",
    "  - **Partitioning:** Based on `created_at` to optimize query performance.\n",
    "  - **Clustering:** On `id` to enhance data retrieval speeds.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Infrastructure**\n",
    "\n",
    "### **A. Apache Airflow**\n",
    "- **Role:** Orchestrates the entire data pipeline as a Directed Acyclic Graph (DAG).\n",
    "- **Features Utilized:**\n",
    "  - **Task Scheduling:** Automates the daily execution of the pipeline.\n",
    "  - **Dependency Management:** Ensures tasks run in the correct order.\n",
    "  - **Monitoring and Logging:** Provides visibility into pipeline execution and failures.\n",
    "\n",
    "### **B. Google Cloud Platform (GCP) Services**\n",
    "- **Google Cloud Storage (GCS):** Stores raw CSV and JSON movie data files.\n",
    "- **BigQuery:** Serves as the data warehouse for storing processed text and metadata.\n",
    "\n",
    "### **C. OpenAI Integration**\n",
    "- **Function:** Generates vector embeddings for preprocessed movie texts.\n",
    "- **Considerations:**\n",
    "  - **API Usage:** Managed through batching and parallel requests to optimize performance and cost.\n",
    "\n",
    "### **D. Pinecone Integration**\n",
    "- **Purpose:** Acts as the vector database for storing and managing embeddings.\n",
    "- **Benefits:**\n",
    "  - **Efficiency:** Designed for high-speed vector searches.\n",
    "  - **Scalability:** Handles growing volumes of vector data seamlessly.\n",
    "\n",
    "### **E. Configuration Management**\n",
    "- **Airflow Variables:**\n",
    "  - **Usage:** Store configuration parameters like GCS bucket names, API keys, project IDs, etc.\n",
    "  - **Advantages:** Centralizes configuration, making the pipeline flexible and easier to manage.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Performance Optimizations**\n",
    "\n",
    "### **A. Dynamic Worker Allocation**\n",
    "- **Strategy:**\n",
    "  - **Calculation:** Set `MAX_WORKERS` based on available CPU cores (`NUM_CORES * 2`, capped at 32).\n",
    "  - **Purpose:** Balances parallelism with resource constraints to prevent overutilization.\n",
    "\n",
    "### **B. Batch Processing with Configurable Chunk Sizes**\n",
    "- **Implementation:**\n",
    "  - **CHUNK_SIZE:** Determines the number of records processed in each chunk.\n",
    "  - **EMBEDDING_BATCH_SIZE:** Controls the number of texts sent per API call to OpenAI.\n",
    "\n",
    "- **Benefits:**\n",
    "  - **Flexibility:** Easily adjust based on dataset size and API rate limits.\n",
    "  - **Efficiency:** Reduces the number of API calls, optimizing cost and performance.\n",
    "\n",
    "### **C. Parallel Uploads to Pinecone and BigQuery**\n",
    "- **Method:**\n",
    "  - **ThreadPoolExecutor:** Utilized to perform uploads concurrently.\n",
    "  \n",
    "- **Advantages:**\n",
    "  - **Speed:** Shortens the total time taken to store embeddings and processed data.\n",
    "  - **Resource Utilization:** Maximizes the use of available network bandwidth and compute resources.\n",
    "\n",
    "### **D. Text Splitting to Handle Token Limits**\n",
    "- **Technique:**\n",
    "  - **Overlap:** Introduces overlapping tokens between chunks to maintain context.\n",
    "  - **Adaptive Splitting:** Adjusts split points based on punctuation and whitespace to preserve semantic meaning.\n",
    "\n",
    "- **Outcome:**\n",
    "  - **Compliance:** Ensures texts conform to OpenAI’s token limits.\n",
    "  - **Quality:** Maintains the integrity of the data by preserving context across splits.\n",
    "\n",
    "### **E. Memory-Efficient Processing**\n",
    "- **Approach:**\n",
    "  - **Dask DataFrames:** Utilize out-of-core computation to handle datasets larger than available memory.\n",
    "  - **Chunked Reads:** Process data in chunks to minimize memory footprint.\n",
    "\n",
    "- **Benefits:**\n",
    "  - **Scalability:** Capable of processing very large datasets without memory overflow.\n",
    "  - **Performance:** Reduces memory swapping and enhances processing speed.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. DAG Characteristics**\n",
    "\n",
    "### **A. Scheduling and Execution**\n",
    "- **Frequency:** Daily runs at midnight (`schedule_interval='0 0 * * *'`).\n",
    "- **Start Date:** October 27, 2024.\n",
    "- **Catchup:** Disabled (`catchup=False`) to prevent backfilling past runs.\n",
    "\n",
    "### **B. Error Handling and Retries**\n",
    "- **Retry Policy:**\n",
    "  - **Retries:** 2 attempts on failure.\n",
    "  - **Retry Delay:** 5 minutes between retries.\n",
    "\n",
    "- **Advantages:**\n",
    "  - **Resilience:** Mitigates transient failures by retrying tasks.\n",
    "  - **Reliability:** Ensures higher success rates for pipeline executions.\n",
    "\n",
    "### **C. Monitoring and Logging**\n",
    "- **Built-In Features:**\n",
    "  - **Airflow UI:** Provides real-time monitoring of DAG runs and task statuses.\n",
    "  - **Logging:** Detailed logs for each task facilitate debugging and performance analysis.\n",
    "\n",
    "### **D. Scalability and Concurrency**\n",
    "- **Concurrency:** Limited to 4 simultaneous DAG runs (`concurrency=4`).\n",
    "- **Pool:** Utilizes a pool named `movie_processing_pool` to manage resource allocation.\n",
    "\n",
    "- **Benefits:**\n",
    "  - **Resource Management:** Prevents overloading the system by controlling the number of concurrent tasks.\n",
    "  - **Scalability:** Allows for scaling up by adjusting pool settings based on resource availability.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Code Explanation**\n",
    "\n",
    "### **A. Imports and Dependencies**\n",
    "- **Standard Libraries:** `os`, `re`, `sys`, `time`, `json`, `logging`, etc.\n",
    "- **Data Processing:** `numpy`, `pandas`, `dask.dataframe`.\n",
    "- **Cloud Services:** `google.cloud.storage`, `google.cloud.bigquery`.\n",
    "- **External Services:** `openai`, `pinecone`.\n",
    "- **Text Processing:** `nltk`, `tiktoken`.\n",
    "- **Parallelism:** `concurrent.futures` (`ThreadPoolExecutor`, `ProcessPoolExecutor`), `multiprocessing`.\n",
    "- **Utilities:** `functools.partial`, `itertools.islice`.\n",
    "\n",
    "### **B. Logging Configuration**\n",
    "```python\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "```\n",
    "- **Purpose:** Set up logging to track pipeline execution and debug issues.\n",
    "\n",
    "### **C. Constants for Batch Processing**\n",
    "```python\n",
    "CHUNK_SIZE = 1000\n",
    "EMBEDDING_BATCH_SIZE = 100\n",
    "MAX_RETRIES = 3\n",
    "NUM_CORES = multiprocessing.cpu_count()\n",
    "MAX_WORKERS = min(32, NUM_CORES * 2)\n",
    "```\n",
    "- **Explanation:**\n",
    "  - **CHUNK_SIZE:** Number of records processed per chunk.\n",
    "  - **EMBEDDING_BATCH_SIZE:** Number of texts per embedding API call.\n",
    "  - **MAX_RETRIES:** Maximum retry attempts for failed operations.\n",
    "  - **NUM_CORES:** Total CPU cores available.\n",
    "  - **MAX_WORKERS:** Limits the number of concurrent workers to prevent resource exhaustion.\n",
    "\n",
    "### **D. Pipeline Configuration Class**\n",
    "```python\n",
    "class PipelineConfig:\n",
    "    def __init__(self):\n",
    "        self.gcs_bucket = Variable.get('GCS_BUCKET')\n",
    "        self.input_path = Variable.get('INPUT_PATH')\n",
    "        self.project_id = Variable.get('GCP_PROJECT_ID')\n",
    "        self.bq_dataset = Variable.get('BQ_DATASET')\n",
    "        self.full_text_table = f\"{self.project_id}.{self.bq_dataset}.full_text\"\n",
    "        self.metadata_table = f\"{self.project_id}.{self.bq_dataset}.metadata\"\n",
    "        self.dropped_table = f\"{self.project_id}.{self.bq_dataset}.dropped\"\n",
    "        self.pinecone_api_key = Variable.get('PINECONE_API_KEY')\n",
    "        self.pinecone_env = Variable.get('PINECONE_ENV')\n",
    "        self.index_name = Variable.get('PINECONE_INDEX_NAME')\n",
    "        self.openai_api_key = Variable.get('OPENAI_API_KEY')\n",
    "        self.num_processes = NUM_CORES\n",
    "```\n",
    "- **Function:**\n",
    "  - Centralizes configuration parameters retrieved from Airflow Variables.\n",
    "  - Enhances maintainability and flexibility by avoiding hard-coded values.\n",
    "\n",
    "### **E. Text Splitting Function**\n",
    "```python\n",
    "def split_text_by_tokens(text: str, encoder, max_tokens: int = 4096, overlap: int = 100) -> List[str]:\n",
    "    # Function implementation...\n",
    "```\n",
    "- **Purpose:**\n",
    "  - Splits lengthy texts into smaller chunks that adhere to OpenAI's token limits.\n",
    "  - Maintains context by overlapping tokens between chunks.\n",
    "\n",
    "- **Design Choices:**\n",
    "  - **Overlap:** Ensures semantic continuity across splits.\n",
    "  - **Dynamic Splitting Points:** Searches for natural splitting points (e.g., punctuation) to preserve meaning.\n",
    "\n",
    "### **F. Parallel Text Preprocessing**\n",
    "```python\n",
    "def parallel_text_preprocessing(texts: List[str]) -> List[str]:\n",
    "    with ProcessPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        processed_texts = list(executor.map(preprocess_text, texts))\n",
    "    return processed_texts\n",
    "```\n",
    "- **Purpose:**\n",
    "  - Accelerates CPU-bound preprocessing tasks by leveraging multiple processes.\n",
    "\n",
    "- **Design Choices:**\n",
    "  - **ProcessPoolExecutor:** Suitable for CPU-intensive tasks like text cleaning and lemmatization.\n",
    "\n",
    "### **G. Batch Generator Utility**\n",
    "```python\n",
    "def batch_generator(iterable, batch_size):\n",
    "    iterator = iter(iterable)\n",
    "    while batch := list(islice(iterator, batch_size)):\n",
    "        yield batch\n",
    "```\n",
    "- **Purpose:**\n",
    "  - Creates batches from an iterable for efficient processing and API interactions.\n",
    "\n",
    "- **Design Choices:**\n",
    "  - **Generator Pattern:** Efficient memory usage by yielding one batch at a time.\n",
    "\n",
    "### **H. Parallel Embedding Generation**\n",
    "```python\n",
    "def parallel_generate_embeddings(texts: List[str], openai_client: OpenAI) -> List[List[float]]:\n",
    "    # Function implementation...\n",
    "```\n",
    "- **Purpose:**\n",
    "  - Generates vector embeddings for texts in parallel, handling large volumes efficiently.\n",
    "\n",
    "- **Design Choices:**\n",
    "  - **ThreadPoolExecutor:** Ideal for I/O-bound tasks like API calls.\n",
    "  - **Retry Mechanism:** Implements exponential backoff to handle transient API failures.\n",
    "  - **Combining Embeddings:** Averages embeddings from split chunks to maintain consistency.\n",
    "\n",
    "### **I. Process Chunk Function**\n",
    "```python\n",
    "def process_chunk(chunk: pd.DataFrame, config: PipelineConfig, openai_client: OpenAI, pinecone_index) -> Dict[str, Any]:\n",
    "    # Function implementation...\n",
    "```\n",
    "- **Purpose:**\n",
    "  - Processes a single data chunk through preprocessing, embedding generation, and vector storage.\n",
    "\n",
    "- **Design Choices:**\n",
    "  - **Modularity:** Encapsulates processing logic for scalability and readability.\n",
    "  - **Parallelism:** Utilizes parallel functions for efficiency.\n",
    "\n",
    "### **J. Parallel Upload to Pinecone**\n",
    "```python\n",
    "def parallel_upload_to_pinecone(vectors: List[tuple], pinecone_index, batch_size: int = 100):\n",
    "    # Function implementation...\n",
    "```\n",
    "- **Purpose:**\n",
    "  - Uploads vectors to Pinecone in parallel batches to optimize throughput.\n",
    "\n",
    "- **Design Choices:**\n",
    "  - **Batch Size:** Configurable to balance between API rate limits and performance.\n",
    "  - **ThreadPoolExecutor:** Enables concurrent uploads, enhancing speed.\n",
    "\n",
    "### **K. Parallel Upload to BigQuery**\n",
    "```python\n",
    "def parallel_upload_to_bigquery(dfs: List[pd.DataFrame], table_id: str, config: PipelineConfig):\n",
    "    # Function implementation...\n",
    "```\n",
    "- **Purpose:**\n",
    "  - Uploads multiple DataFrames to BigQuery concurrently to expedite data storage.\n",
    "\n",
    "- **Design Choices:**\n",
    "  - **Temporary Storage:** Saves CSVs to GCS before loading into BigQuery for efficient data transfer.\n",
    "  - **ThreadPoolExecutor:** Facilitates parallel uploads, reducing total upload time.\n",
    "\n",
    "### **L. Main Processing Function**\n",
    "```python\n",
    "def process_movie_data(**context):\n",
    "    # Function implementation...\n",
    "```\n",
    "- **Purpose:**\n",
    "  - Orchestrates the end-to-end processing of movie data within the DAG.\n",
    "\n",
    "- **Design Choices:**\n",
    "  - **Dask Integration:** Distributes processing across multiple partitions for scalability.\n",
    "  - **Result Aggregation:** Collects and organizes processed data for storage.\n",
    "  - **Sequential Task Flow:** Ensures dependencies are respected and data integrity is maintained.\n",
    "\n",
    "### **M. DAG Definition**\n",
    "```python\n",
    "with DAG(\n",
    "    'movie_vector_processing',\n",
    "    default_args={\n",
    "        'owner': 'airflow',\n",
    "        'depends_on_past': False,\n",
    "        'email_on_failure': True,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 2,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "        'start_date': datetime(2024, 10, 27),\n",
    "        'pool': 'movie_processing_pool',\n",
    "    },\n",
    "    description='Process movie data and generate vector embeddings',\n",
    "    schedule_interval='0 0 * * *',\n",
    "    catchup=False,\n",
    "    tags=['movies', 'vectors', 'embeddings'],\n",
    "    concurrency=4,\n",
    ") as dag:\n",
    "    # Task Definitions\n",
    "    create_full_text_table = BigQueryOperator(\n",
    "        task_id='create_full_text_table',\n",
    "        sql=CREATE_FULL_TEXT_TABLE_QUERY,\n",
    "        use_legacy_sql=False,\n",
    "        params={\n",
    "            'project_id': '{{ var.value.GCP_PROJECT_ID }}',\n",
    "            'dataset': '{{ var.value.BQ_DATASET }}'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    create_metadata_table = BigQueryOperator(\n",
    "        task_id='create_metadata_table',\n",
    "        sql=CREATE_METADATA_TABLE_QUERY,\n",
    "        use_legacy_sql=False,\n",
    "        params={\n",
    "            'project_id': '{{ var.value.GCP_PROJECT_ID }}',\n",
    "            'dataset': '{{ var.value.BQ_DATASET }}'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    process_data = PythonOperator(\n",
    "        task_id='process_movie_data',\n",
    "        python_callable=process_movie_data,\n",
    "        provide_context=True,\n",
    "    )\n",
    "    \n",
    "    [create_full_text_table, create_metadata_table] >> process_data\n",
    "```\n",
    "- **Components:**\n",
    "  - **DAG Configuration:**\n",
    "    - **Scheduling:** Daily at midnight.\n",
    "    - **Concurrency:** Limits simultaneous DAG runs to 4.\n",
    "    - **Retries:** Configured to handle transient failures.\n",
    "    - **Pools:** Manages resource allocation through `movie_processing_pool`.\n",
    "\n",
    "  - **Tasks:**\n",
    "    - **Table Creation:** Ensures necessary BigQuery tables exist before processing.\n",
    "    - **Data Processing:** Executes the main data processing function.\n",
    "\n",
    "- **Design Choices:**\n",
    "  - **Task Dependencies:** Ensures tables are created before processing begins.\n",
    "  - **Modular Tasks:** Separates table setup from data processing for clarity and maintainability.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Deployment Script Overview**\n",
    "\n",
    "### **A. Purpose**\n",
    "- Automates the deployment of the Airflow DAG to Google Cloud Composer.\n",
    "- Manages dependencies, configuration, and infrastructure setup.\n",
    "\n",
    "### **B. Deployment Steps:**\n",
    "\n",
    "1. **Set Environment Variables:**\n",
    "   - Define project ID, region, Composer environment name, DAG file name, and requirements file.\n",
    "\n",
    "2. **Create Temporary Directory:**\n",
    "   - Generates a secure temporary space for deployment artifacts.\n",
    "\n",
    "3. **Generate `requirements.txt`:**\n",
    "   - Lists all Python dependencies required for the DAG to function correctly.\n",
    "\n",
    "4. **Configure Google Cloud Project:**\n",
    "   - Sets the active project context for subsequent `gcloud` commands.\n",
    "\n",
    "5. **Retrieve Composer DAG Bucket Path:**\n",
    "   - Identifies the correct GCS bucket for deploying DAGs and dependencies.\n",
    "\n",
    "6. **Upload DAG and Requirements:**\n",
    "   - Transfers the DAG script and `requirements.txt` to the appropriate locations in the Composer bucket.\n",
    "\n",
    "7. **Install Dependencies:**\n",
    "   - Updates the Composer environment to install necessary Python packages.\n",
    "\n",
    "8. **Set Airflow Variables:**\n",
    "   - Configures essential variables like GCS bucket names, API keys, and project settings within Airflow.\n",
    "\n",
    "9. **Create BigQuery Dataset:**\n",
    "   - Ensures the target BigQuery dataset exists, creating it if necessary.\n",
    "\n",
    "10. **Clean Up:**\n",
    "    - Removes the temporary deployment directory to maintain a clean environment.\n",
    "\n",
    "11. **Deployment Completion Notification:**\n",
    "    - Provides a final message indicating the deployment's success and prompts verification.\n",
    "\n",
    "### **C. Key Components of the Script:**\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Exit immediately if a command exits with a non-zero status\n",
    "set -e\n",
    "# Treat unset variables as an error\n",
    "set -u\n",
    "\n",
    "# Function to display usage\n",
    "usage() {\n",
    "    echo \"Usage: $0 PROJECT_ID REGION ENVIRONMENT_NAME GCS_BUCKET INPUT_PATH BQ_DATASET PINECONE_API_KEY PINECONE_ENV PINECONE_INDEX_NAME OPENAI_API_KEY\"\n",
    "    exit 1\n",
    "}\n",
    "\n",
    "# Check for correct number of arguments\n",
    "if [ \"$#\" -ne 10 ]; then\n",
    "    usage\n",
    "fi\n",
    "\n",
    "# Assign arguments to variables\n",
    "PROJECT_ID=\"$1\"\n",
    "REGION=\"$2\"\n",
    "ENVIRONMENT_NAME=\"$3\"\n",
    "GCS_BUCKET=\"$4\"\n",
    "INPUT_PATH=\"$5\"\n",
    "BQ_DATASET=\"$6\"\n",
    "PINECONE_API_KEY=\"$7\"\n",
    "PINECONE_ENV=\"$8\"\n",
    "PINECONE_INDEX_NAME=\"$9\"\n",
    "OPENAI_API_KEY=\"${10}\"\n",
    "```\n",
    "- **Explanation:**\n",
    "  - **Error Handling:** Uses `set -e` and `set -u` for robust error management.\n",
    "  - **Argument Parsing:** Ensures all necessary parameters are provided for deployment.\n",
    "\n",
    "### **D. Enhanced Deployment Script Features:**\n",
    "- **Security Enhancements:**\n",
    "  - **Secret Management:** Recommends using Airflow’s Secret Backend (e.g., Google Secret Manager) for sensitive data instead of Airflow Variables.\n",
    "\n",
    "- **Idempotency:**\n",
    "  - **Dataset Creation:** Checks for the existence of the BigQuery dataset before attempting creation to avoid errors.\n",
    "\n",
    "- **Logging and Feedback:**\n",
    "  - **Echo Statements:** Provide real-time feedback during deployment for better monitoring and troubleshooting.\n",
    "\n",
    "- **Parameterization:**\n",
    "  - **Script Arguments:** Avoids hardcoding values, making the script reusable across different environments and projects.\n",
    "\n",
    "### **E. Sample Deployment Script Execution:**\n",
    "\n",
    "```bash\n",
    "./deploy_airflow_pipeline.sh your-project-id your-region your-composer-environment your-gcs-bucket your/input/path your_dataset your-pinecone-api-key your-pinecone-environment your-index-name your-openai-api-key\n",
    "```\n",
    "- **Usage:** Replace placeholders with actual values corresponding to your GCP setup and service configurations.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Design Choices and Justifications**\n",
    "\n",
    "### **A. Choice of Apache Airflow:**\n",
    "- **Reasons:**\n",
    "  - **Orchestration Capabilities:** Robust task scheduling, dependency management, and workflow visualization.\n",
    "  - **Extensibility:** Wide range of operators and plugins for integrating with various services.\n",
    "  - **Community Support:** Large and active community ensuring continuous improvements and support.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Operational Overhead:** Requires managing Airflow infrastructure unless using a managed service like Google Cloud Composer.\n",
    "  - **Learning Curve:** Steeper learning curve compared to simpler orchestration tools.\n",
    "\n",
    "### **B. Parallel Processing with Executors:**\n",
    "- **ThreadPoolExecutor:**\n",
    "  - **Use Case:** Suitable for I/O-bound operations like API calls.\n",
    "  - **Benefits:** Efficiently handles multiple concurrent network requests.\n",
    "  \n",
    "- **ProcessPoolExecutor:**\n",
    "  - **Use Case:** Ideal for CPU-bound tasks like text preprocessing.\n",
    "  - **Benefits:** Leverages multiple CPU cores to accelerate processing.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Complexity:** Managing two types of executors adds complexity to the codebase.\n",
    "  - **Resource Management:** Requires careful tuning of worker counts to balance performance and resource utilization.\n",
    "\n",
    "### **C. Use of Dask for Distributed Computing:**\n",
    "- **Reasons:**\n",
    "  - **Scalability:** Efficiently handles large datasets by distributing computations across multiple partitions.\n",
    "  - **Integration:** Seamlessly integrates with Pandas, making it easier to scale existing data processing code.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Overhead:** Introduces additional complexity and dependencies.\n",
    "  - **Resource Consumption:** Requires sufficient computational resources to handle distributed tasks effectively.\n",
    "\n",
    "### **D. Embedding Generation Strategy:**\n",
    "- **Batching Inputs:**\n",
    "  - **Reason:** Reduces the number of API calls, optimizing both performance and cost.\n",
    "  \n",
    "- **Text Splitting:**\n",
    "  - **Purpose:** Ensures texts comply with OpenAI's token limits while maintaining context through overlaps.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Latency:** Larger batches may introduce delays in processing due to increased processing time per batch.\n",
    "  - **Complexity:** Handling text splits and recombining embeddings adds complexity to the pipeline.\n",
    "\n",
    "### **E. Storage Choices:**\n",
    "- **Pinecone for Vector Storage:**\n",
    "  - **Advantages:** Specialized for vector data, enabling efficient similarity searches and scalable storage.\n",
    "  \n",
    "- **BigQuery for Data Storage:**\n",
    "  - **Advantages:** Managed data warehouse offering fast querying, partitioning, and clustering capabilities.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Cost:** Both Pinecone and BigQuery incur costs based on usage and storage, which need to be managed effectively.\n",
    "  - **Data Redundancy:** Storing data in multiple locations may require synchronization and consistency management.\n",
    "\n",
    "### **F. Configuration Management with Airflow Variables:**\n",
    "- **Advantages:**\n",
    "  - **Flexibility:** Easily adjust configuration parameters without modifying the code.\n",
    "  - **Centralization:** All configuration settings are managed in one place.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Security:** Sensitive information stored as Airflow Variables may require additional security measures.\n",
    "  - **Scalability:** Managing a large number of variables can become cumbersome.\n",
    "\n",
    "---\n",
    "\n",
    "## **11. Trade-Offs and Alternative Approaches**\n",
    "\n",
    "### **A. Alternative Orchestration Tools:**\n",
    "\n",
    "1. **Prefect:**\n",
    "   - **Pros:**\n",
    "     - Modern API with dynamic workflows.\n",
    "     - Enhanced observability and error handling.\n",
    "   - **Cons:**\n",
    "     - Smaller community compared to Airflow.\n",
    "     - Less mature ecosystem.\n",
    "\n",
    "2. **Dagster:**\n",
    "   - **Pros:**\n",
    "     - Data-centric design emphasizing data quality and lineage.\n",
    "     - Integrated testing and debugging tools.\n",
    "   - **Cons:**\n",
    "     - Newer tool with fewer integrations.\n",
    "     - Steeper learning curve for teams accustomed to Airflow.\n",
    "\n",
    "- **Trade-offs Compared to Airflow:**\n",
    "  - **Flexibility vs. Maturity:** Airflow offers a more mature and stable platform, while alternatives may provide more modern features but lack extensive community support.\n",
    "\n",
    "### **B. Serverless Architectures:**\n",
    "\n",
    "1. **Google Cloud Functions / AWS Lambda:**\n",
    "   - **Pros:**\n",
    "     - Automatically scales with workload.\n",
    "     - Reduced operational overhead.\n",
    "   - **Cons:**\n",
    "     - Limited execution time.\n",
    "     - Complex orchestration for multi-step workflows.\n",
    "\n",
    "- **Trade-offs Compared to Airflow:**\n",
    "  - **Simplicity vs. Orchestration Power:** Serverless functions are ideal for simple, event-driven tasks but lack the robust orchestration capabilities of Airflow for complex pipelines.\n",
    "\n",
    "### **C. Distributed Data Processing Frameworks:**\n",
    "\n",
    "1. **Apache Spark (Managed via Databricks or Google Cloud Dataproc):**\n",
    "   - **Pros:**\n",
    "     - High-performance data processing with in-memory computations.\n",
    "     - Scales horizontally to handle massive datasets.\n",
    "   - **Cons:**\n",
    "     - Requires managing Spark clusters.\n",
    "     - Higher operational complexity and cost.\n",
    "\n",
    "- **Trade-offs Compared to Airflow:**\n",
    "  - **Processing Power vs. Orchestration:** Spark excels in data processing but lacks built-in orchestration, necessitating integration with Airflow or similar tools.\n",
    "\n",
    "### **D. Managed ETL Services:**\n",
    "\n",
    "1. **Google Cloud Data Fusion / AWS Glue:**\n",
    "   - **Pros:**\n",
    "     - Fully managed with graphical interfaces.\n",
    "     - Simplifies pipeline creation and management.\n",
    "   - **Cons:**\n",
    "     - Less flexibility for custom workflows.\n",
    "     - Potentially higher costs for complex pipelines.\n",
    "\n",
    "- **Trade-offs Compared to Airflow:**\n",
    "  - **Ease of Use vs. Flexibility:** Managed ETL services are easier to use for standard tasks but may not support the bespoke requirements that Airflow can handle.\n",
    "\n",
    "### **E. Hybrid Approaches:**\n",
    "- **Combining Airflow with Serverless or Distributed Components:**\n",
    "  - **Pros:**\n",
    "    - Leverages strengths of multiple tools (e.g., Airflow for orchestration, serverless for scalable tasks).\n",
    "  - **Cons:**\n",
    "    - Increases architectural complexity.\n",
    "    - Requires robust integration and monitoring across platforms.\n",
    "\n",
    "- **Trade-offs Compared to Pure Airflow:**\n",
    "  - **Flexibility vs. Simplicity:** Hybrid approaches offer greater flexibility but at the cost of increased complexity and potential maintenance challenges.\n",
    "\n",
    "---\n",
    "\n",
    "## **12. Justifying Design Choices**\n",
    "\n",
    "### **A. Scalability and Performance:**\n",
    "- **Chunked and Parallel Processing:** Ensures the pipeline can handle large datasets efficiently by distributing workloads across multiple processes and threads.\n",
    "- **Dask Integration:** Facilitates distributed computing, enhancing the pipeline's ability to scale horizontally.\n",
    "\n",
    "### **B. Robustness and Reliability:**\n",
    "- **Error Handling and Retries:** Implements mechanisms to recover from transient failures, ensuring pipeline continuity.\n",
    "- **Logging:** Provides visibility into pipeline operations, aiding in monitoring and debugging.\n",
    "\n",
    "### **C. Cost Optimization:**\n",
    "- **Batch Operations:** Reduces the number of API calls, lowering costs associated with external services like OpenAI and Pinecone.\n",
    "- **Dynamic Worker Allocation:** Balances performance with resource utilization, preventing unnecessary costs from over-provisioning.\n",
    "\n",
    "### **D. Flexibility and Maintainability:**\n",
    "- **Configuration Management:** Centralizes settings through Airflow Variables, making the pipeline adaptable to different environments and requirements.\n",
    "- **Modular Code Structure:** Enhances readability and ease of maintenance, allowing for straightforward updates and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "## **13. Deployment Strategy with Google Cloud Composer**\n",
    "\n",
    "### **A. Overview of Deployment Script:**\n",
    "- **Automation:** Streamlines the deployment process, reducing manual intervention and potential errors.\n",
    "- **Steps:**\n",
    "  1. **Set Environment Variables:** Defines project-specific settings.\n",
    "  2. **Create Temporary Directory:** Manages temporary files securely.\n",
    "  3. **Generate `requirements.txt`:** Lists necessary Python dependencies.\n",
    "  4. **Configure GCP Project:** Sets the active project context.\n",
    "  5. **Retrieve Composer DAG Bucket Path:** Identifies the correct storage location.\n",
    "  6. **Upload DAG and Dependencies:** Transfers necessary files to Composer.\n",
    "  7. **Install Dependencies:** Ensures all Python packages are available in the Composer environment.\n",
    "  8. **Set Airflow Variables:** Configures essential pipeline parameters.\n",
    "  9. **Create BigQuery Dataset:** Ensures data storage infrastructure is in place.\n",
    "  10. **Clean Up:** Removes temporary files post-deployment.\n",
    "  11. **Completion Notification:** Signals successful deployment.\n",
    "\n",
    "### **B. Key Features of the Deployment Script:**\n",
    "- **Parameterization:** Accepts inputs as script arguments, enhancing reusability across different environments.\n",
    "- **Error Handling:** Uses `set -e` and `set -u` to terminate on failures and handle unset variables.\n",
    "- **Security Considerations:** Advises the use of Secret Managers for handling sensitive data.\n",
    "- **Idempotency:** Checks for existing resources (e.g., BigQuery datasets) before creation to avoid failures.\n",
    "\n",
    "### **C. Example Deployment Execution:**\n",
    "```bash\n",
    "./deploy_airflow_pipeline.sh your-project-id your-region your-composer-environment your-gcs-bucket your/input/path your_dataset your-pinecone-api-key your-pinecone-environment your-index-name your-openai-api-key\n",
    "```\n",
    "\n",
    "### **D. Enhancements in the Revised Deployment Script:**\n",
    "- **Security Enhancements:**\n",
    "  - **Secret Management:** Encourages using secure storage for API keys.\n",
    "- **Robustness:**\n",
    "  - **Existence Checks:** Verifies the existence of resources before attempting creation.\n",
    "  - **Logging:** Provides clear feedback during each deployment step.\n",
    "- **Flexibility:**\n",
    "  - **Argument-Based Inputs:** Makes the script adaptable to various deployment scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "## **14. Best Practices and Recommendations**\n",
    "\n",
    "### **A. Security Best Practices:**\n",
    "- **Secret Management:** Utilize services like Google Secret Manager to store and access sensitive information securely.\n",
    "- **Access Controls:** Implement strict IAM roles and permissions to limit access to critical resources.\n",
    "\n",
    "### **B. Monitoring and Alerting:**\n",
    "- **Airflow Integrations:** Integrate with monitoring tools like Prometheus and Grafana for enhanced observability.\n",
    "- **Automated Alerts:** Configure alerts for pipeline failures or performance bottlenecks.\n",
    "\n",
    "### **C. Testing and Validation:**\n",
    "- **Unit Tests:** Develop tests for utility functions (e.g., `split_text_by_tokens`, `preprocess_text`).\n",
    "- **Integration Tests:** Validate the end-to-end pipeline with sample datasets before full-scale deployment.\n",
    "- **Continuous Integration:** Implement CI pipelines to automate testing and deployment processes.\n",
    "\n",
    "### **D. Documentation and Maintainability:**\n",
    "- **Code Documentation:** Include docstrings and comments to explain complex logic and decisions.\n",
    "- **Pipeline Documentation:** Maintain clear documentation outlining the pipeline structure, dependencies, and configurations.\n",
    "- **Version Control:** Use version control systems (e.g., Git) to manage code changes and collaborate effectively.\n",
    "\n",
    "### **E. Cost Management:**\n",
    "- **Resource Monitoring:** Regularly monitor resource usage to identify and mitigate cost overruns.\n",
    "- **Optimized Configurations:** Fine-tune batch sizes, worker counts, and other parameters to balance performance and cost.\n",
    "\n",
    "### **F. Scalability Considerations:**\n",
    "- **Horizontal Scaling:** Ensure the pipeline can scale horizontally by adding more workers or processing nodes as data volume grows.\n",
    "- **Elastic Resources:** Utilize cloud services that offer elastic resource allocation to handle variable workloads efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## **15. Conclusion**\n",
    "\n",
    "- **Recap of Pipeline Strengths:**\n",
    "  - **Robust Orchestration:** Leveraging Apache Airflow for reliable and maintainable workflow management.\n",
    "  - **Scalable Processing:** Utilizing parallel and distributed computing techniques to handle large datasets efficiently.\n",
    "  - **Seamless Integrations:** Integrating with powerful external services like OpenAI, Pinecone, and BigQuery to enhance functionality.\n",
    "\n",
    "- **Acknowledgment of Trade-Offs:**\n",
    "  - **Complexity vs. Flexibility:** Balancing the complexity of parallel processing and error handling with the flexibility and performance gains.\n",
    "  - **Operational Overhead vs. Managed Services:** Weighing the benefits of self-managed pipelines against the ease of using managed services.\n",
    "\n",
    "- **Future Enhancements:**\n",
    "  - **Advanced Monitoring:** Implementing more sophisticated monitoring and alerting mechanisms.\n",
    "  - **Enhanced Security:** Further securing sensitive data through advanced secret management and encryption techniques.\n",
    "  - **Pipeline Optimization:** Continuously tuning pipeline parameters for optimal performance and cost-efficiency.\n",
    "\n",
    "- **Closing Remarks:**\n",
    "  - Emphasize the pipeline's capability to preprocess and prepare data effectively for LLM fine-tuning.\n",
    "  - Highlight the importance of scalable, maintainable, and robust data processing workflows in modern machine learning applications.\n",
    "\n",
    "---\n",
    "\n",
    "## **16. Q&A Session**\n",
    "\n",
    "- **Invite Questions:**\n",
    "  - Encourage the audience to ask questions about specific components, design choices, or alternative approaches.\n",
    "  \n",
    "- **Prepare for Common Questions:**\n",
    "  - **Scalability:** How does the pipeline handle increasing data volumes?\n",
    "  - **Error Handling:** What happens if an API call to OpenAI fails repeatedly?\n",
    "  - **Cost Management:** How do you monitor and control costs associated with API usage and cloud services?\n",
    "  - **Security:** How are sensitive data and API keys protected within the pipeline?\n",
    "\n",
    "---\n",
    "\n",
    "## **Appendix: Code Snippets and Explanations**\n",
    "\n",
    "*(Include key code snippets with annotations to illustrate critical parts of the pipeline. For example:)*\n",
    "\n",
    "### **A. Pipeline Configuration Class**\n",
    "\n",
    "```python\n",
    "class PipelineConfig:\n",
    "    def __init__(self):\n",
    "        self.gcs_bucket = Variable.get('GCS_BUCKET')\n",
    "        self.input_path = Variable.get('INPUT_PATH')\n",
    "        self.project_id = Variable.get('GCP_PROJECT_ID')\n",
    "        self.bq_dataset = Variable.get('BQ_DATASET')\n",
    "        self.full_text_table = f\"{self.project_id}.{self.bq_dataset}.full_text\"\n",
    "        self.metadata_table = f\"{self.project_id}.{self.bq_dataset}.metadata\"\n",
    "        self.dropped_table = f\"{self.project_id}.{self.bq_dataset}.dropped\"\n",
    "        self.pinecone_api_key = Variable.get('PINECONE_API_KEY')\n",
    "        self.pinecone_env = Variable.get('PINECONE_ENV')\n",
    "        self.index_name = Variable.get('PINECONE_INDEX_NAME')\n",
    "        self.openai_api_key = Variable.get('OPENAI_API_KEY')\n",
    "        self.num_processes = NUM_CORES\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - Centralizes all configuration parameters, making the pipeline easily configurable and maintainable.\n",
    "  - Retrieves sensitive information like API keys from Airflow Variables, promoting security and flexibility.\n",
    "\n",
    "### **B. Embedding Generation with Parallel Processing**\n",
    "\n",
    "```python\n",
    "def parallel_generate_embeddings(texts: List[str], openai_client: OpenAI) -> List[List[float]]:\n",
    "    encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def process_batch(batch_texts):\n",
    "        retry_count = 0\n",
    "        current_batch_size = len(batch_texts)\n",
    "        \n",
    "        while retry_count < MAX_RETRIES:\n",
    "            try:\n",
    "                response = openai_client.embeddings.create(\n",
    "                    input=batch_texts,\n",
    "                    model=\"text-embedding-ada-002\"\n",
    "                )\n",
    "                return [item.embedding for item in response.data]\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                logger.error(f\"Error in batch embedding: {e}. Retry {retry_count}/{MAX_RETRIES}\")\n",
    "                if retry_count == MAX_RETRIES:\n",
    "                    return [None] * current_batch_size\n",
    "                time.sleep(2 ** retry_count)\n",
    "    \n",
    "    # Split long texts and track their original indices\n",
    "    processed_texts = []\n",
    "    text_map = {}  # Maps new indices to original indices\n",
    "    current_idx = 0\n",
    "    \n",
    "    for idx, text in enumerate(texts):\n",
    "        chunks = split_text_by_tokens(text, encoder)\n",
    "        for chunk in chunks:\n",
    "            processed_texts.append(chunk)\n",
    "            text_map[current_idx] = {'original_idx': idx, 'total_chunks': len(chunks)}\n",
    "            current_idx += 1\n",
    "    \n",
    "    # Process all chunks in parallel batches\n",
    "    batches = list(batch_generator(processed_texts, EMBEDDING_BATCH_SIZE))\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=min(8, len(batches))) as executor:\n",
    "        batch_results = list(executor.map(process_batch, batches))\n",
    "    \n",
    "    # Flatten batch results\n",
    "    chunk_embeddings = []\n",
    "    for batch in batch_results:\n",
    "        if batch:\n",
    "            chunk_embeddings.extend(batch)\n",
    "    \n",
    "    # Combine embeddings for chunks from the same original text\n",
    "    final_embeddings = [None] * len(texts)\n",
    "    current_original_idx = -1\n",
    "    current_chunks = []\n",
    "    \n",
    "    for i, embedding in enumerate(chunk_embeddings):\n",
    "        if embedding is None:\n",
    "            continue\n",
    "            \n",
    "        original_idx = text_map[i]['original_idx']\n",
    "        total_chunks = text_map[i]['total_chunks']\n",
    "        \n",
    "        if original_idx != current_original_idx:\n",
    "            # Process previous chunks if any\n",
    "            if current_chunks:\n",
    "                final_embeddings[current_original_idx] = np.mean(current_chunks, axis=0).tolist()\n",
    "            # Start new chunk collection\n",
    "            current_original_idx = original_idx\n",
    "            current_chunks = [embedding]\n",
    "        else:\n",
    "            current_chunks.append(embedding)\n",
    "        \n",
    "        # Process last chunk if it's all chunks for this text\n",
    "        if len(current_chunks) == total_chunks:\n",
    "            final_embeddings[current_original_idx] = np.mean(current_chunks, axis=0).tolist()\n",
    "            current_chunks = []\n",
    "    \n",
    "    # Process any remaining chunks\n",
    "    if current_chunks:\n",
    "        final_embeddings[current_original_idx] = np.mean(current_chunks, axis=0).tolist()\n",
    "    \n",
    "    return final_embeddings\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Batch Processing:** Divides texts into batches for efficient API calls.\n",
    "  - **Retry Mechanism:** Enhances robustness by retrying failed API calls.\n",
    "  - **Embedding Aggregation:** Averages embeddings from split chunks to maintain consistency for original texts.\n",
    "  - **Parallelism:** Utilizes `ThreadPoolExecutor` to perform concurrent API requests, speeding up the embedding generation process.\n",
    "\n",
    "---\n",
    "\n",
    "## **17. Final Tips for the Presentation**\n",
    "\n",
    "- **Engage the Audience:**\n",
    "  - Use visuals like flowcharts and diagrams to illustrate the pipeline structure.\n",
    "  - Highlight real-world benefits of the pipeline, such as improved model performance and scalability.\n",
    "\n",
    "- **Demonstrate Key Components:**\n",
    "  - Walk through specific code snippets to showcase how tasks are implemented.\n",
    "  - Explain the rationale behind critical design decisions with practical examples.\n",
    "\n",
    "- **Address Potential Questions:**\n",
    "  - Prepare answers for common queries about scalability, error handling, security, and cost management.\n",
    "  - Be ready to discuss alternative approaches and why Apache Airflow was chosen over others.\n",
    "\n",
    "- **Practice Delivery:**\n",
    "  - Rehearse explaining complex concepts in simple terms.\n",
    "  - Time your presentation to ensure it fits within the allocated slot.\n",
    "\n",
    "- **Provide Takeaways:**\n",
    "  - Summarize the pipeline’s strengths and its impact on LLM fine-tuning.\n",
    "  - Offer insights into future enhancements and scalability plans.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Presentation Title:**\n",
    "### **Building a Semantic Movie Search and Summary API with FastAPI**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction**\n",
    "\n",
    "- **Welcome and Agenda**\n",
    "  - **Introduction to the Project**\n",
    "  - **Overview of the API and Its Components**\n",
    "  - **Detailed Walkthrough of the Codebase**\n",
    "  - **Design Decisions and Justifications**\n",
    "  - **Trade-offs and Alternative Approaches**\n",
    "  - **Deployment Strategy Using Kubernetes**\n",
    "  - **Monitoring and Observability**\n",
    "  - **Q&A Session**\n",
    "\n",
    "- **Project Objective**\n",
    "  - Develop a robust API that enables semantic search and summarization of movie data.\n",
    "  - Enhance user experience by providing relevant movie recommendations and concise summaries based on user queries.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. API Overview**\n",
    "\n",
    "### **Pipeline Steps:**\n",
    "\n",
    "1. **Data Ingestion and Storage:**\n",
    "   - **Sources:** Movie metadata and descriptions are stored in **BigQuery**.\n",
    "   - **Vector Storage:** **Pinecone** is used as a vector database to store semantic embeddings of movie data.\n",
    "\n",
    "2. **API Services:**\n",
    "   - **Semantic Search:** Utilizes **OpenAI** embeddings to perform semantic search over movie data.\n",
    "   - **Summarization:** Employs **OpenAI's** chat models to generate summaries based on retrieved movie descriptions.\n",
    "\n",
    "3. **Monitoring and Metrics:**\n",
    "   - **Prometheus:** Collects and exposes metrics for monitoring API performance.\n",
    "   - **Grafana:** Visualizes metrics for real-time insights and alerting.\n",
    "\n",
    "4. **Deployment:**\n",
    "   - **Containerization:** **Docker** is used to containerize the FastAPI application.\n",
    "   - **Orchestration:** **Kubernetes** manages the deployment, scaling, and maintenance of the API services.\n",
    "\n",
    "### **Visual Diagram:**\n",
    "*(Include a flowchart illustrating the data flow from user requests to Pinecone and BigQuery, and the summarization process.)*\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Key Features**\n",
    "\n",
    "### **A. Semantic Search and Summarization:**\n",
    "- **Semantic Search:**\n",
    "  - Leverages vector embeddings to understand the contextual meaning of user queries.\n",
    "  - Enables retrieval of movies that are semantically similar to the search query, beyond keyword matching.\n",
    "\n",
    "- **Summarization:**\n",
    "  - Uses advanced language models to generate concise summaries of relevant movie data.\n",
    "  - Enhances user experience by providing quick insights into movie details.\n",
    "\n",
    "### **B. Health Monitoring and Metrics:**\n",
    "- **Health Check Endpoint:**\n",
    "  - Provides real-time status of dependent services (Pinecone, BigQuery, OpenAI).\n",
    "  - Ensures API reliability and uptime.\n",
    "\n",
    "- **Prometheus Metrics:**\n",
    "  - Tracks total HTTP requests, request latency, and specific operation latencies (embedding generation, vector search, summary generation).\n",
    "  - Facilitates proactive monitoring and alerting for performance bottlenecks.\n",
    "\n",
    "### **C. Robust Error Handling and Logging:**\n",
    "- **Structured Logging:**\n",
    "  - Utilizes **structlog** for structured and contextual logging.\n",
    "  - Enhances traceability and debugging capabilities.\n",
    "\n",
    "- **Error Management:**\n",
    "  - Implements try-except blocks to handle exceptions gracefully.\n",
    "  - Returns appropriate HTTP exceptions to inform clients of failures.\n",
    "\n",
    "### **D. Scalable and Efficient Processing:**\n",
    "- **Batch Operations:**\n",
    "  - Processes search queries and summarizations in batches to optimize API usage and performance.\n",
    "  \n",
    "- **Parallel Processing:**\n",
    "  - Employs **ThreadPoolExecutor** and **ProcessPoolExecutor** for concurrent tasks, enhancing throughput.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Detailed Code Walkthrough**\n",
    "\n",
    "### **A. Dependencies and Initialization**\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import pinecone\n",
    "from openai import OpenAI\n",
    "from google.cloud import bigquery\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from contextlib import asynccontextmanager\n",
    "import time\n",
    "import logging\n",
    "from prometheus_client import Counter, Histogram, make_asgi_app\n",
    "import structlog\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **FastAPI:** Framework for building the API.\n",
    "  - **Pinecone:** Vector database for storing embeddings.\n",
    "  - **OpenAI:** Provides embedding and chat models for semantic search and summarization.\n",
    "  - **BigQuery:** Stores movie metadata and descriptions.\n",
    "  - **Prometheus Client:** Collects and exposes metrics.\n",
    "  - **Structlog:** Enables structured logging for better traceability.\n",
    "  - **Other Libraries:** Handle environment variables, asynchronous context management, and logging.\n",
    "\n",
    "### **B. Environment and Logging Configuration**\n",
    "\n",
    "```python\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure structured logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = structlog.get_logger()\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **load_dotenv():** Loads environment variables from a `.env` file.\n",
    "  - **Structured Logging:** Configures logging to output structured logs using `structlog`, enhancing readability and searchability in log management systems.\n",
    "\n",
    "### **C. Prometheus Metrics Setup**\n",
    "\n",
    "```python\n",
    "# Prometheus metrics\n",
    "REQUESTS = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])\n",
    "LATENCY = Histogram('http_request_duration_seconds', 'HTTP request latency', ['endpoint'])\n",
    "EMBEDDING_LATENCY = Histogram('embedding_generation_seconds', 'Embedding generation latency')\n",
    "SEARCH_LATENCY = Histogram('vector_search_seconds', 'Vector search latency')\n",
    "SUMMARY_LATENCY = Histogram('summary_generation_seconds', 'Summary generation latency')\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Counters and Histograms:** Define metrics to track request counts, latencies, and specific operation durations.\n",
    "  - **Labels:** Allow filtering and categorization of metrics based on request method, endpoint, and status.\n",
    "\n",
    "### **D. Pydantic Models for Request and Response**\n",
    "\n",
    "```python\n",
    "class SearchRequest(BaseModel):\n",
    "    query: str\n",
    "    top_k: int = 5\n",
    "    min_score: float = 0.7\n",
    "\n",
    "class MovieSummary(BaseModel):\n",
    "    query: str\n",
    "    movies: List[dict]\n",
    "    summary: str\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **SearchRequest:** Defines the structure of incoming search requests, including the query, number of results (`top_k`), and minimum similarity score (`min_score`).\n",
    "  - **MovieSummary:** Defines the structure of the API's response, including the original query, a list of matched movies, and the generated summary.\n",
    "\n",
    "### **E. Lifespan Management with Async Context Manager**\n",
    "\n",
    "```python\n",
    "@asynccontextmanager\n",
    "async def lifespan(app: FastAPI):\n",
    "    \"\"\"Startup and shutdown events handler\"\"\"\n",
    "    logger.info(\"application_startup\", message=\"Initializing services\")\n",
    "    try:\n",
    "        # Initialize Pinecone\n",
    "        pinecone.init(\n",
    "            api_key=os.getenv('PINECONE_API_KEY'),\n",
    "            environment=os.getenv('PINECONE_ENV')\n",
    "        )\n",
    "        app.state.pinecone_index = pinecone.Index(os.getenv('PINECONE_INDEX_NAME'))\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        app.state.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "        \n",
    "        # Initialize BigQuery client\n",
    "        app.state.bq_client = bigquery.Client(project=os.getenv('GCP_PROJECT_ID'))\n",
    "        \n",
    "        logger.info(\"application_startup_success\", message=\"Services initialized successfully\")\n",
    "        yield\n",
    "    except Exception as e:\n",
    "        logger.error(\"application_startup_failed\", error=str(e))\n",
    "        raise\n",
    "    finally:\n",
    "        # Cleanup\n",
    "        logger.info(\"application_shutdown\", message=\"Shutting down services\")\n",
    "        if hasattr(app.state, 'bq_client'):\n",
    "            app.state.bq_client.close()\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Initialization:**\n",
    "    - **Pinecone:** Initializes the Pinecone client and connects to the specified index.\n",
    "    - **OpenAI:** Initializes the OpenAI client with the provided API key.\n",
    "    - **BigQuery:** Initializes the BigQuery client for data retrieval.\n",
    "  - **Error Handling:** Logs and raises exceptions if service initialization fails.\n",
    "  - **Cleanup:** Ensures that resources like the BigQuery client are properly closed during shutdown.\n",
    "\n",
    "### **F. FastAPI Application Initialization**\n",
    "\n",
    "```python\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Movie Search and Summary API\",\n",
    "    description=\"API for semantic search and summarization of movie data\",\n",
    "    version=\"1.0.0\",\n",
    "    lifespan=lifespan\n",
    ")\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Configure appropriately for production\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "# Add Prometheus metrics endpoint\n",
    "metrics_app = make_asgi_app()\n",
    "app.mount(\"/metrics\", metrics_app)\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **FastAPI App:** Sets up the main application with metadata and lifespan management.\n",
    "  - **CORS Middleware:** Configures Cross-Origin Resource Sharing to allow requests from all origins (`allow_origins=[\"*\"]`). *Note: For production, restrict this to trusted origins to enhance security.*\n",
    "  - **Prometheus Metrics Endpoint:** Mounts the `/metrics` endpoint to expose Prometheus metrics for monitoring.\n",
    "\n",
    "### **G. Middleware for Logging and Metrics Collection**\n",
    "\n",
    "```python\n",
    "@app.middleware(\"http\")\n",
    "async def add_logging_and_metrics(request: Request, call_next):\n",
    "    \"\"\"Middleware for logging and metrics collection\"\"\"\n",
    "    start_time = time.time()\n",
    "    request_id = str(time.time())\n",
    "    \n",
    "    logger.info(\n",
    "        \"request_started\",\n",
    "        request_id=request_id,\n",
    "        method=request.method,\n",
    "        url=str(request.url),\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        response = await call_next(request)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        REQUESTS.labels(\n",
    "            method=request.method,\n",
    "            endpoint=request.url.path,\n",
    "            status=response.status_code\n",
    "        ).inc()\n",
    "        LATENCY.labels(endpoint=request.url.path).observe(duration)\n",
    "        \n",
    "        logger.info(\n",
    "            \"request_completed\",\n",
    "            request_id=request_id,\n",
    "            duration=duration,\n",
    "            status_code=response.status_code\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"request_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__\n",
    "        )\n",
    "        raise\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Request Logging:**\n",
    "    - Logs the start of each request with method and URL.\n",
    "    - Logs the completion of each request with duration and status code.\n",
    "    - Logs any failures with error details.\n",
    "  - **Metrics Collection:**\n",
    "    - **REQUESTS Counter:** Increments the total request count categorized by method, endpoint, and status.\n",
    "    - **LATENCY Histogram:** Observes the duration of each request categorized by endpoint.\n",
    "\n",
    "### **H. Core Functionalities**\n",
    "\n",
    "#### **1. Generating Embeddings with OpenAI**\n",
    "\n",
    "```python\n",
    "def get_embedding(text: str, request_id: str = None) -> List[float]:\n",
    "    \"\"\"Generate embedding for the input text\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\"generating_embedding\", request_id=request_id, text_length=len(text))\n",
    "    \n",
    "    try:\n",
    "        response = app.state.openai_client.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        EMBEDDING_LATENCY.observe(duration)\n",
    "        \n",
    "        logger.info(\n",
    "            \"embedding_generated\",\n",
    "            request_id=request_id,\n",
    "            duration=duration\n",
    "        )\n",
    "        return response.data[0].embedding\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"embedding_generation_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__\n",
    "        )\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error generating embedding: {str(e)}\"\n",
    "        )\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Purpose:** Generates a vector embedding for the given text using OpenAI's `text-embedding-ada-002` model.\n",
    "  - **Logging:** Records the start and completion of embedding generation, including text length and duration.\n",
    "  - **Metrics:** Observes the latency of embedding generation.\n",
    "  - **Error Handling:** Logs errors and raises appropriate HTTP exceptions for API clients.\n",
    "\n",
    "#### **2. Searching in Pinecone**\n",
    "\n",
    "```python\n",
    "def search_pinecone(vector: List[float], top_k: int, min_score: float, request_id: str = None) -> List[tuple]:\n",
    "    \"\"\"Search Pinecone index for similar vectors\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\n",
    "        \"searching_pinecone\",\n",
    "        request_id=request_id,\n",
    "        top_k=top_k,\n",
    "        min_score=min_score\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        results = app.state.pinecone_index.query(\n",
    "            vector=vector,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        SEARCH_LATENCY.observe(duration)\n",
    "        \n",
    "        filtered_results = [\n",
    "            (match.id, match.score)\n",
    "            for match in results.matches\n",
    "            if match.score >= min_score\n",
    "        ]\n",
    "        \n",
    "        logger.info(\n",
    "            \"pinecone_search_completed\",\n",
    "            request_id=request_id,\n",
    "            duration=duration,\n",
    "            results_count=len(filtered_results)\n",
    "        )\n",
    "        return filtered_results\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"pinecone_search_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__\n",
    "        )\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error searching Pinecone: {str(e)}\"\n",
    "        )\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Purpose:** Performs a semantic search in Pinecone using the provided vector.\n",
    "  - **Parameters:**\n",
    "    - **vector:** The embedding of the search query.\n",
    "    - **top_k:** Number of top results to retrieve.\n",
    "    - **min_score:** Minimum similarity score threshold for results.\n",
    "  - **Logging and Metrics:**\n",
    "    - Logs the initiation and completion of the search with parameters and results count.\n",
    "    - Observes the latency of the search operation.\n",
    "  - **Filtering:** Ensures only results meeting the minimum similarity score are returned.\n",
    "  - **Error Handling:** Logs errors and raises appropriate HTTP exceptions.\n",
    "\n",
    "#### **3. Retrieving Movie Descriptions from BigQuery**\n",
    "\n",
    "```python\n",
    "def get_full_text_from_bigquery(ids: List[str], request_id: str = None) -> List[dict]:\n",
    "    \"\"\"Retrieve full text from BigQuery for given IDs\"\"\"\n",
    "    logger.info(\n",
    "        \"querying_bigquery\",\n",
    "        request_id=request_id,\n",
    "        ids_count=len(ids)\n",
    "    )\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT id, full_text\n",
    "    FROM `{os.getenv('GCP_PROJECT_ID')}.{os.getenv('BQ_DATASET')}.full_text`\n",
    "    WHERE id IN UNNEST(@ids)\n",
    "    \"\"\"\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "        query_parameters=[\n",
    "            bigquery.ArrayParameter(\"ids\", \"STRING\", ids)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        results = app.state.bq_client.query(query, job_config=job_config).result()\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        results_list = [dict(row) for row in results]\n",
    "        \n",
    "        logger.info(\n",
    "            \"bigquery_query_completed\",\n",
    "            request_id=request_id,\n",
    "            duration=duration,\n",
    "            results_count=len(results_list)\n",
    "        )\n",
    "        return results_list\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"bigquery_query_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__\n",
    "        )\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error querying BigQuery: {str(e)}\"\n",
    "        )\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Purpose:** Fetches detailed movie descriptions from BigQuery based on a list of movie IDs.\n",
    "  - **Query Construction:**\n",
    "    - Uses parameterized queries to prevent SQL injection and optimize performance.\n",
    "  - **Logging and Metrics:**\n",
    "    - Logs the start and completion of the BigQuery query, including the number of IDs queried and duration.\n",
    "  - **Result Processing:**\n",
    "    - Converts query results into a list of dictionaries for easy manipulation.\n",
    "  - **Error Handling:** Logs errors and raises appropriate HTTP exceptions.\n",
    "\n",
    "#### **4. Generating Summaries with OpenAI**\n",
    "\n",
    "```python\n",
    "def generate_summary(query: str, texts: List[dict], request_id: str = None) -> str:\n",
    "    \"\"\"Generate summary using OpenAI\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\n",
    "        \"generating_summary\",\n",
    "        request_id=request_id,\n",
    "        query=query,\n",
    "        texts_count=len(texts)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"Based on the following movie descriptions, provide a brief summary that addresses this search query: \"{query}\"\n",
    "    \n",
    "Movie descriptions:\n",
    "{chr(10).join([f'- {text[\"full_text\"]}' for text in texts])}\n",
    "\n",
    "Please provide a concise summary that highlights the most relevant aspects related to the search query.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = app.state.openai_client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo-preview\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that provides concise and relevant summaries of movie information.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.7,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        SUMMARY_LATENCY.observe(duration)\n",
    "        \n",
    "        summary = response.choices[0].message.content\n",
    "        \n",
    "        logger.info(\n",
    "            \"summary_generated\",\n",
    "            request_id=request_id,\n",
    "            duration=duration,\n",
    "            summary_length=len(summary)\n",
    "        )\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"summary_generation_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__\n",
    "        )\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail=f\"Error generating summary: {str(e)}\"\n",
    "        )\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Purpose:** Generates a summary of relevant movies based on the user's search query and the retrieved movie descriptions.\n",
    "  - **Prompt Construction:**\n",
    "    - Includes the search query and a list of movie descriptions to provide context for the summarization.\n",
    "    - Uses bullet points for clarity and organization.\n",
    "  - **OpenAI Chat Model:**\n",
    "    - Utilizes `gpt-4-turbo-preview` for generating high-quality summaries.\n",
    "    - Configured with a system prompt to guide the assistant's behavior.\n",
    "  - **Logging and Metrics:**\n",
    "    - Logs the initiation and completion of summary generation, including the number of texts summarized and the length of the summary.\n",
    "    - Observes the latency of the summarization process.\n",
    "  - **Error Handling:** Logs errors and raises appropriate HTTP exceptions.\n",
    "\n",
    "### **I. API Endpoints**\n",
    "\n",
    "#### **1. Search and Summarize Endpoint (`/search`)**\n",
    "\n",
    "```python\n",
    "@app.post(\"/search\", response_model=MovieSummary)\n",
    "async def search_and_summarize(request: SearchRequest, req: Request):\n",
    "    \"\"\"Endpoint to search for similar movies and generate a summary\"\"\"\n",
    "    request_id = str(time.time())\n",
    "    logger.info(\n",
    "        \"search_request_received\",\n",
    "        request_id=request_id,\n",
    "        query=request.query,\n",
    "        top_k=request.top_k\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Generate embedding for query\n",
    "        query_embedding = get_embedding(request.query, request_id)\n",
    "        \n",
    "        # Search Pinecone\n",
    "        similar_vectors = search_pinecone(\n",
    "            vector=query_embedding,\n",
    "            top_k=request.top_k,\n",
    "            min_score=request.min_score,\n",
    "            request_id=request_id\n",
    "        )\n",
    "        \n",
    "        if not similar_vectors:\n",
    "            logger.info(\n",
    "                \"no_results_found\",\n",
    "                request_id=request_id,\n",
    "                query=request.query\n",
    "            )\n",
    "            return MovieSummary(\n",
    "                query=request.query,\n",
    "                movies=[],\n",
    "                summary=\"No relevant movies found for your query.\"\n",
    "            )\n",
    "        \n",
    "        # Get IDs and scores\n",
    "        ids = [id for id, _ in similar_vectors]\n",
    "        scores = {id: score for id, score in similar_vectors}\n",
    "        \n",
    "        # Get full text from BigQuery\n",
    "        movie_texts = get_full_text_from_bigquery(ids, request_id)\n",
    "        \n",
    "        # Add similarity scores to movie data\n",
    "        for movie in movie_texts:\n",
    "            movie['similarity_score'] = scores.get(movie['id'], 0)\n",
    "        \n",
    "        # Sort by similarity score\n",
    "        movie_texts.sort(key=lambda x: x['similarity_score'], reverse=True)\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = generate_summary(request.query, movie_texts, request_id)\n",
    "        \n",
    "        logger.info(\n",
    "            \"search_request_completed\",\n",
    "            request_id=request_id,\n",
    "            movies_count=len(movie_texts)\n",
    "        )\n",
    "        \n",
    "        return MovieSummary(\n",
    "            query=request.query,\n",
    "            movies=movie_texts,\n",
    "            summary=summary\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            \"search_request_failed\",\n",
    "            request_id=request_id,\n",
    "            error=str(e),\n",
    "            error_type=type(e).__name__,\n",
    "            query=request.query\n",
    "        )\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Workflow:**\n",
    "    1. **Receive Request:** Accepts a search query with `top_k` and `min_score` parameters.\n",
    "    2. **Generate Embedding:** Converts the query into a vector using OpenAI's embedding model.\n",
    "    3. **Search Pinecone:** Retrieves similar vectors (movies) from Pinecone based on the query embedding.\n",
    "    4. **Fetch Movie Descriptions:** Retrieves detailed movie descriptions from BigQuery using the matched IDs.\n",
    "    5. **Generate Summary:** Creates a concise summary of the relevant movies using OpenAI's chat model.\n",
    "    6. **Respond:** Returns the original query, list of matched movies with similarity scores, and the generated summary.\n",
    "  - **Logging and Metrics:**\n",
    "    - Logs the reception and completion of each search request.\n",
    "    - Tracks the number of movies processed and the overall success of the request.\n",
    "  - **Error Handling:** Catches and logs any exceptions during the process, returning appropriate HTTP error responses.\n",
    "\n",
    "#### **2. Health Check Endpoint (`/health`)**\n",
    "\n",
    "```python\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    try:\n",
    "        # Verify all services are connected\n",
    "        _ = app.state.pinecone_index.describe_index_stats()\n",
    "        _ = app.state.bq_client.list_datasets()\n",
    "        _ = app.state.openai_client.models.list()\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"healthy\",\n",
    "            \"services\": {\n",
    "                \"pinecone\": \"connected\",\n",
    "                \"bigquery\": \"connected\",\n",
    "                \"openai\": \"connected\"\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(\"health_check_failed\", error=str(e))\n",
    "        return {\n",
    "            \"status\": \"unhealthy\",\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Purpose:** Provides a simple way to verify the health and connectivity of all dependent services (Pinecone, BigQuery, OpenAI).\n",
    "  - **Workflow:**\n",
    "    - Attempts to describe Pinecone index stats.\n",
    "    - Lists BigQuery datasets to confirm connectivity.\n",
    "    - Lists OpenAI models to ensure the API is accessible.\n",
    "  - **Response:**\n",
    "    - **Healthy:** Confirms that all services are connected and operational.\n",
    "    - **Unhealthy:** Returns an error message indicating the failure.\n",
    "\n",
    "### **J. Running the Application**\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Uvicorn:** ASGI server used to run the FastAPI application.\n",
    "  - **Host and Port:** Configured to listen on all interfaces (`0.0.0.0`) and port `8000`.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Deployment Strategy**\n",
    "\n",
    "### **A. Containerization with Docker**\n",
    "\n",
    "#### **Dockerfile Overview**\n",
    "\n",
    "```dockerfile\n",
    "# Use Python 3.9 slim image\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    build-essential \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements first to leverage Docker cache\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code\n",
    "COPY . .\n",
    "\n",
    "# Set ownership and permissions\n",
    "RUN chown -R app:app /app\n",
    "\n",
    "# Switch to non-root user\n",
    "USER app\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=3s \\\n",
    "  CMD curl -f http://localhost:8000/health || exit 1\n",
    "\n",
    "# Command to run the application\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Base Image:** Uses `python:3.9-slim` for a lightweight Python environment.\n",
    "  - **Working Directory:** Sets `/app` as the working directory inside the container.\n",
    "  - **System Dependencies:** Installs `build-essential` for compiling Python packages that require compilation.\n",
    "  - **Python Dependencies:** Copies and installs dependencies from `requirements.txt`, leveraging Docker's caching mechanism for efficiency.\n",
    "  - **Application Code:** Copies the rest of the application code into the container.\n",
    "  - **User Management:** Creates a non-root user (`app`) for running the application, enhancing security.\n",
    "  - **Port Exposure:** Exposes port `8000` for the API.\n",
    "  - **Health Check:** Configures a health check to ensure the application is running properly.\n",
    "  - **Startup Command:** Uses `uvicorn` to run the FastAPI application.\n",
    "\n",
    "### **B. Kubernetes Deployment**\n",
    "\n",
    "#### **1. ConfigMap (`configmap.yaml`)**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: movie-search-config\n",
    "data:\n",
    "  GCP_PROJECT_ID: \"your-project-id\"\n",
    "  BQ_DATASET: \"your-dataset\"\n",
    "  PINECONE_ENV: \"your-pinecone-env\"\n",
    "  PINECONE_INDEX_NAME: \"your-index-name\"\n",
    "  LOG_LEVEL: \"INFO\"\n",
    "  # Structured logging configuration\n",
    "  LOGGING_CONFIG: |\n",
    "    {\n",
    "      \"version\": 1,\n",
    "      \"disable_existing_loggers\": false,\n",
    "      \"formatters\": {\n",
    "        \"json\": {\n",
    "          \"format\": \"%(levelname)s %(asctime)s %(name)s %(message)s\",\n",
    "          \"datefmt\": \"%Y-%m-%d %H:%M:%S\",\n",
    "          \"class\": \"pythonjsonlogger.jsonlogger.JsonFormatter\"\n",
    "        }\n",
    "      },\n",
    "      \"handlers\": {\n",
    "        \"console\": {\n",
    "          \"class\": \"logging.StreamHandler\",\n",
    "          \"formatter\": \"json\",\n",
    "          \"stream\": \"ext://sys.stdout\"\n",
    "        }\n",
    "      },\n",
    "      \"root\": {\n",
    "        \"level\": \"INFO\",\n",
    "        \"handlers\": [\"console\"]\n",
    "      }\n",
    "    }\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Purpose:** Stores non-sensitive configuration parameters such as project IDs, dataset names, Pinecone environment details, and logging configurations.\n",
    "  - **Logging Configuration:** Defines a JSON formatter for structured logging, enhancing log management and analysis.\n",
    "\n",
    "#### **2. Secret (`secret.yaml`)**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Secret\n",
    "metadata:\n",
    "  name: movie-search-secrets\n",
    "type: Opaque\n",
    "data:\n",
    "  OPENAI_API_KEY: \"base64-encoded-key\"\n",
    "  PINECONE_API_KEY: \"base64-encoded-key\"\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Purpose:** Stores sensitive information like API keys in an encoded format to ensure security.\n",
    "  - **Usage:** These secrets are referenced in the Deployment to provide necessary credentials to the application.\n",
    "\n",
    "#### **3. Deployment (`deployment.yaml`)**\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: movie-search-api\n",
    "  labels:\n",
    "    app: movie-search-api\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: movie-search-api\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: movie-search-api\n",
    "      annotations:\n",
    "        # Enable GCP Cloud Logging\n",
    "        logging.cloud.google.com/agent: '{\"plugins\":[\"opentelemetry\",\"prometheus\",\"application\"]}'\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: movie-search-api\n",
    "        image: gcr.io/your-project-id/movie-search-api:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"250m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"500m\"\n",
    "        env:\n",
    "        # Add trace context to logs\n",
    "        - name: OTEL_SERVICE_NAME\n",
    "          value: \"movie-search-api\"\n",
    "        - name: OTEL_PROPAGATORS\n",
    "          value: \"tracecontext,baggage\"\n",
    "        # Add pod metadata to logs\n",
    "        - name: POD_NAME\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              fieldPath: metadata.name\n",
    "        - name: NAMESPACE\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              fieldPath: metadata.namespace\n",
    "        envFrom:\n",
    "        - configMapRef:\n",
    "            name: movie-search-config\n",
    "        - secretRef:\n",
    "            name: movie-search-secrets\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 10\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 15\n",
    "          periodSeconds: 20\n",
    "        # Mount fluentbit config for log processing\n",
    "        volumeMounts:\n",
    "        - name: varlog\n",
    "          mountPath: /var/log\n",
    "        - name: varlibdockercontainers\n",
    "          mountPath: /var/lib/docker/containers\n",
    "          readOnly: true\n",
    "        - name: fluent-bit-config\n",
    "          mountPath: /fluent-bit/etc/\n",
    "      # Sidecar container for log collection\n",
    "      - name: fluent-bit\n",
    "        image: fluent/fluent-bit:latest\n",
    "        volumeMounts:\n",
    "        - name: varlog\n",
    "          mountPath: /var/log\n",
    "        - name: varlibdockercontainers\n",
    "          mountPath: /var/lib/docker/containers\n",
    "          readOnly: true\n",
    "        - name: fluent-bit-config\n",
    "          mountPath: /fluent-bit/etc/\n",
    "      volumes:\n",
    "      - name: varlog\n",
    "        emptyDir: {}\n",
    "      - name: varlibdockercontainers\n",
    "        hostPath:\n",
    "          path: /var/lib/docker/containers\n",
    "      - name: fluent-bit-config\n",
    "        configMap:\n",
    "          name: fluent-bit-config\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Replicas:** Deploys 3 replicas for high availability and load balancing.\n",
    "  - **Containers:**\n",
    "    - **Main Container (`movie-search-api`):** Runs the FastAPI application.\n",
    "    - **Sidecar Container (`fluent-bit`):** Collects and processes logs, forwarding them to Google Stackdriver.\n",
    "  - **Environment Variables:**\n",
    "    - **OTEL_SERVICE_NAME & OTEL_PROPAGATORS:** Enable OpenTelemetry tracing for observability.\n",
    "    - **POD_NAME & NAMESPACE:** Provide context for logs.\n",
    "    - **ConfigMap & Secrets:** Inject configuration and sensitive data into the container.\n",
    "  - **Probes:**\n",
    "    - **Readiness Probe:** Checks if the application is ready to receive traffic.\n",
    "    - **Liveness Probe:** Ensures the application is alive and healthy.\n",
    "  - **Resource Management:** Defines CPU and memory requests and limits to manage resource allocation.\n",
    "  - **Volumes:**\n",
    "    - **Log Volumes:** Mounts directories for log collection.\n",
    "    - **Fluent Bit Config:** Mounts the Fluent Bit configuration from the ConfigMap.\n",
    "\n",
    "#### **4. Fluent Bit Configuration (`fluent-bit-config.yaml`)**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: fluent-bit-config\n",
    "data:\n",
    "  fluent-bit.conf: |\n",
    "    [SERVICE]\n",
    "        Flush         1\n",
    "        Log_Level     info\n",
    "        Daemon        off\n",
    "        Parsers_File  parsers.conf\n",
    "\n",
    "    [INPUT]\n",
    "        Name             tail\n",
    "        Path             /var/log/containers/*.log\n",
    "        Parser           docker\n",
    "        Tag              kube.*\n",
    "        Mem_Buf_Limit    5MB\n",
    "        Skip_Long_Lines  On\n",
    "\n",
    "    [FILTER]\n",
    "        Name                kubernetes\n",
    "        Match               kube.*\n",
    "        Kube_URL           https://kubernetes.default.svc:443\n",
    "        Merge_Log          On\n",
    "        K8S-Logging.Parser On\n",
    "\n",
    "    [OUTPUT]\n",
    "        Name            stackdriver\n",
    "        Match           *\n",
    "        resource        k8s_container\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Inputs:**\n",
    "    - **tail:** Reads log files from the specified path.\n",
    "  - **Filters:**\n",
    "    - **kubernetes:** Enriches logs with Kubernetes metadata.\n",
    "  - **Outputs:**\n",
    "    - **stackdriver:** Forwards processed logs to Google Stackdriver for centralized logging and monitoring.\n",
    "\n",
    "#### **5. Service (`service.yaml`)**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: movie-search-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: movie-search-api\n",
    "  ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "      targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Type:** `LoadBalancer` exposes the service externally using a cloud provider's load balancer.\n",
    "  - **Ports:**\n",
    "    - **Port 80:** Standard HTTP port for external traffic.\n",
    "    - **TargetPort 8000:** Port where the FastAPI application is running inside the container.\n",
    "\n",
    "#### **6. Horizontal Pod Autoscaler (`hpa.yaml`)**\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: movie-search-api-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: movie-search-api\n",
    "  minReplicas: 3\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Purpose:** Automatically scales the number of pods based on CPU utilization.\n",
    "  - **Parameters:**\n",
    "    - **minReplicas:** Minimum number of pods (3).\n",
    "    - **maxReplicas:** Maximum number of pods (10).\n",
    "    - **Metric:** Scales up when CPU usage exceeds 70% on average.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Kubernetes Cluster Setup**\n",
    "\n",
    "### **A. Cluster Creation Script**\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Set environment variables\n",
    "export PROJECT_ID=your-project-id\n",
    "export CLUSTER_NAME=movie-search\n",
    "export REGION=us-central1\n",
    "\n",
    "# Set project\n",
    "gcloud config set project $PROJECT_ID\n",
    "\n",
    "# Create cluster with essential features\n",
    "gcloud container clusters create $CLUSTER_NAME \\\n",
    "    --region $REGION \\\n",
    "    --num-nodes 3 \\\n",
    "    --machine-type e2-standard-2 \\\n",
    "    --enable-autoscaling \\\n",
    "    --min-nodes 3 \\\n",
    "    --max-nodes 10 \\\n",
    "    --node-locations $REGION-a,$REGION-b,$REGION-c \\\n",
    "    --logging=SYSTEM,WORKLOAD \\\n",
    "    --monitoring=SYSTEM \\\n",
    "    --enable-ip-alias \\\n",
    "    --workload-pool=${PROJECT_ID}.svc.id.goog \\\n",
    "    --labels=app=movie-search\n",
    "\n",
    "# Get credentials\n",
    "gcloud container clusters get-credentials $CLUSTER_NAME --region $REGION\n",
    "\n",
    "# Create namespace and service account\n",
    "kubectl create namespace movie-search\n",
    "\n",
    "# Create service account for workload identity\n",
    "gcloud iam service-accounts create movie-search-sa \\\n",
    "    --display-name=\"Movie Search Service Account\"\n",
    "\n",
    "# Grant permissions\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=\"serviceAccount:movie-search-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n",
    "    --role=\"roles/bigquery.dataViewer\"\n",
    "\n",
    "gcloud projects add-iam-policy-binding $PROJECT_ID \\\n",
    "    --member=\"serviceAccount:movie-search-sa@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n",
    "    --role=\"roles/logging.logWriter\"\n",
    "\n",
    "# Verify setup\n",
    "kubectl get nodes\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Cluster Configuration:**\n",
    "    - **Autoscaling:** Enables automatic scaling between 3 to 10 nodes based on workload.\n",
    "    - **Machine Type:** Uses `e2-standard-2` for a balance between performance and cost.\n",
    "    - **Multi-Zone Deployment:** Distributes nodes across multiple zones for high availability.\n",
    "    - **Logging and Monitoring:** Configures system and workload logging, integrating with GCP's monitoring services.\n",
    "    - **Workload Identity:** Enhances security by mapping Kubernetes service accounts to GCP IAM service accounts.\n",
    "    - **Labels:** Tags the cluster with labels for easier management and identification.\n",
    "\n",
    "  - **Service Account and Permissions:**\n",
    "    - **Service Account (`movie-search-sa`):** Dedicated account for the Movie Search application with necessary permissions.\n",
    "    - **Permissions Granted:**\n",
    "      - **BigQuery Data Viewer:** Allows reading data from BigQuery datasets.\n",
    "      - **Logging Log Writer:** Enables writing logs to Google Stackdriver.\n",
    "\n",
    "  - **Verification:** Ensures that the cluster nodes are up and running.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Deployment Steps**\n",
    "\n",
    "### **A. Building and Pushing Docker Image**\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Set environment variables\n",
    "export PROJECT_ID=$(gcloud config get-value project)\n",
    "export IMAGE_NAME=\"movie-search-api\"\n",
    "export IMAGE_TAG=$(git rev-parse --short HEAD 2>/dev/null || echo \"latest\")\n",
    "\n",
    "echo \"Building and pushing image to GCR...\"\n",
    "\n",
    "# Build the image\n",
    "docker build -t gcr.io/$PROJECT_ID/$IMAGE_NAME:$IMAGE_TAG \\\n",
    "            -t gcr.io/$PROJECT_ID/$IMAGE_NAME:latest .\n",
    "\n",
    "# Push the images\n",
    "docker push gcr.io/$PROJECT_ID/$IMAGE_NAME:$IMAGE_TAG\n",
    "docker push gcr.io/$PROJECT_ID/$IMAGE_NAME:latest\n",
    "\n",
    "echo \"Successfully built and pushed: gcr.io/$PROJECT_ID/$IMAGE_NAME:$IMAGE_TAG\"\n",
    "\n",
    "# Update Kubernetes deployment if needed\n",
    "read -p \"Do you want to update the Kubernetes deployment? (y/n) \" -n 1 -r\n",
    "echo\n",
    "if [[ $REPLY =~ ^[Yy]$ ]]\n",
    "then\n",
    "    kubectl set image deployment/movie-search-api \\\n",
    "            movie-search-api=gcr.io/$PROJECT_ID/$IMAGE_NAME:$IMAGE_TAG \\\n",
    "            -n movie-search\n",
    "    echo \"Deployment updated successfully!\"\n",
    "fi\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Image Building:**\n",
    "    - **Tags:** Builds the Docker image with both a specific commit tag and a `latest` tag for flexibility.\n",
    "  - **Image Pushing:**\n",
    "    - Pushes the built images to Google Container Registry (GCR) for storage and retrieval by Kubernetes.\n",
    "  - **Deployment Update:**\n",
    "    - Optionally updates the Kubernetes deployment to use the newly pushed image, facilitating continuous deployment.\n",
    "\n",
    "### **B. Kubernetes Deployment Resources**\n",
    "\n",
    "#### **1. ConfigMap and Secrets:**\n",
    "- **ConfigMap:** Contains non-sensitive configurations.\n",
    "- **Secrets:** Stores sensitive data like API keys securely.\n",
    "\n",
    "#### **2. Deployment:**\n",
    "- **Replicas:** Ensures multiple instances for high availability.\n",
    "- **Sidecar:** Fluent Bit collects and forwards logs.\n",
    "- **Probes:** Readiness and liveness probes ensure the application is healthy.\n",
    "\n",
    "#### **3. Service:**\n",
    "- **LoadBalancer:** Exposes the API externally, allowing clients to access it via a stable IP address.\n",
    "\n",
    "#### **4. Horizontal Pod Autoscaler (HPA):**\n",
    "- **Scaling Rules:** Adjusts the number of pods based on CPU utilization to handle varying loads efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Monitoring and Observability**\n",
    "\n",
    "### **A. Prometheus and Grafana Configuration**\n",
    "\n",
    "#### **1. Prometheus Values (`prometheus-values.yaml`)**\n",
    "\n",
    "```yaml\n",
    "grafana:\n",
    "  adminPassword: \"your-secure-password\"\n",
    "  persistence:\n",
    "    enabled: true\n",
    "    size: 10Gi\n",
    "  dashboardProviders:\n",
    "    dashboardproviders.yaml:\n",
    "      apiVersion: 1\n",
    "      providers:\n",
    "      - name: 'default'\n",
    "        orgId: 1\n",
    "        folder: ''\n",
    "        type: file\n",
    "        disableDeletion: false\n",
    "        editable: true\n",
    "        options:\n",
    "          path: /var/lib/grafana/dashboards\n",
    "  dashboards:\n",
    "    default:\n",
    "      api-monitoring:\n",
    "        json: |\n",
    "          {\n",
    "            \"annotations\": {\n",
    "              \"list\": []\n",
    "            },\n",
    "            \"editable\": true,\n",
    "            \"fiscalYearStartMonth\": 0,\n",
    "            \"graphTooltip\": 0,\n",
    "            \"links\": [],\n",
    "            \"liveNow\": false,\n",
    "            \"panels\": [\n",
    "              {\n",
    "                \"datasource\": {\n",
    "                  \"type\": \"prometheus\",\n",
    "                  \"uid\": \"prometheus\"\n",
    "                },\n",
    "                \"fieldConfig\": {\n",
    "                  \"defaults\": {\n",
    "                    \"color\": {\n",
    "                      \"mode\": \"palette-classic\"\n",
    "                    },\n",
    "                    \"custom\": {\n",
    "                      \"axisCenteredZero\": false,\n",
    "                      \"axisColorMode\": \"text\",\n",
    "                      \"axisLabel\": \"\",\n",
    "                      \"axisPlacement\": \"auto\",\n",
    "                      \"barAlignment\": 0,\n",
    "                      \"drawStyle\": \"line\",\n",
    "                      \"fillOpacity\": 10,\n",
    "                      \"gradientMode\": \"none\",\n",
    "                      \"hideFrom\": {\n",
    "                        \"legend\": false,\n",
    "                        \"tooltip\": false,\n",
    "                        \"viz\": false\n",
    "                      },\n",
    "                      \"lineInterpolation\": \"linear\",\n",
    "                      \"lineWidth\": 1,\n",
    "                      \"pointSize\": 5,\n",
    "                      \"scaleDistribution\": {\n",
    "                        \"type\": \"linear\"\n",
    "                      },\n",
    "                      \"showPoints\": \"never\",\n",
    "                      \"spanNulls\": false,\n",
    "                      \"stacking\": {\n",
    "                        \"group\": \"A\",\n",
    "                        \"mode\": \"none\"\n",
    "                      },\n",
    "                      \"thresholdsStyle\": {\n",
    "                        \"mode\": \"off\"\n",
    "                      }\n",
    "                    },\n",
    "                    \"mappings\": [],\n",
    "                    \"thresholds\": {\n",
    "                      \"mode\": \"absolute\",\n",
    "                      \"steps\": [\n",
    "                        {\n",
    "                          \"color\": \"green\",\n",
    "                          \"value\": null\n",
    "                        }\n",
    "                      ]\n",
    "                    },\n",
    "                    \"unit\": \"short\"\n",
    "                  },\n",
    "                  \"overrides\": []\n",
    "                },\n",
    "                \"gridPos\": {\n",
    "                  \"h\": 8,\n",
    "                  \"w\": 12,\n",
    "                  \"x\": 0,\n",
    "                  \"y\": 0\n",
    "                },\n",
    "                \"id\": 1,\n",
    "                \"options\": {\n",
    "                  \"legend\": {\n",
    "                    \"calcs\": [],\n",
    "                    \"displayMode\": \"list\",\n",
    "                    \"placement\": \"bottom\",\n",
    "                    \"showLegend\": true\n",
    "                  },\n",
    "                  \"tooltip\": {\n",
    "                    \"mode\": \"single\",\n",
    "                    \"sort\": \"none\"\n",
    "                  }\n",
    "                },\n",
    "                \"targets\": [\n",
    "                  {\n",
    "                    \"datasource\": {\n",
    "                      \"type\": \"prometheus\",\n",
    "                      \"uid\": \"prometheus\"\n",
    "                    },\n",
    "                    \"expr\": \"rate(search_requests_total[5m])\",\n",
    "                    \"refId\": \"A\"\n",
    "                  }\n",
    "                ],\n",
    "                \"title\": \"Request Rate\",\n",
    "                \"type\": \"timeseries\"\n",
    "              }\n",
    "            ],\n",
    "            \"refresh\": \"5s\",\n",
    "            \"schemaVersion\": 38,\n",
    "            \"style\": \"dark\",\n",
    "            \"tags\": [],\n",
    "            \"templating\": {\n",
    "              \"list\": []\n",
    "            },\n",
    "            \"time\": {\n",
    "              \"from\": \"now-1h\",\n",
    "              \"to\": \"now\"\n",
    "            },\n",
    "            \"timepicker\": {},\n",
    "            \"timezone\": \"\",\n",
    "            \"title\": \"API Monitoring\",\n",
    "            \"version\": 0,\n",
    "            \"weekStart\": \"\"\n",
    "          }\n",
    "\n",
    "prometheusOperator:\n",
    "  enabled: true\n",
    "  serviceMonitor:\n",
    "    enabled: true\n",
    "\n",
    "prometheus:\n",
    "  prometheusSpec:\n",
    "    retention: 15d\n",
    "    resources:\n",
    "      requests:\n",
    "        memory: 512Mi\n",
    "        cpu: 500m\n",
    "      limits:\n",
    "        memory: 2Gi\n",
    "        cpu: 1000m\n",
    "    storageSpec:\n",
    "      volumeClaimTemplate:\n",
    "        spec:\n",
    "          accessModes: [\"ReadWriteOnce\"]\n",
    "          resources:\n",
    "            requests:\n",
    "              storage: 50Gi\n",
    "\n",
    "alertmanager:\n",
    "  enabled: true\n",
    "  config:\n",
    "    global:\n",
    "      resolve_timeout: 5m\n",
    "    route:\n",
    "      group_by: ['job']\n",
    "      group_wait: 30s\n",
    "      group_interval: 5m\n",
    "      repeat_interval: 12h\n",
    "      receiver: 'slack'\n",
    "      routes:\n",
    "      - match:\n",
    "          severity: critical\n",
    "        receiver: 'slack'\n",
    "    receivers:\n",
    "    - name: 'slack'\n",
    "      slack_configs:\n",
    "      - api_url: 'https://hooks.slack.com/services/your-webhook-url'\n",
    "        channel: '#alerts'\n",
    "        send_resolved: true\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Grafana:**\n",
    "    - **Admin Password:** Secures access to Grafana dashboards.\n",
    "    - **Persistence:** Ensures dashboards are retained across pod restarts.\n",
    "    - **Dashboard Configuration:** Preloads an \"API Monitoring\" dashboard to visualize request rates and other metrics.\n",
    "  - **Prometheus:**\n",
    "    - **Retention:** Stores metrics data for 15 days.\n",
    "    - **Resources:** Defines CPU and memory requests and limits to manage performance.\n",
    "    - **Storage:** Allocates 50Gi for persistent metrics storage.\n",
    "  - **Alertmanager:**\n",
    "    - **Configuration:** Sets up alert routing to Slack for critical alerts.\n",
    "    - **Alerts:** Configures rules to group and notify based on severity.\n",
    "\n",
    "#### **2. ServiceMonitor (`service-monitor.yaml`)**\n",
    "\n",
    "```yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: movie-search-monitor\n",
    "  labels:\n",
    "    release: prometheus\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: movie-search-api\n",
    "  endpoints:\n",
    "  - port: http\n",
    "    path: /metrics\n",
    "    interval: 15s\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Purpose:** Instructs Prometheus to monitor the Movie Search API's `/metrics` endpoint.\n",
    "  - **Configuration:**\n",
    "    - **Selector:** Targets pods labeled with `app: movie-search-api`.\n",
    "    - **Endpoints:** Collects metrics every 15 seconds from the `/metrics` endpoint.\n",
    "\n",
    "#### **3. Prometheus Alert Rules (`prometheus-rules.yaml`)**\n",
    "\n",
    "```yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: PrometheusRule\n",
    "metadata:\n",
    "  name: movie-search-alerts\n",
    "  labels:\n",
    "    release: prometheus\n",
    "spec:\n",
    "  groups:\n",
    "  - name: movie-search\n",
    "    rules:\n",
    "    - alert: HighErrorRate\n",
    "      expr: |\n",
    "        sum(rate(http_requests_total{status=~\"5..\"}[5m])) \n",
    "        / \n",
    "        sum(rate(http_requests_total[5m])) \n",
    "        > 0.05\n",
    "      for: 5m\n",
    "      labels:\n",
    "        severity: critical\n",
    "      annotations:\n",
    "        summary: High error rate detected\n",
    "        description: Error rate is above 5% for more than 5 minutes\n",
    "      \n",
    "    - alert: HighLatency\n",
    "      expr: |\n",
    "        histogram_quantile(0.95, sum(rate(search_latency_seconds_bucket[5m])) by (le)) \n",
    "        > 2\n",
    "      for: 5m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: High latency detected\n",
    "        description: 95th percentile latency is above 2 seconds for 5 minutes\n",
    "    \n",
    "    - alert: HighCPUUsage\n",
    "      expr: |\n",
    "        container_cpu_usage_seconds_total{container=\"movie-search-api\"} \n",
    "        > \n",
    "        container_spec_cpu_quota{container=\"movie-search-api\"} * 0.8\n",
    "      for: 15m\n",
    "      labels:\n",
    "        severity: warning\n",
    "      annotations:\n",
    "        summary: High CPU usage detected\n",
    "        description: Container is using more than 80% of its CPU quota\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **HighErrorRate Alert:**\n",
    "    - **Condition:** Error rate (5xx responses) exceeds 5% over 5 minutes.\n",
    "    - **Severity:** Critical.\n",
    "    - **Action:** Notifies via Slack about the high error rate.\n",
    "\n",
    "  - **HighLatency Alert:**\n",
    "    - **Condition:** 95th percentile request latency exceeds 2 seconds over 5 minutes.\n",
    "    - **Severity:** Warning.\n",
    "    - **Action:** Notifies via Slack about increased latency.\n",
    "\n",
    "  - **HighCPUUsage Alert:**\n",
    "    - **Condition:** CPU usage exceeds 80% of the container's quota for 15 minutes.\n",
    "    - **Severity:** Warning.\n",
    "    - **Action:** Notifies via Slack about high CPU usage.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Deployment Process**\n",
    "\n",
    "### **A. Building and Pushing the Docker Image**\n",
    "\n",
    "1. **Build the Docker Image:**\n",
    "   ```bash\n",
    "   docker build -t gcr.io/$PROJECT_ID/$IMAGE_NAME:$IMAGE_TAG \\\n",
    "               -t gcr.io/$PROJECT_ID/$IMAGE_NAME:latest .\n",
    "   ```\n",
    "\n",
    "2. **Push the Docker Image to Google Container Registry (GCR):**\n",
    "   ```bash\n",
    "   docker push gcr.io/$PROJECT_ID/$IMAGE_NAME:$IMAGE_TAG\n",
    "   docker push gcr.io/$PROJECT_ID/$IMAGE_NAME:latest\n",
    "   ```\n",
    "\n",
    "3. **Update Kubernetes Deployment:**\n",
    "   - **Prompt for Confirmation:**\n",
    "     ```bash\n",
    "     read -p \"Do you want to update the Kubernetes deployment? (y/n) \" -n 1 -r\n",
    "     echo\n",
    "     ```\n",
    "   - **Update Image:**\n",
    "     ```bash\n",
    "     if [[ $REPLY =~ ^[Yy]$ ]]\n",
    "     then\n",
    "         kubectl set image deployment/movie-search-api \\\n",
    "                 movie-search-api=gcr.io/$PROJECT_ID/$IMAGE_NAME:$IMAGE_TAG \\\n",
    "                 -n movie-search\n",
    "         echo \"Deployment updated successfully!\"\n",
    "     fi\n",
    "     ```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Image Tagging:** Tags the Docker image with both a specific commit hash and the `latest` tag for flexibility.\n",
    "  - **Image Pushing:** Uploads the built images to GCR, making them accessible to the Kubernetes cluster.\n",
    "  - **Deployment Update:** Optionally updates the Kubernetes Deployment to use the new image, facilitating seamless rollouts.\n",
    "\n",
    "### **B. Applying Kubernetes Manifests**\n",
    "\n",
    "```bash\n",
    "kubectl apply -f config/configmap.yaml\n",
    "kubectl apply -f config/secret.yaml\n",
    "kubectl apply -f config/fluent-bit-config.yaml\n",
    "kubectl apply -f config/deployment.yaml\n",
    "kubectl apply -f config/service.yaml\n",
    "kubectl apply -f config/hpa.yaml\n",
    "kubectl apply -f monitoring/prometheus-values.yaml\n",
    "kubectl apply -f monitoring/service-monitor.yaml\n",
    "kubectl apply -f monitoring/prometheus-rules.yaml\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Order of Deployment:** Apply ConfigMaps and Secrets first, followed by the Deployment and associated resources.\n",
    "  - **Prometheus and Monitoring:** Deploys Prometheus, Grafana, and Alertmanager configurations to set up comprehensive monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Design Choices and Justifications**\n",
    "\n",
    "### **A. Choice of FastAPI:**\n",
    "\n",
    "- **Reasons:**\n",
    "  - **Performance:** High-performance framework suitable for building APIs with asynchronous capabilities.\n",
    "  - **Ease of Use:** Intuitive syntax and automatic documentation generation with Swagger UI.\n",
    "  - **Modern Features:** Supports async programming, dependency injection, and type hinting.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Learning Curve:** Teams unfamiliar with FastAPI or asynchronous programming may require training.\n",
    "  - **Maturity:** While rapidly growing, FastAPI has a smaller ecosystem compared to more established frameworks like Django.\n",
    "\n",
    "### **B. Use of Pinecone for Vector Storage:**\n",
    "\n",
    "- **Advantages:**\n",
    "  - **Specialized Vector Database:** Optimized for storing and querying vector embeddings, enabling efficient semantic search.\n",
    "  - **Scalability:** Handles large-scale vector data with ease.\n",
    "  - **Integration:** Provides seamless integration with various ML and AI tools.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Cost:** Specialized services like Pinecone may incur higher costs compared to general-purpose databases.\n",
    "  - **Vendor Lock-In:** Dependency on Pinecone's proprietary systems may limit flexibility.\n",
    "\n",
    "### **C. Integration with OpenAI for Embeddings and Summarization:**\n",
    "\n",
    "- **Advantages:**\n",
    "  - **State-of-the-Art Models:** Leverages powerful models for generating high-quality embeddings and summaries.\n",
    "  - **Ease of Use:** Simplifies the implementation of complex NLP tasks without building models from scratch.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **API Costs:** Frequent API calls can lead to significant costs.\n",
    "  - **Latency:** Relying on external APIs introduces network latency, potentially affecting response times.\n",
    "  - **Dependency:** Dependency on OpenAI's service availability and changes in their API.\n",
    "\n",
    "### **D. Use of BigQuery for Data Storage:**\n",
    "\n",
    "- **Advantages:**\n",
    "  - **Scalability:** Handles large datasets efficiently with fast querying capabilities.\n",
    "  - **Integration:** Native integration with GCP services facilitates seamless data workflows.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Cost:** Querying large datasets can become expensive.\n",
    "  - **Complexity:** Requires understanding of SQL and BigQuery's pricing model for optimal usage.\n",
    "\n",
    "### **E. Prometheus and Grafana for Monitoring:**\n",
    "\n",
    "- **Advantages:**\n",
    "  - **Comprehensive Metrics:** Collects detailed metrics for performance monitoring and alerting.\n",
    "  - **Visualization:** Grafana provides rich dashboards for real-time insights.\n",
    "  - **Alerting:** Prometheus Alertmanager enables proactive notifications for critical issues.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Setup Complexity:** Configuring Prometheus and Grafana requires additional effort and expertise.\n",
    "  - **Maintenance:** Ongoing maintenance is necessary to ensure monitoring systems are operational and up-to-date.\n",
    "\n",
    "### **F. Kubernetes for Deployment:**\n",
    "\n",
    "- **Advantages:**\n",
    "  - **Scalability:** Automatically scales application instances based on demand.\n",
    "  - **Resilience:** Ensures high availability and fault tolerance.\n",
    "  - **Flexibility:** Supports complex deployment strategies like rolling updates and canary deployments.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Operational Complexity:** Requires expertise in Kubernetes for effective management.\n",
    "  - **Resource Overhead:** Running Kubernetes clusters involves additional resource consumption and costs.\n",
    "\n",
    "### **G. Structured Logging with Structlog:**\n",
    "\n",
    "- **Advantages:**\n",
    "  - **Enhanced Log Management:** Produces structured logs that are easier to parse and analyze.\n",
    "  - **Traceability:** Facilitates tracking of requests and debugging by embedding contextual information.\n",
    "\n",
    "- **Trade-offs:**\n",
    "  - **Configuration Complexity:** Setting up structured logging requires careful configuration.\n",
    "  - **Storage and Analysis:** Structured logs may require specialized storage and analysis tools to fully leverage their benefits.\n",
    "\n",
    "---\n",
    "\n",
    "## **11. Trade-Offs and Alternative Approaches**\n",
    "\n",
    "### **A. Alternative Orchestration Tools:**\n",
    "\n",
    "1. **Flask:**\n",
    "   - **Pros:**\n",
    "     - Simplicity and minimalism for small-scale applications.\n",
    "     - Extensive ecosystem and community support.\n",
    "   - **Cons:**\n",
    "     - Lacks native support for asynchronous programming.\n",
    "     - Requires additional libraries for features like input validation and documentation.\n",
    "   - **Trade-Offs:**\n",
    "     - **Flexibility vs. Performance:** FastAPI offers better performance and modern features but Flask provides simplicity for less demanding applications.\n",
    "\n",
    "2. **Django:**\n",
    "   - **Pros:**\n",
    "     - Comprehensive framework with built-in ORM, admin interface, and authentication.\n",
    "     - Suitable for full-stack applications.\n",
    "   - **Cons:**\n",
    "     - Overhead for APIs that don't require full-stack features.\n",
    "     - Less optimized for asynchronous operations compared to FastAPI.\n",
    "   - **Trade-Offs:**\n",
    "     - **Feature-Rich vs. Lightweight:** Django provides extensive features out-of-the-box but may introduce unnecessary complexity for API-centric projects.\n",
    "\n",
    "### **B. Alternative Vector Databases:**\n",
    "\n",
    "1. **Weaviate:**\n",
    "   - **Pros:**\n",
    "     - Open-source and offers rich features like real-time data ingestion and GraphQL API.\n",
    "     - Extensible with custom modules.\n",
    "   - **Cons:**\n",
    "     - Requires self-hosting, increasing operational overhead.\n",
    "     - Smaller community compared to Pinecone.\n",
    "   - **Trade-Offs:**\n",
    "     - **Control vs. Convenience:** Weaviate offers more control and customization at the expense of easier scalability and maintenance provided by managed services like Pinecone.\n",
    "\n",
    "2. **FAISS (Facebook AI Similarity Search):**\n",
    "   - **Pros:**\n",
    "     - High-performance similarity search library.\n",
    "     - Open-source and highly customizable.\n",
    "   - **Cons:**\n",
    "     - Not a managed service; requires infrastructure to deploy and scale.\n",
    "     - Limited built-in features for real-time updates and metadata handling.\n",
    "   - **Trade-Offs:**\n",
    "     - **Performance vs. Ease of Use:** FAISS provides excellent performance but lacks the ease of integration and management that Pinecone offers.\n",
    "\n",
    "### **C. Alternative Monitoring Tools:**\n",
    "\n",
    "1. **Datadog:**\n",
    "   - **Pros:**\n",
    "     - Comprehensive monitoring, tracing, and logging in a single platform.\n",
    "     - Easy integration with Kubernetes and cloud services.\n",
    "   - **Cons:**\n",
    "     - Can be expensive, especially at scale.\n",
    "     - Less flexibility for custom metrics compared to Prometheus.\n",
    "   - **Trade-Offs:**\n",
    "     - **All-in-One Solution vs. Open-Source Flexibility:** Datadog provides a unified solution with ease of setup but at a higher cost and potentially less customization.\n",
    "\n",
    "2. **Elastic Stack (ELK):**\n",
    "   - **Pros:**\n",
    "     - Powerful log aggregation and analysis capabilities.\n",
    "     - Flexible and customizable dashboards with Kibana.\n",
    "   - **Cons:**\n",
    "     - Requires significant resources to manage and scale.\n",
    "     - Complexity in setup and maintenance.\n",
    "   - **Trade-Offs:**\n",
    "     - **Power vs. Complexity:** ELK offers robust features but introduces higher operational complexity compared to Prometheus and Grafana.\n",
    "\n",
    "### **D. Serverless Deployment:**\n",
    "\n",
    "1. **AWS Lambda / Google Cloud Functions:**\n",
    "   - **Pros:**\n",
    "     - Automatic scaling based on demand.\n",
    "     - Reduced operational overhead as server management is abstracted away.\n",
    "   - **Cons:**\n",
    "     - Limited execution time, which may not suit all workloads.\n",
    "     - Complexity in managing state and dependencies across functions.\n",
    "   - **Trade-Offs:**\n",
    "     - **Operational Simplicity vs. Orchestration Power:** Serverless functions simplify deployment and scaling for event-driven tasks but may lack the orchestration and state management capabilities required for more complex APIs.\n",
    "\n",
    "### **E. Container Orchestration Alternatives:**\n",
    "\n",
    "1. **Docker Swarm:**\n",
    "   - **Pros:**\n",
    "     - Simpler to set up compared to Kubernetes.\n",
    "     - Native Docker integration.\n",
    "   - **Cons:**\n",
    "     - Less feature-rich and scalable than Kubernetes.\n",
    "     - Smaller community and ecosystem support.\n",
    "   - **Trade-Offs:**\n",
    "     - **Simplicity vs. Scalability:** Docker Swarm offers ease of use for smaller deployments but lacks the extensive scalability and features provided by Kubernetes.\n",
    "\n",
    "---\n",
    "\n",
    "## **12. Best Practices and Recommendations**\n",
    "\n",
    "### **A. Security Best Practices:**\n",
    "\n",
    "1. **Secret Management:**\n",
    "   - **Use Kubernetes Secrets:** Securely store sensitive data like API keys.\n",
    "   - **Avoid Hardcoding:** Never hardcode secrets in code or configuration files.\n",
    "   - **Access Controls:** Implement Role-Based Access Control (RBAC) to restrict access to secrets and sensitive resources.\n",
    "\n",
    "2. **Network Security:**\n",
    "   - **Restrict CORS:** Limit allowed origins in CORS middleware to trusted domains in production.\n",
    "   - **Use HTTPS:** Ensure all API endpoints are accessible over HTTPS to encrypt data in transit.\n",
    "\n",
    "3. **Resource Limits:**\n",
    "   - **Define Resource Requests and Limits:** Prevent resource exhaustion and ensure fair resource distribution among pods.\n",
    "   - **Monitor Usage:** Regularly monitor resource utilization to adjust allocations as needed.\n",
    "\n",
    "### **B. Performance Optimization:**\n",
    "\n",
    "1. **Batching and Caching:**\n",
    "   - **Batch API Calls:** Reduce latency and cost by batching embedding generation and summarization requests.\n",
    "   - **Implement Caching:** Cache frequent queries and embeddings to minimize redundant API calls and improve response times.\n",
    "\n",
    "2. **Asynchronous Processing:**\n",
    "   - **Leverage Async Capabilities:** Utilize FastAPI's asynchronous features to handle multiple concurrent requests efficiently.\n",
    "   - **Optimize Thread Pools:** Tune the number of workers in `ThreadPoolExecutor` and `ProcessPoolExecutor` based on workload and system resources.\n",
    "\n",
    "3. **Efficient Data Retrieval:**\n",
    "   - **Optimize BigQuery Queries:** Use appropriate indexing and partitioning to speed up data retrieval.\n",
    "   - **Limit Data Transfer:** Fetch only necessary fields from BigQuery to reduce data transfer overhead.\n",
    "\n",
    "### **C. Scalability Considerations:**\n",
    "\n",
    "1. **Horizontal Scaling:**\n",
    "   - **Use HPA:** Employ Horizontal Pod Autoscaler to automatically scale the number of pods based on CPU utilization.\n",
    "   - **Distribute Load:** Ensure the LoadBalancer service can handle incoming traffic by distributing it evenly across pods.\n",
    "\n",
    "2. **Stateless Design:**\n",
    "   - **Maintain Statelessness:** Design the API to be stateless to facilitate easy scaling and load balancing.\n",
    "   - **Externalize State:** Use external services like Pinecone and BigQuery to manage stateful data.\n",
    "\n",
    "### **D. Observability and Monitoring:**\n",
    "\n",
    "1. **Comprehensive Metrics:**\n",
    "   - **Track Key Metrics:** Monitor request rates, latencies, error rates, and specific operation durations.\n",
    "   - **Set Up Dashboards:** Use Grafana to visualize metrics and gain real-time insights into API performance.\n",
    "\n",
    "2. **Alerting and Incident Response:**\n",
    "   - **Configure Alerts:** Set up Prometheus Alertmanager to notify stakeholders of critical issues via Slack or other channels.\n",
    "   - **Define Alert Rules:** Create meaningful alert rules to avoid alert fatigue and ensure timely responses to genuine issues.\n",
    "\n",
    "3. **Logging Practices:**\n",
    "   - **Structured Logging:** Use structured logs for better searchability and analysis.\n",
    "   - **Log Rotation:** Implement log rotation policies to manage log storage and prevent disk space exhaustion.\n",
    "\n",
    "### **E. Testing and Validation:**\n",
    "\n",
    "1. **Unit Testing:**\n",
    "   - **Test Core Functions:** Implement unit tests for functions like `get_embedding`, `search_pinecone`, and `generate_summary`.\n",
    "   - **Mock External Services:** Use mocking frameworks to simulate responses from OpenAI, Pinecone, and BigQuery during testing.\n",
    "\n",
    "2. **Integration Testing:**\n",
    "   - **End-to-End Tests:** Validate the entire workflow from search request to summary generation.\n",
    "   - **Performance Testing:** Assess the API's performance under various load conditions to identify bottlenecks.\n",
    "\n",
    "3. **Continuous Integration/Continuous Deployment (CI/CD):**\n",
    "   - **Automate Testing:** Integrate testing into the CI pipeline to ensure code quality.\n",
    "   - **Automate Deployments:** Use CI/CD tools to automate the building, testing, and deployment of the application.\n",
    "\n",
    "### **F. Documentation and Maintainability:**\n",
    "\n",
    "1. **API Documentation:**\n",
    "   - **Leverage FastAPI's Auto-Generated Docs:** Provide interactive Swagger UI and ReDoc documentation for API consumers.\n",
    "   - **Enhance Documentation:** Add detailed descriptions, examples, and usage guidelines to improve developer experience.\n",
    "\n",
    "2. **Code Documentation:**\n",
    "   - **Use Docstrings:** Provide clear and concise docstrings for functions and classes.\n",
    "   - **Maintain Readability:** Follow consistent coding standards and practices to enhance code readability and maintainability.\n",
    "\n",
    "3. **Version Control:**\n",
    "   - **Use Git:** Maintain code in a version-controlled repository to track changes and collaborate effectively.\n",
    "   - **Implement Branching Strategies:** Use feature branches, pull requests, and code reviews to manage code quality and collaboration.\n",
    "\n",
    "---\n",
    "\n",
    "## **13. Conclusion**\n",
    "\n",
    "- **Recap of API Strengths:**\n",
    "  - **Semantic Understanding:** Leverages vector embeddings to provide contextually relevant search results.\n",
    "  - **Advanced Summarization:** Utilizes state-of-the-art language models to generate concise summaries.\n",
    "  - **Scalability and Reliability:** Deploys on Kubernetes with autoscaling and robust monitoring to handle varying loads.\n",
    "  - **Comprehensive Monitoring:** Implements Prometheus and Grafana for detailed insights and proactive alerting.\n",
    "\n",
    "- **Acknowledgment of Trade-Offs:**\n",
    "  - **Cost vs. Performance:** Balancing the costs associated with managed services like Pinecone and OpenAI APIs against the performance and scalability benefits they provide.\n",
    "  - **Operational Complexity:** Managing Kubernetes clusters and monitoring tools introduces additional operational responsibilities.\n",
    "  - **Dependency Management:** Reliance on external services necessitates careful handling of API changes and service availability.\n",
    "\n",
    "- **Future Enhancements:**\n",
    "  - **Enhanced Caching Mechanisms:** Implementing more sophisticated caching strategies to further reduce latency and API costs.\n",
    "  - **Advanced Security Measures:** Incorporating authentication and authorization to secure API endpoints.\n",
    "  - **Expanded Monitoring:** Adding more granular metrics and dashboards to capture a wider range of performance indicators.\n",
    "  - **Feature Extensions:** Introducing additional features like personalized recommendations, user authentication, and more detailed movie analytics.\n",
    "\n",
    "- **Closing Remarks:**\n",
    "  - Emphasize the API's capability to deliver high-quality, semantically relevant movie search and summarization services.\n",
    "  - Highlight the importance of scalable, maintainable, and secure API design in modern data-driven applications.\n",
    "\n",
    "---\n",
    "\n",
    "## **14. Q&A Session**\n",
    "\n",
    "- **Invite Questions:**\n",
    "  - Encourage the audience to ask about specific components, design decisions, or implementation challenges.\n",
    "\n",
    "- **Prepare for Common Questions:**\n",
    "  - **Scalability:** How does the API handle increasing data volumes and user requests?\n",
    "  - **Cost Management:** What strategies are in place to control costs associated with OpenAI and Pinecone APIs?\n",
    "  - **Security:** How are sensitive data and API keys protected within the application and Kubernetes cluster?\n",
    "  - **Error Handling:** What happens if external services like OpenAI or Pinecone experience downtime?\n",
    "  - **Performance Optimization:** How can the API's performance be further improved?\n",
    "\n",
    "---\n",
    "\n",
    "## **15. Appendix: Key Code Snippets and Explanations**\n",
    "\n",
    "### **A. Pipeline Configuration Class**\n",
    "\n",
    "```python\n",
    "class PipelineConfig:\n",
    "    def __init__(self):\n",
    "        self.gcs_bucket = Variable.get('GCS_BUCKET')\n",
    "        self.input_path = Variable.get('INPUT_PATH')\n",
    "        self.project_id = Variable.get('GCP_PROJECT_ID')\n",
    "        self.bq_dataset = Variable.get('BQ_DATASET')\n",
    "        self.full_text_table = f\"{self.project_id}.{self.bq_dataset}.full_text\"\n",
    "        self.metadata_table = f\"{self.project_id}.{self.bq_dataset}.metadata\"\n",
    "        self.dropped_table = f\"{self.project_id}.{self.bq_dataset}.dropped\"\n",
    "        self.pinecone_api_key = Variable.get('PINECONE_API_KEY')\n",
    "        self.pinecone_env = Variable.get('PINECONE_ENV')\n",
    "        self.index_name = Variable.get('PINECONE_INDEX_NAME')\n",
    "        self.openai_api_key = Variable.get('OPENAI_API_KEY')\n",
    "        self.num_processes = NUM_CORES\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Centralized Configuration:** Retrieves all necessary configuration parameters from environment variables, enhancing flexibility and maintainability.\n",
    "  - **Avoids Hardcoding:** Prevents the use of hardcoded values, making the application adaptable to different environments and configurations.\n",
    "\n",
    "### **B. Embedding Generation with Parallel Processing**\n",
    "\n",
    "```python\n",
    "def parallel_generate_embeddings(texts: List[str], openai_client: OpenAI) -> List[List[float]]:\n",
    "    encoder = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    def process_batch(batch_texts):\n",
    "        retry_count = 0\n",
    "        current_batch_size = len(batch_texts)\n",
    "        \n",
    "        while retry_count < MAX_RETRIES:\n",
    "            try:\n",
    "                response = openai_client.embeddings.create(\n",
    "                    input=batch_texts,\n",
    "                    model=\"text-embedding-ada-002\"\n",
    "                )\n",
    "                return [item.embedding for item in response.data]\n",
    "            except Exception as e:\n",
    "                retry_count += 1\n",
    "                logger.error(f\"Error in batch embedding: {e}. Retry {retry_count}/{MAX_RETRIES}\")\n",
    "                if retry_count == MAX_RETRIES:\n",
    "                    return [None] * current_batch_size\n",
    "                time.sleep(2 ** retry_count)\n",
    "    \n",
    "    # Split long texts and track their original indices\n",
    "    processed_texts = []\n",
    "    text_map = {}  # Maps new indices to original indices\n",
    "    current_idx = 0\n",
    "    \n",
    "    for idx, text in enumerate(texts):\n",
    "        chunks = split_text_by_tokens(text, encoder)\n",
    "        for chunk in chunks:\n",
    "            processed_texts.append(chunk)\n",
    "            text_map[current_idx] = {'original_idx': idx, 'total_chunks': len(chunks)}\n",
    "            current_idx += 1\n",
    "    \n",
    "    # Process all chunks in parallel batches\n",
    "    batches = list(batch_generator(processed_texts, EMBEDDING_BATCH_SIZE))\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=min(8, len(batches))) as executor:\n",
    "        batch_results = list(executor.map(process_batch, batches))\n",
    "    \n",
    "    # Flatten batch results\n",
    "    chunk_embeddings = []\n",
    "    for batch in batch_results:\n",
    "        if batch:\n",
    "            chunk_embeddings.extend(batch)\n",
    "    \n",
    "    # Combine embeddings for chunks from the same original text\n",
    "    final_embeddings = [None] * len(texts)\n",
    "    current_original_idx = -1\n",
    "    current_chunks = []\n",
    "    \n",
    "    for i, embedding in enumerate(chunk_embeddings):\n",
    "        if embedding is None:\n",
    "            continue\n",
    "            \n",
    "        original_idx = text_map[i]['original_idx']\n",
    "        total_chunks = text_map[i]['total_chunks']\n",
    "        \n",
    "        if original_idx != current_original_idx:\n",
    "            # Process previous chunks if any\n",
    "            if current_chunks:\n",
    "                final_embeddings[current_original_idx] = np.mean(current_chunks, axis=0).tolist()\n",
    "            # Start new chunk collection\n",
    "            current_original_idx = original_idx\n",
    "            current_chunks = [embedding]\n",
    "        else:\n",
    "            current_chunks.append(embedding)\n",
    "        \n",
    "        # Process last chunk if it's all chunks for this text\n",
    "        if len(current_chunks) == total_chunks:\n",
    "            final_embeddings[current_original_idx] = np.mean(current_chunks, axis=0).tolist()\n",
    "            current_chunks = []\n",
    "    \n",
    "    # Process any remaining chunks\n",
    "    if current_chunks:\n",
    "        final_embeddings[current_original_idx] = np.mean(current_chunks, axis=0).tolist()\n",
    "    \n",
    "    return final_embeddings\n",
    "```\n",
    "\n",
    "- **Explanation:**\n",
    "  - **Batch Processing:** Divides texts into manageable batches to optimize OpenAI API usage.\n",
    "  - **Retry Mechanism:** Implements exponential backoff to handle transient API failures, enhancing reliability.\n",
    "  - **Embedding Aggregation:** Averages embeddings from split chunks to maintain consistency for original texts.\n",
    "  - **Parallelism:** Utilizes `ThreadPoolExecutor` to perform concurrent API calls, speeding up the embedding generation process.\n",
    "  - **Text Splitting:** Ensures that long texts are split into chunks that comply with OpenAI's token limits while maintaining contextual integrity through overlapping tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## **16. Final Tips for the Presentation**\n",
    "\n",
    "### **A. Engage the Audience:**\n",
    "- **Use Visuals:** Incorporate flowcharts, diagrams, and screenshots of dashboards to illustrate the API workflow and monitoring setup.\n",
    "- **Demonstrate Live Examples:** Show live API requests and responses to demonstrate functionality and performance.\n",
    "- **Highlight Real-World Benefits:** Emphasize how the API improves user experience through semantic search and summarization.\n",
    "\n",
    "### **B. Explain Complex Concepts Clearly:**\n",
    "- **Break Down Processes:** Simplify explanations of technical processes like embedding generation, vector search, and summarization.\n",
    "- **Use Analogies:** Relate complex concepts to familiar ideas to enhance understanding.\n",
    "\n",
    "### **C. Showcase Monitoring and Observability:**\n",
    "- **Demonstrate Dashboards:** Walk through Grafana dashboards showing key metrics and alerts.\n",
    "- **Explain Alerting Mechanisms:** Discuss how Prometheus Alertmanager integrates with Slack for real-time notifications.\n",
    "\n",
    "### **D. Address Trade-Offs Transparently:**\n",
    "- **Be Honest About Limitations:** Acknowledge areas where the current setup may face challenges or limitations.\n",
    "- **Discuss Future Improvements:** Share ideas for enhancing the API, such as implementing caching, adding authentication, or expanding monitoring capabilities.\n",
    "\n",
    "### **E. Practice and Rehearse:**\n",
    "- **Time Management:** Ensure each section of the presentation fits within the allocated time.\n",
    "- **Anticipate Questions:** Prepare answers for potential questions regarding scalability, cost management, security, and technology choices.\n",
    "\n",
    "### **F. Provide Takeaways:**\n",
    "- **Summarize Key Points:** Reinforce the main strengths and innovations of the API.\n",
    "- **Highlight Impact:** Discuss the positive impact on users and potential business benefits.\n",
    "\n",
    "---\n",
    "\n",
    "## **17. Additional Recommendations**\n",
    "\n",
    "### **A. Implement Authentication and Authorization:**\n",
    "- **Secure Endpoints:** Protect the API with authentication mechanisms like API keys or OAuth to prevent unauthorized access.\n",
    "- **Role-Based Access Control (RBAC):** Implement RBAC to restrict access to certain functionalities based on user roles.\n",
    "\n",
    "### **B. Enhance API Performance:**\n",
    "- **Implement Caching:** Use caching strategies (e.g., Redis) to store frequently accessed data and reduce redundant API calls.\n",
    "- **Optimize Database Queries:** Fine-tune BigQuery queries for faster data retrieval and lower costs.\n",
    "\n",
    "### **C. Expand API Features:**\n",
    "- **Personalized Recommendations:** Incorporate user-specific data to provide personalized movie recommendations.\n",
    "- **Advanced Search Filters:** Allow users to filter search results based on genres, ratings, release dates, etc.\n",
    "\n",
    "### **D. Continuous Improvement:**\n",
    "- **Collect User Feedback:** Gather feedback from API users to identify areas for improvement.\n",
    "- **Regular Updates:** Keep dependencies and services updated to benefit from the latest features and security patches.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training or fine-tuning a model\n",
    "\n",
    "Certainly! Developing a model to function as an effective customer service agent involves multiple stages, including data preparation, model training or fine-tuning, and deployment. Deploying the model to cloud platforms like **Google Cloud Platform (GCP)** or **Amazon Web Services (AWS)** ensures scalability, reliability, and accessibility. Below are comprehensive notes detailing both approaches—**fine-tuning an existing model** and **training a model from scratch**—along with detailed steps to deploy the trained model to GCP and AWS.\n",
    "\n",
    "---\n",
    "\n",
    "## **Table of Contents**\n",
    "\n",
    "1. [Introduction](#1-introduction)\n",
    "2. [Approach 1: Fine-Tuning an Existing Pre-Trained Model](#2-approach-1-fine-tuning-an-existing-pre-trained-model)\n",
    "    - [2.1. Benefits of Fine-Tuning](#21-benefits-of-fine-tuning)\n",
    "    - [2.2. Steps to Fine-Tune](#22-steps-to-fine-tune)\n",
    "        - [Step 1: Select an Appropriate Base Model](#step-1-select-an-appropriate-base-model)\n",
    "        - [Step 2: Collect and Prepare Training Data](#step-2-collect-and-prepare-training-data)\n",
    "        - [Step 3: Data Preprocessing and Annotation](#step-3-data-preprocessing-and-annotation)\n",
    "        - [Step 4: Define the Training Objectives](#step-4-define-the-training-objectives)\n",
    "        - [Step 5: Fine-Tuning the Model](#step-5-fine-tuning-the-model)\n",
    "        - [Step 6: Evaluate and Validate the Model](#step-6-evaluate-and-validate-the-model)\n",
    "        - [Step 7: Deployment and Integration](#step-7-deployment-and-integration)\n",
    "    - [2.3. Tools and Platforms for Fine-Tuning](#23-tools-and-platforms-for-fine-tuning)\n",
    "3. [Approach 2: Training a Model from Scratch](#3-approach-2-training-a-model-from-scratch)\n",
    "    - [3.1. Benefits and Challenges of Training from Scratch](#31-benefits-and-challenges-of-training-from-scratch)\n",
    "    - [3.2. Steps to Train from Scratch](#32-steps-to-train-from-scratch)\n",
    "        - [Step 1: Define the Model Architecture](#step-1-define-the-model-architecture)\n",
    "        - [Step 2: Gather and Curate a Large-Scale Dataset](#step-2-gather-and-curate-a-large-scale-dataset)\n",
    "        - [Step 3: Data Preprocessing and Cleaning](#step-3-data-preprocessing-and-cleaning)\n",
    "        - [Step 4: Tokenization and Vocabulary Building](#step-4-tokenization-and-vocabulary-building)\n",
    "        - [Step 5: Training the Model](#step-5-training-the-model)\n",
    "        - [Step 6: Fine-Tuning and Optimization](#step-6-fine-tuning-and-optimization)\n",
    "        - [Step 7: Evaluation and Testing](#step-7-evaluation-and-testing)\n",
    "        - [Step 8: Deployment and Scaling](#step-8-deployment-and-scaling)\n",
    "    - [3.3. Tools and Frameworks for Training from Scratch](#33-tools-and-frameworks-for-training-from-scratch)\n",
    "4. [Comparison: Fine-Tuning vs. Training from Scratch](#4-comparison-fine-tuning-vs-training-from-scratch)\n",
    "5. [Best Practices for Developing a Customer Service Agent Model](#5-best-practices-for-developing-a-customer-service-agent-model)\n",
    "6. [Ethical Considerations](#6-ethical-considerations)\n",
    "7. [Deployment Steps](#7-deployment-steps)\n",
    "    - [7.1. Deploying to Google Cloud Platform (GCP)](#71-deploying-to-google-cloud-platform-gcp)\n",
    "        - [7.1.1. Using Google Cloud AI Platform](#711-using-google-cloud-ai-platform)\n",
    "        - [7.1.2. Using Google Kubernetes Engine (GKE)](#712-using-google-kubernetes-engine-gke)\n",
    "    - [7.2. Deploying to Amazon Web Services (AWS)](#72-deploying-to-amazon-web-services-aws)\n",
    "        - [7.2.1. Using AWS SageMaker](#721-using-aws-sagemaker)\n",
    "        - [7.2.2. Using AWS Elastic Kubernetes Service (EKS)](#722-using-aws-elastic-kubernetes-service-eks)\n",
    "        - [7.2.3. Using AWS Lambda with API Gateway](#723-using-aws-lambda-with-api-gateway)\n",
    "8. [Conclusion](#8-conclusion)\n",
    "9. [Q&A Session](#9-qa-session)\n",
    "10. [Appendix: Additional Resources](#10-appendix-additional-resources)\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Introduction**\n",
    "\n",
    "Developing a customer service agent using machine learning involves creating a system capable of understanding and responding to customer inquiries effectively. The two primary approaches to building such a model are:\n",
    "\n",
    "1. **Fine-Tuning an Existing Pre-Trained Model:** Leveraging a model that has already been trained on large datasets and adapting it to the specific nuances of customer service interactions.\n",
    "2. **Training a Model from Scratch:** Building and training a new model specifically tailored to customer service tasks without relying on pre-trained weights.\n",
    "\n",
    "Deploying the trained model to cloud platforms like **Google Cloud Platform (GCP)** or **Amazon Web Services (AWS)** ensures scalability, reliability, and accessibility. This guide provides comprehensive steps for both approaches, including deployment strategies to GCP and AWS.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Approach 1: Fine-Tuning an Existing Pre-Trained Model**\n",
    "\n",
    "### **2.1. Benefits of Fine-Tuning**\n",
    "\n",
    "- **Resource Efficiency:** Requires significantly less computational power and data compared to training from scratch.\n",
    "- **Time Savings:** Reduces development time as the base model already possesses general language understanding capabilities.\n",
    "- **Performance:** Leverages the extensive knowledge and patterns learned during the base model's pre-training.\n",
    "- **Flexibility:** Allows customization to specific domains (e.g., customer service) without reinventing the wheel.\n",
    "\n",
    "### **2.2. Steps to Fine-Tune**\n",
    "\n",
    "#### **Step 1: Select an Appropriate Base Model**\n",
    "\n",
    "Choose a pre-trained language model that best fits your requirements. Consider factors like model size, performance, and licensing.\n",
    "\n",
    "- **Popular Choices:**\n",
    "    - **GPT Series (e.g., GPT-3, GPT-4):** Excellent for generative tasks and dialogue systems.\n",
    "    - **BERT and its Variants (e.g., RoBERTa, DistilBERT):** Ideal for understanding and classification tasks.\n",
    "    - **T5 (Text-to-Text Transfer Transformer):** Versatile for various NLP tasks by treating them as text generation problems.\n",
    "\n",
    "- **Considerations:**\n",
    "    - **Model Size vs. Performance:** Larger models typically perform better but require more resources.\n",
    "    - **Licensing and Usage Restrictions:** Ensure compliance with the model's licensing terms.\n",
    "    - **Framework Compatibility:** Ensure the model is compatible with your chosen ML framework (e.g., HuggingFace Transformers).\n",
    "\n",
    "#### **Step 2: Collect and Prepare Training Data**\n",
    "\n",
    "Gather a dataset that reflects the kind of interactions the customer service agent will handle.\n",
    "\n",
    "- **Data Sources:**\n",
    "    - **Historical Customer Service Logs:** Emails, chat transcripts, call transcripts.\n",
    "    - **Public Datasets:** Datasets like [Customer Support on Twitter](https://github.com/hmason/CS-twitter), [Ubuntu Dialogue Corpus](https://github.com/rkadlec/ubuntu-ranking-dataset-creator).\n",
    "    - **Synthetic Data:** Generate simulated conversations if real data is scarce.\n",
    "\n",
    "- **Data Quantity:**\n",
    "    - **Minimum Requirement:** Several thousand well-annotated examples.\n",
    "    - **Optimal Range:** Tens of thousands to millions, depending on complexity and desired performance.\n",
    "\n",
    "#### **Step 3: Data Preprocessing and Annotation**\n",
    "\n",
    "Ensure the data is clean, consistent, and properly annotated.\n",
    "\n",
    "- **Cleaning:**\n",
    "    - Remove any personally identifiable information (PII) to maintain privacy.\n",
    "    - Correct typos, grammatical errors, and standardize language usage.\n",
    "    - Remove irrelevant content or noise from conversations.\n",
    "\n",
    "- **Annotation:**\n",
    "    - **Intent Classification:** Label the intent behind each customer query (e.g., \"Password Reset,\" \"Billing Inquiry\").\n",
    "    - **Entity Recognition:** Identify key entities within the queries (e.g., \"Order Number,\" \"Product Name\").\n",
    "    - **Response Generation:** Ensure that each customer query is paired with an appropriate and helpful response.\n",
    "\n",
    "- **Formatting:**\n",
    "    - Structure data in a consistent format (e.g., JSON, CSV) with clearly defined fields for inputs and outputs.\n",
    "\n",
    "#### **Step 4: Define the Training Objectives**\n",
    "\n",
    "Clearly outline what you aim to achieve with the fine-tuned model.\n",
    "\n",
    "- **Primary Objectives:**\n",
    "    - **Understanding Customer Queries:** Accurately interpret the intent and context of customer inquiries.\n",
    "    - **Generating Appropriate Responses:** Provide accurate, helpful, and contextually relevant answers.\n",
    "    - **Maintaining Conversational Flow:** Ensure smooth and natural interactions without abrupt topic shifts.\n",
    "\n",
    "- **Secondary Objectives:**\n",
    "    - **Handling Ambiguity:** Gracefully manage unclear or vague queries by seeking clarification.\n",
    "    - **Multi-Turn Conversations:** Maintain context across multiple interactions within the same conversation.\n",
    "    - **Language Support:** Support multiple languages if required.\n",
    "\n",
    "#### **Step 5: Fine-Tuning the Model**\n",
    "\n",
    "Utilize frameworks like **HuggingFace Transformers** to fine-tune the selected base model on your dataset.\n",
    "\n",
    "- **Environment Setup:**\n",
    "    - **Hardware:** Preferably GPUs or TPUs to expedite training.\n",
    "    - **Software:** Install necessary libraries (`transformers`, `datasets`, `torch`, etc.).\n",
    "\n",
    "- **Fine-Tuning Process:**\n",
    "    1. **Load the Pre-Trained Model and Tokenizer:**\n",
    "        ```python\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "        model_name = \"gpt-3\"  # Example model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        ```\n",
    "\n",
    "    2. **Prepare the Dataset:**\n",
    "        ```python\n",
    "        from datasets import load_dataset\n",
    "\n",
    "        dataset = load_dataset(\"your_dataset\")\n",
    "        ```\n",
    "\n",
    "    3. **Tokenize the Data:**\n",
    "        ```python\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "        tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "        ```\n",
    "\n",
    "    4. **Define Training Arguments:**\n",
    "        ```python\n",
    "        from transformers import TrainingArguments\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=3,\n",
    "            weight_decay=0.01,\n",
    "        )\n",
    "        ```\n",
    "\n",
    "    5. **Initialize the Trainer:**\n",
    "        ```python\n",
    "        from transformers import Trainer\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        )\n",
    "        ```\n",
    "\n",
    "    6. **Start Fine-Tuning:**\n",
    "        ```python\n",
    "        trainer.train()\n",
    "        ```\n",
    "\n",
    "- **Hyperparameter Tuning:**\n",
    "    - **Learning Rate:** Critical for convergence; typically between `1e-5` and `5e-5`.\n",
    "    - **Batch Size:** Balances memory usage and training stability; commonly between 16 and 64.\n",
    "    - **Number of Epochs:** Usually between 3 to 10; monitor for overfitting.\n",
    "    - **Weight Decay:** Helps prevent overfitting; common values range from `0.01` to `0.1`.\n",
    "\n",
    "#### **Step 6: Evaluate and Validate the Model**\n",
    "\n",
    "Assess the performance of the fine-tuned model to ensure it meets desired standards.\n",
    "\n",
    "- **Evaluation Metrics:**\n",
    "    - **Accuracy:** Measures the correctness of intent classification.\n",
    "    - **F1 Score:** Balances precision and recall, especially useful for imbalanced datasets.\n",
    "    - **BLEU Score:** Evaluates the quality of generated responses against reference answers.\n",
    "    - **Perplexity:** Assesses the model's confidence in predictions.\n",
    "    - **Human Evaluation:** Subjective assessment by human reviewers for response relevance and helpfulness.\n",
    "\n",
    "- **Validation Process:**\n",
    "    - Split the dataset into training, validation, and test sets.\n",
    "    - Perform evaluations on the validation set during training to monitor performance.\n",
    "    - Conduct final evaluations on the unseen test set to gauge real-world performance.\n",
    "\n",
    "#### **Step 7: Deployment and Integration**\n",
    "\n",
    "Deploy the fine-tuned model to serve customer service requests.\n",
    "\n",
    "- **Packaging the Model:**\n",
    "    - Save the fine-tuned model and tokenizer.\n",
    "        ```python\n",
    "        model.save_pretrained(\"./fine-tuned-model\")\n",
    "        tokenizer.save_pretrained(\"./fine-tuned-model\")\n",
    "        ```\n",
    "\n",
    "- **Deployment Strategies:**\n",
    "    - **REST API:** Serve the model via a RESTful API using frameworks like **FastAPI** or **Flask**.\n",
    "    - **Serverless Deployment:** Utilize serverless platforms to scale automatically based on demand.\n",
    "    - **Containerization:** Package the model into Docker containers for consistent deployment across environments.\n",
    "\n",
    "---\n",
    "\n",
    "### **2.3. Tools and Platforms for Fine-Tuning**\n",
    "\n",
    "- **HuggingFace Transformers:** Comprehensive library for natural language processing tasks, offering pre-trained models and fine-tuning utilities.\n",
    "- **PyTorch / TensorFlow:** Popular deep learning frameworks for model training and deployment.\n",
    "- **Weights & Biases / TensorBoard:** Tools for tracking experiments, visualizing metrics, and managing hyperparameters.\n",
    "- **Cloud Platforms:** GCP, AWS, or Azure for scalable compute resources (GPUs/TPUs).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Approach 2: Training a Model from Scratch**\n",
    "\n",
    "### **3.1. Benefits and Challenges of Training from Scratch**\n",
    "\n",
    "- **Benefits:**\n",
    "    - **Customization:** Full control over model architecture tailored to specific requirements.\n",
    "    - **Domain-Specific Knowledge:** Ability to incorporate specialized knowledge directly into the model.\n",
    "    - **No Dependency on External Models:** Complete independence from pre-trained models and their limitations.\n",
    "\n",
    "- **Challenges:**\n",
    "    - **Resource Intensive:** Requires significant computational power and large datasets.\n",
    "    - **Time-Consuming:** Longer development and training times compared to fine-tuning.\n",
    "    - **Expertise Required:** Necessitates deep understanding of model architectures, training dynamics, and optimization techniques.\n",
    "    - **Risk of Lower Performance:** Without extensive data and training, the model may underperform compared to fine-tuned counterparts.\n",
    "\n",
    "### **3.2. Steps to Train from Scratch**\n",
    "\n",
    "#### **Step 1: Define the Model Architecture**\n",
    "\n",
    "Design a neural network architecture suitable for customer service tasks.\n",
    "\n",
    "- **Considerations:**\n",
    "    - **Sequence-to-Sequence Models:** For generating responses based on input queries.\n",
    "    - **Attention Mechanisms:** To capture contextual relationships within conversations.\n",
    "    - **Transformer-Based Architectures:** State-of-the-art for handling long-range dependencies and contextual understanding.\n",
    "\n",
    "- **Example Architecture:**\n",
    "    - **Encoder-Decoder Model:** Encodes the input query and decodes it into a coherent response.\n",
    "    - **Transformer Blocks:** Utilize multi-head attention and feed-forward networks for processing.\n",
    "\n",
    "#### **Step 2: Gather and Curate a Large-Scale Dataset**\n",
    "\n",
    "Acquire a substantial dataset to train the model effectively.\n",
    "\n",
    "- **Data Sources:**\n",
    "    - **Customer Service Transcripts:** Historical records from customer interactions.\n",
    "    - **Public Dialogue Datasets:** Such as [Persona-Chat](https://github.com/facebookresearch/ParlAI/tree/master/projects/personachat) or [ConvAI2](https://github.com/facebookresearch/ParlAI/tree/master/projects/convai2).\n",
    "    - **Synthetic Data Generation:** Create simulated conversations to augment real data.\n",
    "\n",
    "- **Data Volume:**\n",
    "    - **Minimum Requirement:** Millions of conversation pairs for robust training.\n",
    "    - **Optimal Range:** Tens of millions, especially for complex models.\n",
    "\n",
    "#### **Step 3: Data Preprocessing and Cleaning**\n",
    "\n",
    "Ensure the dataset is clean, consistent, and free from noise.\n",
    "\n",
    "- **Cleaning Steps:**\n",
    "    - **Remove PII:** Strip any personally identifiable information to maintain privacy.\n",
    "    - **Standardize Formats:** Ensure uniform formatting across all conversation logs.\n",
    "    - **Handle Missing Data:** Address incomplete or corrupted entries.\n",
    "    - **Filter Irrelevant Content:** Exclude off-topic or inappropriate conversations.\n",
    "\n",
    "- **Normalization:**\n",
    "    - **Lowercasing:** Standardize text casing unless case is semantically significant.\n",
    "    - **Tokenization:** Split text into tokens (words, subwords) suitable for model input.\n",
    "    - **Handling Special Characters:** Manage or remove unnecessary symbols.\n",
    "\n",
    "#### **Step 4: Tokenization and Vocabulary Building**\n",
    "\n",
    "Convert textual data into numerical representations for model ingestion.\n",
    "\n",
    "- **Tokenization Strategies:**\n",
    "    - **Word-Level Tokenization:** Splits text into individual words.\n",
    "    - **Subword Tokenization (e.g., Byte-Pair Encoding):** Breaks words into smaller units, handling out-of-vocabulary words effectively.\n",
    "    - **Character-Level Tokenization:** Splits text into individual characters; less common for large models.\n",
    "\n",
    "- **Vocabulary Considerations:**\n",
    "    - **Size:** Balance between vocabulary coverage and computational efficiency.\n",
    "    - **Coverage:** Ensure the vocabulary includes all necessary words and subwords for the target domain.\n",
    "    - **Handling Rare Words:** Implement strategies for unknown or rare words.\n",
    "\n",
    "- **Implementation:**\n",
    "    - Use libraries like **SentencePiece** or **HuggingFace Tokenizers** to build and manage the vocabulary.\n",
    "\n",
    "#### **Step 5: Training the Model**\n",
    "\n",
    "Train the neural network on the prepared dataset.\n",
    "\n",
    "- **Training Setup:**\n",
    "    - **Hardware:** Utilize GPUs or TPUs for accelerated training.\n",
    "    - **Software:** Leverage deep learning frameworks like **PyTorch** or **TensorFlow**.\n",
    "\n",
    "- **Training Process:**\n",
    "    1. **Initialize Model Parameters:**\n",
    "        - Random initialization or informed initialization based on specific requirements.\n",
    "    2. **Define Loss Function:**\n",
    "        - **Cross-Entropy Loss:** Common for classification and language modeling tasks.\n",
    "        - **Teacher Forcing:** Strategy to guide the model during training by providing the correct output tokens.\n",
    "    3. **Optimizer Selection:**\n",
    "        - **Adam / AdamW:** Popular optimizers for training transformer-based models.\n",
    "    4. **Learning Rate Scheduling:**\n",
    "        - Implement learning rate schedulers to adjust the learning rate dynamically during training.\n",
    "    5. **Regularization Techniques:**\n",
    "        - **Dropout:** Prevents overfitting by randomly deactivating neurons during training.\n",
    "        - **Weight Decay:** Penalizes large weights to encourage simpler models.\n",
    "    6. **Gradient Clipping:**\n",
    "        - Prevents exploding gradients by capping the gradient norms.\n",
    "\n",
    "- **Training Strategies:**\n",
    "    - **Distributed Training:** Spread training across multiple GPUs or nodes to handle large models and datasets.\n",
    "    - **Mixed Precision Training:** Utilize 16-bit floating points to reduce memory usage and speed up training without significant loss in precision.\n",
    "\n",
    "#### **Step 6: Fine-Tuning and Optimization**\n",
    "\n",
    "Optimize the model for better performance and efficiency.\n",
    "\n",
    "- **Hyperparameter Tuning:**\n",
    "    - Experiment with different learning rates, batch sizes, and optimizer settings.\n",
    "    - Utilize techniques like grid search or Bayesian optimization for systematic tuning.\n",
    "\n",
    "- **Model Pruning:**\n",
    "    - Reduce model size by removing less important weights, enhancing inference speed and reducing memory footprint.\n",
    "\n",
    "- **Quantization:**\n",
    "    - Convert model weights to lower precision (e.g., INT8) to improve inference efficiency without significantly affecting accuracy.\n",
    "\n",
    "#### **Step 7: Evaluation and Testing**\n",
    "\n",
    "Assess the model's performance to ensure it meets the desired criteria.\n",
    "\n",
    "- **Evaluation Metrics:**\n",
    "    - **Perplexity:** Measures how well the model predicts a sample; lower values indicate better performance.\n",
    "    - **BLEU / ROUGE Scores:** Evaluate the quality of generated responses against reference answers.\n",
    "    - **F1 Score:** Balances precision and recall for intent classification.\n",
    "    - **Human Evaluation:** Subjective assessment by human reviewers for response relevance, helpfulness, and naturalness.\n",
    "\n",
    "- **Validation Strategies:**\n",
    "    - **Hold-Out Validation:** Use a separate validation set to monitor performance during training.\n",
    "    - **Cross-Validation:** Though less common in large-scale models, can be used for more robust evaluation.\n",
    "    - **A/B Testing:** Compare different model versions in real-world scenarios to determine performance.\n",
    "\n",
    "#### **Step 8: Deployment and Scaling**\n",
    "\n",
    "Deploy the trained model to serve customer service requests effectively.\n",
    "\n",
    "- **Packaging the Model:**\n",
    "    - Save model weights and tokenizer configurations.\n",
    "        ```python\n",
    "        model.save_pretrained(\"./trained-model\")\n",
    "        tokenizer.save_pretrained(\"./trained-model\")\n",
    "        ```\n",
    "\n",
    "- **Deployment Strategies:**\n",
    "    - **REST API:** Serve the model via RESTful endpoints using frameworks like **FastAPI** or **Flask**.\n",
    "    - **Serverless Deployment:** Utilize serverless platforms for automatic scaling based on demand.\n",
    "    - **Containerization:** Package the model into Docker containers for consistent deployment across environments.\n",
    "    - **Microservices Architecture:** Integrate the model into a microservices ecosystem for modularity and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "### **3.3. Tools and Frameworks for Training from Scratch**\n",
    "\n",
    "- **Deep Learning Frameworks:**\n",
    "    - **PyTorch:** Highly flexible and widely adopted for research and production.\n",
    "    - **TensorFlow:** Comprehensive ecosystem with tools like TensorFlow Serving for deployment.\n",
    "\n",
    "- **Tokenization Libraries:**\n",
    "    - **SentencePiece:** Unsupervised text tokenizer and detokenizer.\n",
    "    - **HuggingFace Tokenizers:** Efficient tokenization for transformer models.\n",
    "\n",
    "- **Distributed Training Tools:**\n",
    "    - **Horovod:** Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet.\n",
    "    - **DeepSpeed:** Optimizes distributed training for large models.\n",
    "\n",
    "- **Experiment Tracking:**\n",
    "    - **Weights & Biases:** Comprehensive tool for tracking experiments, metrics, and hyperparameters.\n",
    "    - **TensorBoard:** Visualization tool integrated with TensorFlow and PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Comparison: Fine-Tuning vs. Training from Scratch**\n",
    "\n",
    "| Aspect                     | Fine-Tuning                                   | Training from Scratch                       |\n",
    "|----------------------------|-----------------------------------------------|---------------------------------------------|\n",
    "| **Resource Requirements** | Lower computational and data needs           | High computational and large datasets      |\n",
    "| **Development Time**      | Shorter due to existing model capabilities    | Longer due to building and training phases  |\n",
    "| **Performance**            | Generally high, leveraging pre-trained knowledge | Potentially comparable but depends on data and training |\n",
    "| **Customization**         | Limited to adapting existing models           | Fully customizable architecture and features |\n",
    "| **Expertise Needed**      | Moderate, familiarity with transfer learning  | High, deep understanding of model architectures and training |\n",
    "| **Cost**                  | Generally lower due to reduced resource needs | Higher due to extensive resources and time |\n",
    "| **Flexibility**           | Less flexibility in model architecture        | Complete flexibility in designing the model |\n",
    "\n",
    "**Recommendation:** For most applications, especially when resources and time are constrained, **fine-tuning a pre-trained model** is the preferred approach. Training from scratch is suitable for scenarios requiring highly specialized models and when substantial resources are available.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Best Practices for Developing a Customer Service Agent Model**\n",
    "\n",
    "- **Data Quality Over Quantity:** Ensure the training data is clean, relevant, and diverse to improve model performance.\n",
    "- **Continuous Learning:** Implement mechanisms for the model to learn from new interactions and adapt over time.\n",
    "- **Context Management:** Enable the model to handle multi-turn conversations by maintaining context across interactions.\n",
    "- **Fallback Mechanisms:** Design the system to escalate to human agents when the model is uncertain or unable to handle specific queries.\n",
    "- **User Privacy:** Strictly adhere to data privacy regulations (e.g., GDPR) by anonymizing and securing user data.\n",
    "- **Performance Monitoring:** Continuously monitor model performance and user satisfaction to identify areas for improvement.\n",
    "- **Scalability:** Ensure the deployment infrastructure can handle varying loads without degradation in performance.\n",
    "- **Security:** Protect the model and deployment infrastructure from unauthorized access and potential attacks.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Ethical Considerations**\n",
    "\n",
    "- **Bias Mitigation:** Ensure the model does not propagate or amplify existing biases present in the training data.\n",
    "- **Transparency:** Clearly communicate to users when they are interacting with an AI-powered agent.\n",
    "- **Accountability:** Establish protocols for handling errors, misunderstandings, and user dissatisfaction.\n",
    "- **Consent:** Obtain necessary permissions when using user data for training and ensure compliance with data protection laws.\n",
    "- **Inclusivity:** Design the model to cater to a diverse user base, accommodating various languages and cultural contexts.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Deployment Steps**\n",
    "\n",
    "Deploying the trained customer service agent model to cloud platforms like **Google Cloud Platform (GCP)** or **Amazon Web Services (AWS)** involves several steps, including setting up the cloud environment, deploying the model, and configuring endpoints for user interactions. Below are detailed steps for both GCP and AWS.\n",
    "\n",
    "### **7.1. Deploying to Google Cloud Platform (GCP)**\n",
    "\n",
    "#### **7.1.1. Using Google Cloud AI Platform**\n",
    "\n",
    "**Google Cloud AI Platform** offers managed services for deploying machine learning models, providing scalability, security, and integration with other GCP services.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Set Up GCP Project:**\n",
    "    - **Create a GCP Account:** If you don't have one, sign up at [GCP Console](https://console.cloud.google.com/).\n",
    "    - **Create a New Project:** Navigate to the GCP Console and create a new project.\n",
    "    - **Enable Billing:** Ensure billing is enabled for your project.\n",
    "\n",
    "2. **Enable Necessary APIs:**\n",
    "    ```bash\n",
    "    gcloud services enable aiplatform.googleapis.com\n",
    "    gcloud services enable storage.googleapis.com\n",
    "    ```\n",
    "\n",
    "3. **Install and Initialize Google Cloud SDK:**\n",
    "    - **Download and Install:** Follow instructions [here](https://cloud.google.com/sdk/docs/install).\n",
    "    - **Initialize SDK:**\n",
    "        ```bash\n",
    "        gcloud init\n",
    "        ```\n",
    "\n",
    "4. **Upload the Model to Google Cloud Storage (GCS):**\n",
    "    - **Create a GCS Bucket:**\n",
    "        ```bash\n",
    "        gsutil mb gs://your-model-bucket\n",
    "        ```\n",
    "    - **Upload the Model:**\n",
    "        ```bash\n",
    "        gsutil cp -r ./trained-model gs://your-model-bucket/models/customer_service_agent/\n",
    "        ```\n",
    "\n",
    "5. **Deploy the Model to AI Platform:**\n",
    "    - **Create a Model Resource:**\n",
    "        ```bash\n",
    "        gcloud ai models create customer-service-agent \\\n",
    "            --region=us-central1\n",
    "        ```\n",
    "    - **Create a Version for the Model:**\n",
    "        ```bash\n",
    "        gcloud ai versions create v1 \\\n",
    "            --model=customer-service-agent \\\n",
    "            --origin=gs://your-model-bucket/models/customer_service_agent/ \\\n",
    "            --runtime-version=2.5 \\\n",
    "            --python-version=3.7 \\\n",
    "            --framework=tensorflow \\\n",
    "            --machine-type=n1-standard-4\n",
    "        ```\n",
    "        - **Parameters:**\n",
    "            - **--runtime-version:** Specify the AI Platform runtime version compatible with your model.\n",
    "            - **--python-version:** Python version used in your training environment.\n",
    "            - **--framework:** TensorFlow, PyTorch, etc., depending on your model.\n",
    "\n",
    "6. **Set Up an Endpoint for Inference:**\n",
    "    - **Get the Model URI:**\n",
    "        ```bash\n",
    "        MODEL_URI=gs://your-model-bucket/models/customer_service_agent/v1/\n",
    "        ```\n",
    "    - **Create an Endpoint:**\n",
    "        ```bash\n",
    "        gcloud ai endpoints create \\\n",
    "            --display-name=customer-service-agent-endpoint\n",
    "        ```\n",
    "    - **Deploy the Model to the Endpoint:**\n",
    "        ```bash\n",
    "        ENDPOINT_ID=$(gcloud ai endpoints list --filter=\"display_name=customer-service-agent-endpoint\" --format=\"value(name)\")\n",
    "        \n",
    "        gcloud ai endpoints deploy-model $ENDPOINT_ID \\\n",
    "            --model=customer-service-agent \\\n",
    "            --display-name=customer-service-agent-deployment \\\n",
    "            --machine-type=n1-standard-4\n",
    "        ```\n",
    "\n",
    "7. **Test the Deployed Model:**\n",
    "    - **Create a Request JSON File (`request.json`):**\n",
    "        ```json\n",
    "        {\n",
    "            \"instances\": [\n",
    "                {\n",
    "                    \"input\": \"I need help resetting my password.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        ```\n",
    "    - **Make an Inference Request:**\n",
    "        ```bash\n",
    "        gcloud ai endpoints predict $ENDPOINT_ID \\\n",
    "            --region=us-central1 \\\n",
    "            --json-request=request.json\n",
    "        ```\n",
    "    - **Review the Response:** The model should return a relevant and helpful response.\n",
    "\n",
    "8. **Set Up Authentication and Access Controls:**\n",
    "    - **Service Accounts:** Create and manage service accounts with appropriate permissions to interact with the model.\n",
    "    - **IAM Roles:** Assign roles like `AI Platform Viewer` or `AI Platform User` to control access.\n",
    "\n",
    "9. **Monitor and Scale the Deployment:**\n",
    "    - **Monitoring:** Utilize GCP's monitoring tools to track model performance, latency, and usage.\n",
    "    - **Scaling:** AI Platform automatically handles scaling, but you can configure auto-scaling parameters if needed.\n",
    "\n",
    "**Advantages of Using AI Platform:**\n",
    "- **Managed Service:** Reduces the overhead of managing infrastructure.\n",
    "- **Scalability:** Automatically scales based on demand.\n",
    "- **Integration:** Seamlessly integrates with other GCP services like BigQuery, Cloud Storage, and IAM.\n",
    "\n",
    "**Considerations:**\n",
    "- **Cost:** Managed services may incur higher costs compared to self-managed deployments.\n",
    "- **Flexibility:** Limited control over the underlying infrastructure compared to custom deployments.\n",
    "\n",
    "#### **7.1.2. Using Google Kubernetes Engine (GKE)**\n",
    "\n",
    "For greater control and flexibility, deploying the model on **Google Kubernetes Engine (GKE)** allows you to manage the infrastructure and scaling manually.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Set Up GKE Cluster:**\n",
    "    - **Create a GKE Cluster:**\n",
    "        ```bash\n",
    "        gcloud container clusters create customer-service-cluster \\\n",
    "            --zone=us-central1-a \\\n",
    "            --num-nodes=3 \\\n",
    "            --machine-type=e2-standard-4\n",
    "        ```\n",
    "\n",
    "2. **Install Kubernetes CLI (kubectl):**\n",
    "    ```bash\n",
    "    gcloud components install kubectl\n",
    "    gcloud container clusters get-credentials customer-service-cluster --zone=us-central1-a\n",
    "    ```\n",
    "\n",
    "3. **Containerize the Model Serving Application:**\n",
    "    - **Create a Serving Application:**\n",
    "        - **Using FastAPI and Uvicorn:**\n",
    "            ```python\n",
    "            # app.py\n",
    "            from fastapi import FastAPI, HTTPException\n",
    "            from pydantic import BaseModel\n",
    "            import torch\n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "            app = FastAPI()\n",
    "\n",
    "            class Query(BaseModel):\n",
    "                input: str\n",
    "\n",
    "            # Load the model and tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(\"./trained-model\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(\"./trained-model\")\n",
    "\n",
    "            @app.post(\"/predict\")\n",
    "            async def predict(query: Query):\n",
    "                inputs = tokenizer.encode(query.input, return_tensors=\"pt\")\n",
    "                outputs = model.generate(inputs, max_length=50)\n",
    "                response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                return {\"response\": response}\n",
    "            ```\n",
    "    - **Create a Dockerfile:**\n",
    "        ```dockerfile\n",
    "        # Use Python base image\n",
    "        FROM python:3.8-slim\n",
    "\n",
    "        # Set environment variables\n",
    "        ENV PYTHONDONTWRITEBYTECODE=1\n",
    "        ENV PYTHONUNBUFFERED=1\n",
    "\n",
    "        # Set work directory\n",
    "        WORKDIR /app\n",
    "\n",
    "        # Install dependencies\n",
    "        COPY requirements.txt .\n",
    "        RUN pip install --upgrade pip\n",
    "        RUN pip install -r requirements.txt\n",
    "\n",
    "        # Copy application code\n",
    "        COPY . .\n",
    "\n",
    "        # Expose port\n",
    "        EXPOSE 8000\n",
    "\n",
    "        # Run the application\n",
    "        CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "        ```\n",
    "    - **Build and Push the Docker Image:**\n",
    "        ```bash\n",
    "        docker build -t gcr.io/your-project-id/customer-service-agent:latest .\n",
    "        docker push gcr.io/your-project-id/customer-service-agent:latest\n",
    "        ```\n",
    "\n",
    "4. **Deploy the Application to GKE:**\n",
    "    - **Create a Kubernetes Deployment (`deployment.yaml`):**\n",
    "        ```yaml\n",
    "        apiVersion: apps/v1\n",
    "        kind: Deployment\n",
    "        metadata:\n",
    "          name: customer-service-agent\n",
    "        spec:\n",
    "          replicas: 3\n",
    "          selector:\n",
    "            matchLabels:\n",
    "              app: customer-service-agent\n",
    "          template:\n",
    "            metadata:\n",
    "              labels:\n",
    "                app: customer-service-agent\n",
    "            spec:\n",
    "              containers:\n",
    "              - name: customer-service-agent\n",
    "                image: gcr.io/your-project-id/customer-service-agent:latest\n",
    "                ports:\n",
    "                - containerPort: 8000\n",
    "                resources:\n",
    "                  requests:\n",
    "                    memory: \"1Gi\"\n",
    "                    cpu: \"500m\"\n",
    "                  limits:\n",
    "                    memory: \"2Gi\"\n",
    "                    cpu: \"1\"\n",
    "        ```\n",
    "    - **Apply the Deployment:**\n",
    "        ```bash\n",
    "        kubectl apply -f deployment.yaml\n",
    "        ```\n",
    "\n",
    "5. **Expose the Deployment via a Service:**\n",
    "    - **Create a Service (`service.yaml`):**\n",
    "        ```yaml\n",
    "        apiVersion: v1\n",
    "        kind: Service\n",
    "        metadata:\n",
    "          name: customer-service-agent-service\n",
    "        spec:\n",
    "          type: LoadBalancer\n",
    "          selector:\n",
    "            app: customer-service-agent\n",
    "          ports:\n",
    "            - protocol: TCP\n",
    "              port: 80\n",
    "              targetPort: 8000\n",
    "        ```\n",
    "    - **Apply the Service:**\n",
    "        ```bash\n",
    "        kubectl apply -f service.yaml\n",
    "        ```\n",
    "\n",
    "6. **Configure Ingress (Optional):**\n",
    "    - **Set Up an Ingress Controller:** Use **NGINX Ingress Controller** or **GCP's HTTP(S) Load Balancer** for advanced routing and SSL termination.\n",
    "    - **Create Ingress Resource (`ingress.yaml`):**\n",
    "        ```yaml\n",
    "        apiVersion: networking.k8s.io/v1\n",
    "        kind: Ingress\n",
    "        metadata:\n",
    "          name: customer-service-agent-ingress\n",
    "          annotations:\n",
    "            kubernetes.io/ingress.class: \"gce\"\n",
    "            networking.gke.io/managed-certificates: \"customer-service-cert\"\n",
    "        spec:\n",
    "          rules:\n",
    "          - host: your-domain.com\n",
    "            http:\n",
    "              paths:\n",
    "              - path: /\n",
    "                pathType: Prefix\n",
    "                backend:\n",
    "                  service:\n",
    "                    name: customer-service-agent-service\n",
    "                    port:\n",
    "                      number: 80\n",
    "        ```\n",
    "    - **Apply the Ingress:**\n",
    "        ```bash\n",
    "        kubectl apply -f ingress.yaml\n",
    "        ```\n",
    "\n",
    "7. **Set Up SSL/TLS Certificates:**\n",
    "    - **Managed Certificates:** Use GCP's Managed Certificates for automatic SSL provisioning.\n",
    "    - **Create a Managed Certificate (`certificate.yaml`):**\n",
    "        ```yaml\n",
    "        apiVersion: networking.gke.io/v1beta1\n",
    "        kind: ManagedCertificate\n",
    "        metadata:\n",
    "          name: customer-service-cert\n",
    "        spec:\n",
    "          domains:\n",
    "            - your-domain.com\n",
    "        ```\n",
    "    - **Apply the Certificate:**\n",
    "        ```bash\n",
    "        kubectl apply -f certificate.yaml\n",
    "        ```\n",
    "\n",
    "8. **Monitor and Scale:**\n",
    "    - **Horizontal Pod Autoscaler (HPA):**\n",
    "        - **Create HPA (`hpa.yaml`):**\n",
    "            ```yaml\n",
    "            apiVersion: autoscaling/v2beta2\n",
    "            kind: HorizontalPodAutoscaler\n",
    "            metadata:\n",
    "              name: customer-service-agent-hpa\n",
    "            spec:\n",
    "              scaleTargetRef:\n",
    "                apiVersion: apps/v1\n",
    "                kind: Deployment\n",
    "                name: customer-service-agent\n",
    "              minReplicas: 3\n",
    "              maxReplicas: 10\n",
    "              metrics:\n",
    "              - type: Resource\n",
    "                resource:\n",
    "                  name: cpu\n",
    "                  target:\n",
    "                    type: Utilization\n",
    "                    averageUtilization: 70\n",
    "            ```\n",
    "        - **Apply HPA:**\n",
    "            ```bash\n",
    "            kubectl apply -f hpa.yaml\n",
    "            ```\n",
    "\n",
    "    - **Monitoring:** Use **Google Cloud Monitoring** and **Logging** to track application performance and logs.\n",
    "\n",
    "**Advantages of Using GKE:**\n",
    "- **Flexibility:** Full control over the deployment environment and configurations.\n",
    "- **Scalability:** Manual and automated scaling options.\n",
    "- **Integration:** Seamlessly integrates with other GCP services like Cloud Monitoring, IAM, and VPC.\n",
    "\n",
    "**Considerations:**\n",
    "- **Operational Overhead:** Requires managing Kubernetes resources and configurations.\n",
    "- **Complexity:** More complex setup compared to managed services like AI Platform.\n",
    "\n",
    "---\n",
    "\n",
    "### **7.2. Deploying to Amazon Web Services (AWS)**\n",
    "\n",
    "AWS offers robust services for deploying machine learning models, ensuring scalability, security, and seamless integration with other AWS services. Below are steps to deploy your customer service agent model using **AWS SageMaker**, **AWS Elastic Kubernetes Service (EKS)**, and **AWS Lambda with API Gateway**.\n",
    "\n",
    "#### **7.2.1. Using AWS SageMaker**\n",
    "\n",
    "**AWS SageMaker** is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning models quickly.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Set Up AWS Account:**\n",
    "    - **Create an AWS Account:** If you don't have one, sign up at [AWS Console](https://aws.amazon.com/console/).\n",
    "    - **Configure IAM Roles:** Create roles with necessary permissions for SageMaker, S3, and other services.\n",
    "\n",
    "2. **Upload the Model to Amazon S3:**\n",
    "    - **Create an S3 Bucket:**\n",
    "        ```bash\n",
    "        aws s3 mb s3://your-model-bucket\n",
    "        ```\n",
    "    - **Upload the Model:**\n",
    "        ```bash\n",
    "        aws s3 cp ./trained-model s3://your-model-bucket/models/customer_service_agent/ --recursive\n",
    "        ```\n",
    "\n",
    "3. **Create a SageMaker Model:**\n",
    "    - **Define the Model URI:**\n",
    "        ```python\n",
    "        model_uri = \"s3://your-model-bucket/models/customer_service_agent/\"\n",
    "        ```\n",
    "    - **Create the Model:**\n",
    "        ```python\n",
    "        import boto3\n",
    "\n",
    "        sagemaker = boto3.client('sagemaker', region_name='us-east-1')\n",
    "\n",
    "        response = sagemaker.create_model(\n",
    "            ModelName='customer-service-agent-model',\n",
    "            PrimaryContainer={\n",
    "                'Image': '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.9.1-gpu-py38-cu111-ubuntu20.04',\n",
    "                'ModelDataUrl': model_uri,\n",
    "                'Environment': {\n",
    "                    'SAGEMAKER_PROGRAM': 'app.py',\n",
    "                    'SAGEMAKER_REGION': 'us-east-1'\n",
    "                }\n",
    "            },\n",
    "            ExecutionRoleArn='arn:aws:iam::your-account-id:role/SageMakerRole'\n",
    "        )\n",
    "        ```\n",
    "\n",
    "4. **Deploy the Model to an Endpoint:**\n",
    "    - **Create an Endpoint Configuration:**\n",
    "        ```python\n",
    "        response = sagemaker.create_endpoint_config(\n",
    "            EndpointConfigName='customer-service-agent-endpoint-config',\n",
    "            ProductionVariants=[\n",
    "                {\n",
    "                    'VariantName': 'AllTraffic',\n",
    "                    'ModelName': 'customer-service-agent-model',\n",
    "                    'InitialInstanceCount': 1,\n",
    "                    'InstanceType': 'ml.m5.large',\n",
    "                    'InitialVariantWeight': 1\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "        ```\n",
    "    - **Create the Endpoint:**\n",
    "        ```python\n",
    "        response = sagemaker.create_endpoint(\n",
    "            EndpointName='customer-service-agent-endpoint',\n",
    "            EndpointConfigName='customer-service-agent-endpoint-config'\n",
    "        )\n",
    "        ```\n",
    "\n",
    "    - **Monitor Endpoint Creation:**\n",
    "        ```python\n",
    "        import time\n",
    "\n",
    "        while True:\n",
    "            status = sagemaker.describe_endpoint(EndpointName='customer-service-agent-endpoint')['EndpointStatus']\n",
    "            print(f\"Endpoint status: {status}\")\n",
    "            if status == 'InService':\n",
    "                break\n",
    "            elif status == 'Failed':\n",
    "                raise Exception(\"Endpoint creation failed.\")\n",
    "            time.sleep(60)\n",
    "        ```\n",
    "\n",
    "5. **Invoke the Endpoint for Predictions:**\n",
    "    - **Prepare the Request Payload:**\n",
    "        ```python\n",
    "        import json\n",
    "\n",
    "        payload = {\n",
    "            \"instances\": [\n",
    "                {\n",
    "                    \"input\": \"I need help resetting my password.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        ```\n",
    "    - **Invoke the Endpoint:**\n",
    "        ```python\n",
    "        import boto3\n",
    "\n",
    "        runtime = boto3.client('runtime.sagemaker', region_name='us-east-1')\n",
    "\n",
    "        response = runtime.invoke_endpoint(\n",
    "            EndpointName='customer-service-agent-endpoint',\n",
    "            ContentType='application/json',\n",
    "            Body=json.dumps(payload)\n",
    "        )\n",
    "\n",
    "        result = json.loads(response['Body'].read().decode())\n",
    "        print(result)\n",
    "        ```\n",
    "\n",
    "6. **Set Up Auto-Scaling and Monitoring:**\n",
    "    - **Auto-Scaling:**\n",
    "        - **Configure SageMaker Endpoint Auto-Scaling:**\n",
    "            ```bash\n",
    "            aws application-autoscaling register-scalable-target \\\n",
    "                --service-namespace sagemaker \\\n",
    "                --resource-id endpoint/customer-service-agent-endpoint/variant/AllTraffic \\\n",
    "                --scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n",
    "                --min-capacity 1 \\\n",
    "                --max-capacity 10\n",
    "            ```\n",
    "        - **Create Scaling Policy:**\n",
    "            ```bash\n",
    "            aws application-autoscaling put-scaling-policy \\\n",
    "                --policy-name cpu-scaling-policy \\\n",
    "                --service-namespace sagemaker \\\n",
    "                --resource-id endpoint/customer-service-agent-endpoint/variant/AllTraffic \\\n",
    "                --scalable-dimension sagemaker:variant:DesiredInstanceCount \\\n",
    "                --policy-type TargetTrackingScaling \\\n",
    "                --target-tracking-scaling-policy-configuration file://policy-config.json\n",
    "            ```\n",
    "            - **`policy-config.json`:**\n",
    "                ```json\n",
    "                {\n",
    "                    \"TargetValue\": 70.0,\n",
    "                    \"PredefinedMetricSpecification\": {\n",
    "                        \"PredefinedMetricType\": \"SageMakerVariantInvocationsPerInstance\"\n",
    "                    },\n",
    "                    \"ScaleOutCooldown\": 60,\n",
    "                    \"ScaleInCooldown\": 60\n",
    "                }\n",
    "                ```\n",
    "\n",
    "    - **Monitoring:**\n",
    "        - **Use Amazon CloudWatch:** Monitor metrics like invocation count, latency, and error rates.\n",
    "        - **Set Up Alarms:** Configure CloudWatch alarms to notify stakeholders of critical issues.\n",
    "\n",
    "**Advantages of Using SageMaker:**\n",
    "- **Fully Managed Service:** Simplifies deployment, scaling, and management of models.\n",
    "- **Integration:** Seamlessly integrates with other AWS services like S3, IAM, and CloudWatch.\n",
    "- **Scalability:** Automatically handles scaling based on demand.\n",
    "\n",
    "**Considerations:**\n",
    "- **Cost:** Managed services can incur higher costs, especially with high usage.\n",
    "- **Flexibility:** Limited control over underlying infrastructure compared to custom deployments.\n",
    "\n",
    "#### **7.2.2. Using AWS Elastic Kubernetes Service (EKS)**\n",
    "\n",
    "For organizations requiring more control and customization, deploying the model on **AWS Elastic Kubernetes Service (EKS)** offers flexibility akin to GKE.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Set Up EKS Cluster:**\n",
    "    - **Create an EKS Cluster:**\n",
    "        ```bash\n",
    "        aws eks create-cluster \\\n",
    "            --name customer-service-cluster \\\n",
    "            --role-arn arn:aws:iam::your-account-id:role/EKS-ClusterRole \\\n",
    "            --resources-vpc-config subnetIds=subnet-abcde123,subnet-bcdef234,securityGroupIds=sg-0123456789abcdef0\n",
    "        ```\n",
    "    - **Configure kubectl for EKS:**\n",
    "        ```bash\n",
    "        aws eks update-kubeconfig --name customer-service-cluster --region us-east-1\n",
    "        ```\n",
    "\n",
    "2. **Containerize the Model Serving Application:**\n",
    "    - **Use the Same Dockerfile as GKE Deployment:**\n",
    "        - Ensure that the Docker image is pushed to **Amazon Elastic Container Registry (ECR)**.\n",
    "        ```bash\n",
    "        aws ecr create-repository --repository-name customer-service-agent\n",
    "        ```\n",
    "        - **Authenticate Docker to ECR:**\n",
    "            ```bash\n",
    "            aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin your-account-id.dkr.ecr.us-east-1.amazonaws.com\n",
    "            ```\n",
    "        - **Tag and Push the Image:**\n",
    "            ```bash\n",
    "            docker tag customer-service-agent:latest your-account-id.dkr.ecr.us-east-1.amazonaws.com/customer-service-agent:latest\n",
    "            docker push your-account-id.dkr.ecr.us-east-1.amazonaws.com/customer-service-agent:latest\n",
    "            ```\n",
    "\n",
    "3. **Deploy the Application to EKS:**\n",
    "    - **Create a Kubernetes Deployment (`deployment.yaml`):**\n",
    "        ```yaml\n",
    "        apiVersion: apps/v1\n",
    "        kind: Deployment\n",
    "        metadata:\n",
    "          name: customer-service-agent\n",
    "        spec:\n",
    "          replicas: 3\n",
    "          selector:\n",
    "            matchLabels:\n",
    "              app: customer-service-agent\n",
    "          template:\n",
    "            metadata:\n",
    "              labels:\n",
    "                app: customer-service-agent\n",
    "            spec:\n",
    "              containers:\n",
    "              - name: customer-service-agent\n",
    "                image: your-account-id.dkr.ecr.us-east-1.amazonaws.com/customer-service-agent:latest\n",
    "                ports:\n",
    "                - containerPort: 8000\n",
    "                resources:\n",
    "                  requests:\n",
    "                    memory: \"1Gi\"\n",
    "                    cpu: \"500m\"\n",
    "                  limits:\n",
    "                    memory: \"2Gi\"\n",
    "                    cpu: \"1\"\n",
    "        ```\n",
    "    - **Apply the Deployment:**\n",
    "        ```bash\n",
    "        kubectl apply -f deployment.yaml\n",
    "        ```\n",
    "\n",
    "4. **Expose the Deployment via a Service:**\n",
    "    - **Create a Service (`service.yaml`):**\n",
    "        ```yaml\n",
    "        apiVersion: v1\n",
    "        kind: Service\n",
    "        metadata:\n",
    "          name: customer-service-agent-service\n",
    "        spec:\n",
    "          type: LoadBalancer\n",
    "          selector:\n",
    "            app: customer-service-agent\n",
    "          ports:\n",
    "            - protocol: TCP\n",
    "              port: 80\n",
    "              targetPort: 8000\n",
    "        ```\n",
    "    - **Apply the Service:**\n",
    "        ```bash\n",
    "        kubectl apply -f service.yaml\n",
    "        ```\n",
    "\n",
    "5. **Set Up Ingress (Optional):**\n",
    "    - **Use AWS Load Balancer Controller:** For advanced routing and SSL termination.\n",
    "    - **Create an Ingress Resource (`ingress.yaml`):**\n",
    "        ```yaml\n",
    "        apiVersion: networking.k8s.io/v1\n",
    "        kind: Ingress\n",
    "        metadata:\n",
    "          name: customer-service-agent-ingress\n",
    "          annotations:\n",
    "            kubernetes.io/ingress.class: alb\n",
    "            alb.ingress.kubernetes.io/scheme: internet-facing\n",
    "            alb.ingress.kubernetes.io/ssl-redirect: \"443\"\n",
    "            alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\":80},{\"HTTPS\":443}]'\n",
    "        spec:\n",
    "          rules:\n",
    "          - host: your-domain.com\n",
    "            http:\n",
    "              paths:\n",
    "              - path: /\n",
    "                pathType: Prefix\n",
    "                backend:\n",
    "                  service:\n",
    "                    name: customer-service-agent-service\n",
    "                    port:\n",
    "                      number: 80\n",
    "        ```\n",
    "    - **Apply the Ingress:**\n",
    "        ```bash\n",
    "        kubectl apply -f ingress.yaml\n",
    "        ```\n",
    "\n",
    "6. **Set Up SSL/TLS Certificates:**\n",
    "    - **Use AWS Certificate Manager (ACM):**\n",
    "        - **Request a Certificate:** Obtain SSL certificates for your domain.\n",
    "        - **Associate with Ingress:** Configure the Ingress resource to use the ACM certificate for HTTPS traffic.\n",
    "\n",
    "7. **Monitor and Scale:**\n",
    "    - **Horizontal Pod Autoscaler (HPA):**\n",
    "        - **Create HPA (`hpa.yaml`):**\n",
    "            ```yaml\n",
    "            apiVersion: autoscaling/v2beta2\n",
    "            kind: HorizontalPodAutoscaler\n",
    "            metadata:\n",
    "              name: customer-service-agent-hpa\n",
    "            spec:\n",
    "              scaleTargetRef:\n",
    "                apiVersion: apps/v1\n",
    "                kind: Deployment\n",
    "                name: customer-service-agent\n",
    "              minReplicas: 3\n",
    "              maxReplicas: 10\n",
    "              metrics:\n",
    "              - type: Resource\n",
    "                resource:\n",
    "                  name: cpu\n",
    "                  target:\n",
    "                    type: Utilization\n",
    "                    averageUtilization: 70\n",
    "            ```\n",
    "        - **Apply HPA:**\n",
    "            ```bash\n",
    "            kubectl apply -f hpa.yaml\n",
    "            ```\n",
    "\n",
    "    - **Monitoring:** Use **Amazon CloudWatch** to monitor metrics and logs. Integrate with tools like **Prometheus** and **Grafana** for enhanced observability.\n",
    "\n",
    "**Advantages of Using EKS:**\n",
    "- **Flexibility:** Full control over Kubernetes configurations and deployments.\n",
    "- **Scalability:** Manual and automated scaling options.\n",
    "- **Integration:** Seamlessly integrates with other AWS services like IAM, VPC, and CloudWatch.\n",
    "\n",
    "**Considerations:**\n",
    "- **Operational Overhead:** Requires managing Kubernetes resources and configurations.\n",
    "- **Complexity:** More complex setup compared to managed services like SageMaker.\n",
    "\n",
    "#### **7.2.3. Using AWS Lambda with API Gateway**\n",
    "\n",
    "For lightweight and event-driven deployments, **AWS Lambda** combined with **API Gateway** offers a serverless approach to deploying your customer service agent.\n",
    "\n",
    "**Steps:**\n",
    "\n",
    "1. **Set Up AWS Account and Permissions:**\n",
    "    - **Create an AWS Account:** If you don't have one.\n",
    "    - **Configure IAM Roles:** Ensure the Lambda function has permissions to access necessary services.\n",
    "\n",
    "2. **Prepare the Lambda Function:**\n",
    "    - **Function Code (`lambda_function.py`):**\n",
    "        ```python\n",
    "        import json\n",
    "        import torch\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "        # Load model and tokenizer from S3 or package with the Lambda\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"/opt/model\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(\"/opt/model\")\n",
    "\n",
    "        def lambda_handler(event, context):\n",
    "            body = json.loads(event['body'])\n",
    "            input_text = body.get('input', '')\n",
    "\n",
    "            inputs = tokenizer.encode(input_text, return_tensors='pt')\n",
    "            outputs = model.generate(inputs, max_length=50)\n",
    "            response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            return {\n",
    "                'statusCode': 200,\n",
    "                'body': json.dumps({'response': response_text}),\n",
    "                'headers': {\n",
    "                    'Content-Type': 'application/json',\n",
    "                    'Access-Control-Allow-Origin': '*'\n",
    "                }\n",
    "            }\n",
    "        ```\n",
    "\n",
    "    - **Packaging the Model:**\n",
    "        - **Include the Trained Model:** Package the model directory within the Lambda deployment package or reference it from S3.\n",
    "        - **Layered Deployment:** Use Lambda Layers to include the model and dependencies.\n",
    "\n",
    "3. **Create an S3 Bucket for the Model (if not packaging directly):**\n",
    "    ```bash\n",
    "    aws s3 mb s3://your-model-bucket\n",
    "    aws s3 cp -r ./trained-model s3://your-model-bucket/models/customer_service_agent/\n",
    "    ```\n",
    "\n",
    "4. **Create a Lambda Function:**\n",
    "    - **Via AWS Console or CLI:**\n",
    "        ```bash\n",
    "        aws lambda create-function \\\n",
    "            --function-name customer-service-agent-lambda \\\n",
    "            --runtime python3.8 \\\n",
    "            --role arn:aws:iam::your-account-id:role/LambdaExecutionRole \\\n",
    "            --handler lambda_function.lambda_handler \\\n",
    "            --code S3Bucket=your-model-bucket,S3Key=models/customer_service_agent/lambda_deployment_package.zip \\\n",
    "            --timeout 30 \\\n",
    "            --memory-size 2048 \\\n",
    "            --layers arn:aws:lambda:us-east-1:your-account-id:layer:your-layer-name:1\n",
    "        ```\n",
    "\n",
    "5. **Set Up API Gateway:**\n",
    "    - **Create a REST API:**\n",
    "        ```bash\n",
    "        aws apigateway create-rest-api --name 'CustomerServiceAPI'\n",
    "        ```\n",
    "    - **Get the API ID:**\n",
    "        ```bash\n",
    "        API_ID=$(aws apigateway get-rest-apis --query \"items[?name=='CustomerServiceAPI'].id\" --output text)\n",
    "        ```\n",
    "    - **Create a Resource and Method:**\n",
    "        ```bash\n",
    "        PARENT_RESOURCE_ID=$(aws apigateway get-resources --rest-api-id $API_ID --query \"items[?path=='/'].id\" --output text)\n",
    "\n",
    "        aws apigateway create-resource --rest-api-id $API_ID --parent-id $PARENT_RESOURCE_ID --path-part predict\n",
    "\n",
    "        RESOURCE_ID=$(aws apigateway get-resources --rest-api-id $API_ID --query \"items[?pathPart=='predict'].id\" --output text)\n",
    "\n",
    "        aws apigateway put-method --rest-api-id $API_ID --resource-id $RESOURCE_ID --http-method POST --authorization-type NONE\n",
    "\n",
    "        aws apigateway put-integration --rest-api-id $API_ID --resource-id $RESOURCE_ID --http-method POST --type AWS_PROXY --integration-http-method POST --uri arn:aws:apigateway:us-east-1:lambda:path/2015-03-31/functions/arn:aws:lambda:us-east-1:your-account-id:function:customer-service-agent-lambda/invocations\n",
    "\n",
    "        aws lambda add-permission --function-name customer-service-agent-lambda --statement-id apigateway-test-2 --action lambda:InvokeFunction --principal apigateway.amazonaws.com --source-arn \"arn:aws:execute-api:us-east-1:your-account-id:$API_ID/*/POST/predict\"\n",
    "        ```\n",
    "\n",
    "    - **Deploy the API:**\n",
    "        ```bash\n",
    "        aws apigateway create-deployment --rest-api-id $API_ID --stage-name prod\n",
    "        ```\n",
    "\n",
    "6. **Test the API Endpoint:**\n",
    "    - **Make a POST Request:**\n",
    "        ```bash\n",
    "        curl -X POST https://$API_ID.execute-api.us-east-1.amazonaws.com/prod/predict \\\n",
    "        -H \"Content-Type: application/json\" \\\n",
    "        -d '{\"input\": \"I need help resetting my password.\"}'\n",
    "        ```\n",
    "\n",
    "    - **Review the Response:** The Lambda function should return a JSON response with the generated answer.\n",
    "\n",
    "7. **Monitor and Scale:**\n",
    "    - **Use AWS CloudWatch:** Monitor Lambda metrics like invocation count, duration, and error rates.\n",
    "    - **Configure API Gateway Throttling:** Set up request rate limits and burst capacities to protect the backend.\n",
    "\n",
    "**Advantages of Using Lambda with API Gateway:**\n",
    "- **Serverless:** No need to manage servers; scales automatically with demand.\n",
    "- **Cost-Effective:** Pay only for actual usage.\n",
    "- **Quick Deployment:** Rapidly deploy and update functions.\n",
    "\n",
    "**Considerations:**\n",
    "- **Cold Starts:** May introduce latency for infrequently used functions.\n",
    "- **Resource Limits:** Lambda has limits on execution time, memory, and package size.\n",
    "- **State Management:** Stateless functions; need external storage for stateful interactions.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Conclusion**\n",
    "\n",
    "Developing a customer service agent using machine learning involves a series of well-defined steps, from data preparation and model training to deployment and monitoring. Choosing between **fine-tuning an existing model** and **training a model from scratch** depends on your organization's resources, expertise, and specific requirements. Deploying the model to cloud platforms like **GCP** and **AWS** ensures that the system is scalable, reliable, and accessible to users.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "- **Fine-Tuning vs. Training from Scratch:** Fine-tuning offers efficiency and leverages existing knowledge, while training from scratch provides complete customization at the cost of higher resource requirements.\n",
    "- **Deployment Strategies:** Utilize managed services like **AI Platform** and **SageMaker** for ease of deployment, or opt for container orchestration with **GKE** and **EKS** for greater control.\n",
    "- **Monitoring and Scaling:** Implement robust monitoring and autoscaling mechanisms to maintain performance and handle varying loads effectively.\n",
    "- **Ethical Considerations:** Ensure fairness, transparency, and privacy in the model's interactions and data handling.\n",
    "\n",
    "By adhering to best practices and leveraging the strengths of modern cloud platforms, you can develop and deploy an effective, scalable, and user-friendly customer service agent.\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Q&A Session**\n",
    "\n",
    "- **Common Questions:**\n",
    "    - **Scalability:** How does the deployment handle sudden spikes in user requests?\n",
    "    - **Cost Management:** What strategies are in place to optimize cloud costs associated with model deployment?\n",
    "    - **Security:** How are API endpoints secured to prevent unauthorized access?\n",
    "    - **Model Updates:** How can the model be updated or retrained without significant downtime?\n",
    "    - **Latency:** What measures ensure low-latency responses for real-time customer interactions?\n",
    "\n",
    "- **Preparing for Questions:**\n",
    "    - **Understand Deployment Choices:** Be ready to explain why certain deployment strategies were chosen over others.\n",
    "    - **Performance Metrics:** Have data on model performance, response times, and scalability.\n",
    "    - **Future Enhancements:** Discuss potential improvements and how they align with business goals.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Appendix: Additional Resources**\n",
    "\n",
    "- **Books and Tutorials:**\n",
    "    - *\"Deep Learning with Python\"* by François Chollet\n",
    "    - *\"Natural Language Processing with Transformers\"* by Lewis Tunstall, Leandro von Werra, and Thomas Wolf\n",
    "\n",
    "- **Online Courses:**\n",
    "    - [HuggingFace Course](https://huggingface.co/course/chapter1)\n",
    "    - [Coursera - Natural Language Processing Specialization](https://www.coursera.org/specializations/natural-language-processing)\n",
    "\n",
    "- **Documentation:**\n",
    "    - [HuggingFace Transformers](https://huggingface.co/docs/transformers/index)\n",
    "    - [AWS SageMaker Documentation](https://docs.aws.amazon.com/sagemaker/index.html)\n",
    "    - [Google Cloud AI Platform Documentation](https://cloud.google.com/ai-platform/docs)\n",
    "\n",
    "- **Tools and Libraries:**\n",
    "    - [HuggingFace Transformers](https://github.com/huggingface/transformers)\n",
    "    - [PyTorch](https://pytorch.org/)\n",
    "    - [TensorFlow](https://www.tensorflow.org/)\n",
    "    - [Docker](https://www.docker.com/)\n",
    "    - [Kubernetes](https://kubernetes.io/)\n",
    "    - [FastAPI](https://fastapi.tiangolo.com/)\n",
    "\n",
    "- **Sample Projects:**\n",
    "    - [Conversational AI with HuggingFace](https://github.com/huggingface/transformers/tree/main/examples/pytorch/chatbot)\n",
    "    - [Customer Service Chatbot Deployment](https://github.com/aws-samples/aws-sagemaker-conversational-ai-chatbot)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Service LLM training\n",
    "**Expanded Interest Analysis and AWS Implementation Details**\n",
    "\n",
    "---\n",
    "\n",
    "### **Interest Analysis in Depth**\n",
    "\n",
    "**Objective**: Use a Large Language Model (LLM) to analyze fund managers' interactions—such as emails, chat logs, and call notes—to assess their interest in specific assets. The goal is to categorize their interest levels (e.g., high, medium, low) and identify which assets they are most interested in. This analysis enables sales executives to prioritize outreach and tailor conversations effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Data Preprocessing**\n",
    "\n",
    "**1. Data Collection**\n",
    "\n",
    "   - **Sources**:\n",
    "     - **Emails**: Extract email communications between sales executives and fund managers from email servers or CRM integrations.\n",
    "     - **Chat Logs**: Collect messages from communication platforms like Slack, Microsoft Teams, or proprietary chat systems.\n",
    "     - **Call Notes and Transcripts**: Gather notes and transcriptions from sales calls, possibly using speech-to-text services for recorded conversations.\n",
    "     - **Meeting Summaries**: Include summaries or minutes from meetings.\n",
    "\n",
    "**2. Data Privacy and Compliance**\n",
    "\n",
    "   - **Anonymization and Pseudonymization**:\n",
    "     - Remove Personally Identifiable Information (PII) like names, addresses, and account numbers.\n",
    "     - Replace sensitive data with anonymized identifiers.\n",
    "   - **Regulatory Compliance**:\n",
    "     - Ensure adherence to GDPR, CCPA, and other relevant regulations.\n",
    "     - Obtain necessary consents and provide opt-out options where required.\n",
    "   - **Data Encryption**:\n",
    "     - Encrypt data at rest and in transit using AWS Key Management Service (KMS).\n",
    "\n",
    "**3. Data Cleaning**\n",
    "\n",
    "   - **Normalization**:\n",
    "     - Convert text to lowercase.\n",
    "     - Remove special characters, HTML tags, and formatting artifacts.\n",
    "   - **Stop Words Removal**:\n",
    "     - Remove common stop words (e.g., \"and,\" \"the,\" \"is\") unless necessary for context.\n",
    "   - **Spelling Correction**:\n",
    "     - Correct typos and misspellings using tools like Amazon Comprehend or custom dictionaries.\n",
    "   - **Tokenization**:\n",
    "     - Split text into tokens (words or subwords) suitable for the LLM.\n",
    "\n",
    "**4. Data Labeling**\n",
    "\n",
    "   - **Manual Annotation**:\n",
    "     - Use a team of domain experts to label a subset of interactions.\n",
    "     - Define clear guidelines for labeling interest levels and asset mentions.\n",
    "   - **Annotation Tools**:\n",
    "     - Utilize Amazon SageMaker Ground Truth for scalable labeling.\n",
    "     - Implement quality checks and consensus mechanisms among annotators.\n",
    "\n",
    "**5. Data Augmentation**\n",
    "\n",
    "   - **Synthetic Data**:\n",
    "     - Generate synthetic examples to balance classes if certain interest levels are underrepresented.\n",
    "   - **Paraphrasing**:\n",
    "     - Use data augmentation techniques to create variations of existing data, enhancing model robustness.\n",
    "\n",
    "**6. Data Formatting**\n",
    "\n",
    "   - **Structured Format**:\n",
    "     - Organize data into JSON or CSV files with fields like \"text,\" \"interest_level,\" and \"assets_mentioned.\"\n",
    "   - **Version Control**:\n",
    "     - Use AWS CodeCommit or Git repositories to version datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Fine-Tuning**\n",
    "\n",
    "**1. Selecting a Pre-trained LLM**\n",
    "\n",
    "   - **Model Choice**:\n",
    "     - **Amazon Bedrock**: Access foundation models like GPT through AWS's managed service.\n",
    "     - **Open-Source Models**: Consider models like GPT-J or GPT-NeoX for more control.\n",
    "   - **Licensing**:\n",
    "     - Ensure the chosen model's license permits commercial use and fine-tuning.\n",
    "\n",
    "**2. Environment Setup in AWS**\n",
    "\n",
    "   - **AWS SageMaker**:\n",
    "     - Use SageMaker for training, tuning, and deploying the model.\n",
    "   - **Compute Resources**:\n",
    "     - Choose GPU instances like `ml.p3.8xlarge` for training.\n",
    "   - **Storage**:\n",
    "     - Store datasets in Amazon S3 buckets with proper access controls.\n",
    "\n",
    "**3. Fine-Tuning Process**\n",
    "\n",
    "   - **Hyperparameter Tuning**:\n",
    "     - Use SageMaker's Automatic Model Tuning to find optimal hyperparameters.\n",
    "   - **Training Script**:\n",
    "     - Develop a script compatible with SageMaker that:\n",
    "       - Loads the pre-trained model.\n",
    "       - Processes input data.\n",
    "       - Implements the fine-tuning loop.\n",
    "   - **Loss Function and Optimization**:\n",
    "     - Use appropriate loss functions like Cross-Entropy Loss for classification.\n",
    "     - Optimize using AdamW optimizer or similar.\n",
    "\n",
    "**4. Validation and Testing**\n",
    "\n",
    "   - **Data Split**:\n",
    "     - Split data into training (70%), validation (15%), and testing (15%) sets.\n",
    "   - **Metrics Evaluation**:\n",
    "     - Monitor metrics like Accuracy, Precision, Recall, F1-Score, and Confusion Matrix.\n",
    "   - **Cross-Validation**:\n",
    "     - Consider K-Fold Cross-Validation for robustness.\n",
    "\n",
    "**5. Addressing Overfitting**\n",
    "\n",
    "   - **Regularization**:\n",
    "     - Apply techniques like Dropout and Weight Decay.\n",
    "   - **Early Stopping**:\n",
    "     - Stop training when validation loss stops decreasing.\n",
    "\n",
    "**6. Logging and Experiment Tracking**\n",
    "\n",
    "   - **AWS SageMaker Experiments**:\n",
    "     - Track parameters, metrics, and artifacts.\n",
    "   - **Logging Frameworks**:\n",
    "     - Use TensorBoard or AWS CloudWatch for logging.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Deployment on AWS**\n",
    "\n",
    "**1. Preparing the Model for Deployment**\n",
    "\n",
    "   - **Model Serialization**:\n",
    "     - Save the fine-tuned model in the appropriate format (e.g., PyTorch `.pt` file).\n",
    "   - **Inference Code**:\n",
    "     - Create an inference script (`inference.py`) that:\n",
    "       - Loads the model.\n",
    "       - Defines the input and output handling.\n",
    "       - Performs prediction.\n",
    "\n",
    "**2. Deploying with AWS SageMaker**\n",
    "\n",
    "   - **Model Endpoint Creation**:\n",
    "     - Package the model and inference script into a SageMaker model.\n",
    "     - Deploy as a real-time endpoint.\n",
    "   - **Infrastructure Configuration**:\n",
    "     - Select instance types optimized for inference, such as `ml.c5.large`.\n",
    "   - **Scalability Options**:\n",
    "     - Enable autoscaling based on metrics like CPU utilization or request count.\n",
    "\n",
    "**3. Security and Compliance**\n",
    "\n",
    "   - **VPC Integration**:\n",
    "     - Deploy endpoints within a Virtual Private Cloud for network isolation.\n",
    "   - **IAM Roles and Policies**:\n",
    "     - Assign minimal required permissions to services.\n",
    "   - **Encryption**:\n",
    "     - Enable SSL for data in transit.\n",
    "     - Use encrypted storage volumes.\n",
    "\n",
    "**4. Monitoring and Maintenance**\n",
    "\n",
    "   - **AWS CloudWatch**:\n",
    "     - Monitor metrics like latency, error rates, and resource utilization.\n",
    "   - **AWS CloudTrail**:\n",
    "     - Keep audit logs of API calls and activities.\n",
    "   - **Model Drift Detection**:\n",
    "     - Periodically assess model performance to detect drift.\n",
    "\n",
    "**5. Cost Optimization**\n",
    "\n",
    "   - **Instance Right-Sizing**:\n",
    "     - Match instance types to workload requirements.\n",
    "   - **Spot Instances**:\n",
    "     - Consider Spot Instances for non-critical batch inference to reduce costs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Leveraging the Model in AWS**\n",
    "\n",
    "**1. Integration with CRM Systems**\n",
    "\n",
    "   - **API Gateway**:\n",
    "     - Use Amazon API Gateway to create RESTful APIs that interface with the SageMaker endpoint.\n",
    "   - **Lambda Functions**:\n",
    "     - Implement AWS Lambda for lightweight data preprocessing before sending data to the model.\n",
    "   - **Event-Driven Architecture**:\n",
    "     - Trigger model inference upon new data entry in the CRM using services like Amazon EventBridge.\n",
    "\n",
    "**2. Real-Time Inference Workflow**\n",
    "\n",
    "   - **Process Flow**:\n",
    "     1. **Data Ingestion**: New interaction data is captured in the CRM.\n",
    "     2. **Preprocessing**: Data is sent to a Lambda function for preprocessing.\n",
    "     3. **Model Invocation**: The preprocessed data is sent to the SageMaker endpoint via API Gateway.\n",
    "     4. **Prediction Retrieval**: The model returns interest levels and asset associations.\n",
    "     5. **CRM Update**: Predictions are written back to the CRM for sales executives to view.\n",
    "\n",
    "**3. Batch Inference**\n",
    "\n",
    "   - **AWS Step Functions**:\n",
    "     - Orchestrate batch processing workflows.\n",
    "   - **AWS Glue**:\n",
    "     - Use for ETL processes if dealing with large datasets.\n",
    "   - **SageMaker Batch Transform**:\n",
    "     - Perform batch predictions on large datasets stored in S3.\n",
    "\n",
    "**4. Visualization and Reporting**\n",
    "\n",
    "   - **Amazon QuickSight**:\n",
    "     - Create dashboards to visualize interest trends and high-potential fund managers.\n",
    "   - **Custom CRM Dashboards**:\n",
    "     - Integrate visual components directly into the CRM interface.\n",
    "\n",
    "**5. Continuous Improvement**\n",
    "\n",
    "   - **Feedback Loop**:\n",
    "     - Collect feedback from sales executives on prediction accuracy.\n",
    "     - Use this data to retrain the model periodically.\n",
    "   - **Automated Retraining Pipelines**:\n",
    "     - Set up pipelines using SageMaker Pipelines to automate the retraining process.\n",
    "\n",
    "**6. Alerts and Notifications**\n",
    "\n",
    "   - **Amazon SNS**:\n",
    "     - Send notifications to sales teams when high-interest levels are detected.\n",
    "   - **Automated Task Creation**:\n",
    "     - Create tasks or reminders in the CRM based on model predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Additional Implementation Details**\n",
    "\n",
    "**1. Model Interpretability**\n",
    "\n",
    "   - **SHAP Values**:\n",
    "     - Use SHAP (SHapley Additive exPlanations) to interpret model predictions.\n",
    "   - **Explainability Reports**:\n",
    "     - Provide insights into why a particular interest level was assigned.\n",
    "\n",
    "**2. Multi-Language Support**\n",
    "\n",
    "   - **Language Detection**:\n",
    "     - Detect language using Amazon Comprehend and route to appropriate models.\n",
    "   - **Multilingual Models**:\n",
    "     - Fine-tune models on datasets in different languages if necessary.\n",
    "\n",
    "**3. Compliance and Auditability**\n",
    "\n",
    "   - **Data Lineage Tracking**:\n",
    "     - Keep records of data sources and transformations.\n",
    "   - **Model Versioning**:\n",
    "     - Use SageMaker Model Registry to manage different versions.\n",
    "   - **Reproducibility**:\n",
    "     - Store code and configurations in AWS CodeCommit or CodePipeline.\n",
    "\n",
    "**4. Disaster Recovery and High Availability**\n",
    "\n",
    "   - **Cross-Region Replication**:\n",
    "     - Replicate data and models across AWS regions.\n",
    "   - **Backup Strategies**:\n",
    "     - Regularly backup data and model artifacts to S3 with lifecycle policies.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "By expanding on the interest analysis and detailing the steps for preprocessing, fine-tuning, deploying, and leveraging the model in AWS, you can create a robust system that enhances your finance CRM project. This system will enable sales executives to:\n",
    "\n",
    "- **Prioritize Outreach**: Focus on fund managers with high interest in specific assets.\n",
    "- **Personalize Communication**: Tailor conversations based on predicted interests.\n",
    "- **Improve Efficiency**: Automate the analysis of vast amounts of interaction data.\n",
    "\n",
    "**AWS provides a comprehensive suite of services** that support each stage of this pipeline, from data collection and processing to model deployment and monitoring. By leveraging these services, you can build a scalable, secure, and compliant solution that delivers actionable insights to your sales team.\n",
    "\n",
    "---\n",
    "\n",
    "**Next Steps**:\n",
    "\n",
    "1. **Pilot Project**: Start with a small dataset to validate the approach.\n",
    "2. **Stakeholder Engagement**: Involve sales executives in the development process for feedback.\n",
    "3. **Iterative Development**: Use agile methodologies to refine the model and deployment.\n",
    "\n",
    "**Key AWS Services to Use**:\n",
    "\n",
    "- **Data Storage**: Amazon S3\n",
    "- **Data Labeling**: Amazon SageMaker Ground Truth\n",
    "- **Model Training and Deployment**: Amazon SageMaker\n",
    "- **API Management**: Amazon API Gateway\n",
    "- **Serverless Functions**: AWS Lambda\n",
    "- **Workflow Orchestration**: AWS Step Functions\n",
    "- **Monitoring**: Amazon CloudWatch\n",
    "- **Security**: AWS IAM, AWS KMS, VPC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Finetuning an LLM details\n",
    "**Considerations When Fine-Tuning a Large Language Model (LLM)**\n",
    "\n",
    "---\n",
    "\n",
    "Fine-tuning a Large Language Model involves adapting a pre-trained model to a specific task or domain by further training it on a smaller, task-specific dataset. This process requires careful planning and understanding of several key concepts to ensure optimal performance while avoiding common pitfalls such as overfitting or underfitting. Below, we delve into the critical considerations, define essential concepts, and provide mathematical formulas relevant to the development, training, and testing phases.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Considerations When Fine-Tuning an LLM**\n",
    "\n",
    "1. **Data Quality and Quantity**\n",
    "\n",
    "   - **Quality**: High-quality, relevant, and annotated data ensures the model learns useful patterns.\n",
    "   - **Quantity**: Sufficient data is necessary to prevent overfitting; however, LLMs can often adapt well with smaller datasets due to pre-training.\n",
    "\n",
    "2. **Domain Specificity**\n",
    "\n",
    "   - Ensure that the fine-tuning data is representative of the target domain to capture domain-specific language patterns and terminology.\n",
    "\n",
    "3. **Overfitting and Underfitting**\n",
    "\n",
    "   - **Overfitting**: The model learns noise and specific patterns from the training data, reducing generalization.\n",
    "   - **Underfitting**: The model is too simple to capture underlying data patterns, leading to poor performance on both training and test data.\n",
    "\n",
    "4. **Learning Rate Selection**\n",
    "\n",
    "   - A crucial hyperparameter that affects convergence speed and model performance.\n",
    "   - **Too High**: May cause the model to diverge.\n",
    "   - **Too Low**: Leads to slow convergence.\n",
    "\n",
    "5. **Regularization Techniques**\n",
    "\n",
    "   - **Weight Decay (L2 Regularization)**: Prevents weights from becoming too large.\n",
    "   - **Dropout**: Randomly sets a fraction of inputs to zero during training to prevent co-adaptation.\n",
    "\n",
    "6. **Batch Size**\n",
    "\n",
    "   - Affects training stability and resource utilization.\n",
    "   - **Large Batch Sizes**: Faster computation but may converge to sharp minima.\n",
    "   - **Small Batch Sizes**: Better generalization but slower training.\n",
    "\n",
    "7. **Optimization Algorithms**\n",
    "\n",
    "   - Choose appropriate optimizers like **Adam**, **AdamW**, or **SGD with Momentum** for effective training.\n",
    "\n",
    "8. **Hyperparameter Tuning**\n",
    "\n",
    "   - Systematically adjust hyperparameters like learning rate, batch size, and regularization coefficients.\n",
    "\n",
    "9. **Computational Resources**\n",
    "\n",
    "   - Ensure adequate GPU/TPU resources for efficient training and experimentation.\n",
    "\n",
    "10. **Evaluation Metrics**\n",
    "\n",
    "    - Select metrics that align with the task objectives (e.g., accuracy, F1-score, perplexity).\n",
    "\n",
    "11. **Ethical Considerations**\n",
    "\n",
    "    - Be mindful of biases in training data that could lead to unethical model behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Concepts and Mathematical Formulas**\n",
    "\n",
    "#### **1. Loss Functions**\n",
    "\n",
    "A loss function quantifies the difference between the predicted output and the actual target. Minimizing this function is the primary objective during training.\n",
    "\n",
    "- **Cross-Entropy Loss**: Commonly used for classification problems.\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{CE}} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)\n",
    "  $$\n",
    "\n",
    "  - $C$: Number of classes.\n",
    "  - $y_i$: Actual label (one-hot encoded).\n",
    "  - $\\hat{y}_i$: Predicted probability for class $i$.\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Used for regression tasks.\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{MSE}} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2\n",
    "  $$\n",
    "\n",
    "  - $N$: Number of samples.\n",
    "\n",
    "#### **2. Optimization Algorithms**\n",
    "\n",
    "Algorithms used to update model parameters to minimize the loss function.\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD)**:\n",
    "\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta_t)\n",
    "  $$\n",
    "\n",
    "  - $\\theta_t$: Model parameters at step $t$.\n",
    "  - $\\eta$: Learning rate.\n",
    "  - $\\nabla_{\\theta} \\mathcal{L}$: Gradient of the loss with respect to parameters.\n",
    "\n",
    "- **Adam Optimizer**: An adaptive learning rate method combining momentum and RMSProp.\n",
    "\n",
    "  Update rules:\n",
    "\n",
    "  $$\n",
    "  m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_{\\theta} \\mathcal{L}(\\theta_t)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla_{\\theta} \\mathcal{L}(\\theta_t))^2\n",
    "  $$\n",
    "\n",
    "  Bias correction:\n",
    "\n",
    "  $$\n",
    "  \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "  $$\n",
    "\n",
    "  Parameter update:\n",
    "\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "  $$\n",
    "\n",
    "  - $\\beta_1, \\beta_2$: Exponential decay rates for the moment estimates.\n",
    "  - $\\epsilon$: Small constant to prevent division by zero.\n",
    "\n",
    "- **AdamW Optimizer**: Adam with decoupled weight decay.\n",
    "\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\left( \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} + \\lambda \\theta_t \\right)\n",
    "  $$\n",
    "\n",
    "  - $\\lambda$: Weight decay coefficient.\n",
    "\n",
    "#### **3. Regularization Techniques**\n",
    "\n",
    "Methods to prevent overfitting by penalizing complex models.\n",
    "\n",
    "- **Weight Decay (L2 Regularization)**:\n",
    "\n",
    "  Adds a penalty proportional to the square of the magnitude of weights.\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{data}} + \\lambda \\|\\theta\\|_2^2\n",
    "  $$\n",
    "\n",
    "  - $\\|\\theta\\|_2^2$: L2 norm of the weights.\n",
    "  - $\\lambda$: Regularization strength.\n",
    "\n",
    "- **Dropout**:\n",
    "\n",
    "  During training, each neuron is kept active with probability $p$ or set to zero otherwise.\n",
    "\n",
    "  Mathematically, for neuron output $h_i$:\n",
    "\n",
    "  $$\n",
    "  \\tilde{h}_i = h_i \\cdot z_i\n",
    "  $$\n",
    "\n",
    "  - $z_i \\sim \\text{Bernoulli}(p)$.\n",
    "\n",
    "#### **4. Learning Rate Scheduling**\n",
    "\n",
    "Adjusting the learning rate during training can lead to better convergence.\n",
    "\n",
    "- **Exponential Decay**:\n",
    "\n",
    "  $$\n",
    "  \\eta_t = \\eta_0 \\cdot \\gamma^t\n",
    "  $$\n",
    "\n",
    "  - $\\eta_0$: Initial learning rate.\n",
    "  - $\\gamma$: Decay rate $(0 < \\gamma < 1)$.\n",
    "  - $t$: Current epoch or step.\n",
    "\n",
    "- **Cosine Annealing**:\n",
    "\n",
    "  $$\n",
    "  \\eta_t = \\eta_{\\text{min}} + \\frac{1}{2} (\\eta_{\\text{max}} - \\eta_{\\text{min}}) \\left(1 + \\cos\\left( \\frac{T_{\\text{cur}}}{T_{\\text{max}}} \\pi \\right)\\right)\n",
    "  $$\n",
    "\n",
    "  - Smoothly adjusts the learning rate between $\\eta_{\\text{max}}$ and $\\eta_{\\text{min}}$.\n",
    "\n",
    "#### **5. Gradient Clipping**\n",
    "\n",
    "Prevents exploding gradients by capping the gradient norm.\n",
    "\n",
    "- **Global Norm Clipping**:\n",
    "\n",
    "  $$\n",
    "  \\text{If } \\|\\nabla_{\\theta} \\mathcal{L}\\|_2 > \\tau, \\quad \\nabla_{\\theta} \\mathcal{L} \\leftarrow \\nabla_{\\theta} \\mathcal{L} \\cdot \\frac{\\tau}{\\|\\nabla_{\\theta} \\mathcal{L}\\|_2}\n",
    "  $$\n",
    "\n",
    "  - $\\tau$: Threshold value.\n",
    "\n",
    "#### **6. Evaluation Metrics**\n",
    "\n",
    "Metrics to assess model performance on validation and test data.\n",
    "\n",
    "- **Accuracy**:\n",
    "\n",
    "  $$\n",
    "  \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}}\n",
    "  $$\n",
    "\n",
    "- **Perplexity** (for language models):\n",
    "\n",
    "  $$\n",
    "  \\text{Perplexity} = \\exp\\left( \\frac{1}{N} \\sum_{i=1}^{N} \\mathcal{L}_{\\text{CE}}^{(i)} \\right)\n",
    "  $$\n",
    "\n",
    "  - Lower perplexity indicates better predictive performance.\n",
    "\n",
    "- **Precision, Recall, F1-Score**: Used for classification tasks.\n",
    "\n",
    "  $$\n",
    "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives + False Negatives}}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\text{F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision + Recall}}\n",
    "  $$\n",
    "\n",
    "#### **7. Early Stopping**\n",
    "\n",
    "A technique to prevent overfitting by halting training when performance on a validation set starts to degrade.\n",
    "\n",
    "- Monitor validation loss $\\mathcal{L}_{\\text{val}}$.\n",
    "- Stop training if $\\mathcal{L}_{\\text{val}}$ does not improve for $k$ consecutive epochs.\n",
    "\n",
    "#### **8. Batch Normalization**\n",
    "\n",
    "Normalizes inputs to layers to stabilize learning.\n",
    "\n",
    "For input $x$:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_i, \\quad \\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "Normalized output:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "- $\\epsilon$: Small constant.\n",
    "\n",
    "#### **9. Transfer Learning**\n",
    "\n",
    "Leveraging knowledge from a pre-trained model.\n",
    "\n",
    "- **Fine-Tuning**: Adjust all weights of the pre-trained model on new data.\n",
    "- **Feature Extraction**: Freeze certain layers and only train additional layers.\n",
    "\n",
    "---\n",
    "\n",
    "### **Development, Training, and Testing Phases**\n",
    "\n",
    "#### **Development Phase**\n",
    "\n",
    "1. **Data Preparation**\n",
    "\n",
    "   - **Cleaning**: Remove noise, correct errors.\n",
    "   - **Tokenization**: Split text into tokens (words, subwords).\n",
    "   - **Encoding**: Convert tokens to numerical representations (e.g., using a tokenizer's vocabulary).\n",
    "\n",
    "2. **Dataset Splitting**\n",
    "\n",
    "   - **Training Set**: Used to train the model.\n",
    "   - **Validation Set**: Used to tune hyperparameters and prevent overfitting.\n",
    "   - **Test Set**: Used to evaluate final model performance.\n",
    "\n",
    "3. **Model Selection**\n",
    "\n",
    "   - Choose a pre-trained model architecture compatible with your task (e.g., GPT, BERT).\n",
    "\n",
    "4. **Hyperparameter Initialization**\n",
    "\n",
    "   - Set initial values for learning rate $\\eta$, batch size $B$, weight decay $\\lambda$, etc.\n",
    "\n",
    "#### **Training Phase**\n",
    "\n",
    "1. **Forward Pass**\n",
    "\n",
    "   - Compute model predictions $\\hat{y}$ given inputs $x$:\n",
    "\n",
    "     $$\n",
    "     \\hat{y} = f_{\\theta}(x)\n",
    "     $$\n",
    "\n",
    "2. **Compute Loss**\n",
    "\n",
    "   - Calculate the loss using an appropriate loss function $\\mathcal{L}$:\n",
    "\n",
    "     $$\n",
    "     \\mathcal{L} = \\mathcal{L}(y, \\hat{y})\n",
    "     $$\n",
    "\n",
    "3. **Backward Pass (Backpropagation)**\n",
    "\n",
    "   - Compute gradients of the loss with respect to model parameters:\n",
    "\n",
    "     $$\n",
    "     \\nabla_{\\theta} \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial \\theta}\n",
    "     $$\n",
    "\n",
    "4. **Parameter Update**\n",
    "\n",
    "   - Update model parameters using an optimization algorithm:\n",
    "\n",
    "     $$\n",
    "     \\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} \\mathcal{L}\n",
    "     $$\n",
    "\n",
    "5. **Iterate Over Batches**\n",
    "\n",
    "   - Repeat forward and backward passes for each batch in the training set.\n",
    "\n",
    "6. **Epoch Completion**\n",
    "\n",
    "   - An epoch is completed when the model has seen all training data once.\n",
    "\n",
    "7. **Validation Step**\n",
    "\n",
    "   - Evaluate the model on the validation set.\n",
    "   - Adjust hyperparameters if necessary.\n",
    "\n",
    "#### **Testing Phase**\n",
    "\n",
    "1. **Final Evaluation**\n",
    "\n",
    "   - Use the trained model to predict outputs on the test set.\n",
    "\n",
    "2. **Compute Evaluation Metrics**\n",
    "\n",
    "   - Calculate metrics like accuracy, perplexity, or F1-score to assess performance.\n",
    "\n",
    "3. **Analyze Results**\n",
    "\n",
    "   - Interpret model performance.\n",
    "   - Identify areas for potential improvement.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mathematical Formulas**\n",
    "\n",
    "#### **Gradient Computation**\n",
    "\n",
    "- For each parameter $\\theta$:\n",
    "\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta_t)\n",
    "  $$\n",
    "\n",
    "#### **Loss Minimization Objective**\n",
    "\n",
    "- The goal is to minimize the expected loss over the data distribution:\n",
    "\n",
    "  $$\n",
    "  \\min_{\\theta} \\ \\mathbb{E}_{(x,y) \\sim \\mathcal{D}} \\left[ \\mathcal{L}(y, f_{\\theta}(x)) \\right]\n",
    "  $$\n",
    "\n",
    "#### **Regularized Loss Function**\n",
    "\n",
    "- Incorporating regularization:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{data}} + \\lambda R(\\theta)\n",
    "  $$\n",
    "\n",
    "  - $R(\\theta)$: Regularization term (e.g., $\\|\\theta\\|_2^2$ for L2 regularization).\n",
    "\n",
    "#### **Batch Gradient Descent**\n",
    "\n",
    "- For a batch of size $B$:\n",
    "\n",
    "  $$\n",
    "  \\nabla_{\\theta} \\mathcal{L}_{\\text{batch}} = \\frac{1}{B} \\sum_{i=1}^{B} \\nabla_{\\theta} \\mathcal{L}^{(i)}\n",
    "  $$\n",
    "\n",
    "#### **Weight Update with Momentum**\n",
    "\n",
    "- Incorporates past gradients to smooth updates:\n",
    "\n",
    "  $$\n",
    "  v_t = \\gamma v_{t-1} + \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta_t)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\theta_{t+1} = \\theta_t - v_t\n",
    "  $$\n",
    "\n",
    "  - $\\gamma$: Momentum coefficient.\n",
    "\n",
    "---\n",
    "\n",
    "### **Practical Tips for Fine-Tuning**\n",
    "\n",
    "1. **Start with Pre-Trained Weights**\n",
    "\n",
    "   - Initialize with weights from a model pre-trained on a large corpus.\n",
    "\n",
    "2. **Adjust Learning Rate**\n",
    "\n",
    "   - Use a smaller learning rate for fine-tuning than training from scratch.\n",
    "\n",
    "3. **Layer Freezing**\n",
    "\n",
    "   - Freeze lower layers initially and fine-tune higher layers, especially if the dataset is small.\n",
    "\n",
    "4. **Use Gradient Accumulation**\n",
    "\n",
    "   - Simulate larger batch sizes when limited by GPU memory.\n",
    "\n",
    "5. **Monitor Training Closely**\n",
    "\n",
    "   - Watch for signs of overfitting, such as the validation loss increasing while training loss decreases.\n",
    "\n",
    "6. **Data Augmentation**\n",
    "\n",
    "   - Enhance the training data with techniques like synonym replacement or back-translation to improve generalization.\n",
    "\n",
    "7. **Evaluate Regularly**\n",
    "\n",
    "   - Use the validation set to evaluate performance at regular intervals.\n",
    "\n",
    "8. **Implement Early Stopping**\n",
    "\n",
    "   - Stop training when validation performance ceases to improve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
